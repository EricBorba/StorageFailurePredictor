{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset specific variables (replace regular_expression by the variables of interest)\n",
    "#%reset_selective <regular_expression>\n",
    "\n",
    "# reset all variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasaet - AlibabaOverTime (Failurelogs)\n",
    "\n",
    "df_AlibabaOver_Failurelogs = pd.read_csv('/media/erb/hdd1/DataSet/alibabaOvertime/ssd_failure_label/ssd_failure_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset - Alibaba Snapshot (TimeStamps of failed SSDs, SMART attributes in 39 columns, SSDs location, applications, SSD models and Disk ID)\n",
    "\n",
    "df_AlibabaSnapShot_FailuresAppsLocation = pd.read_csv('/media/erb/hdd1/DataSet/alibabaSnapShot/ssd_failure_tag/ssd_failure_tag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "keep must be either \"first\", \"last\" or False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/erb/Documents/sourcecode/SMARTFilter/IOMeter and Bench Extractor.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erb/Documents/sourcecode/SMARTFilter/IOMeter%20and%20Bench%20Extractor.ipynb#ch0000004?line=4'>5</a>\u001b[0m df_Failurelogs_FailuresAppsLocation[\u001b[39m'\u001b[39m\u001b[39mfailure_time\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m  pd\u001b[39m.\u001b[39mto_datetime(df_Failurelogs_FailuresAppsLocation[\u001b[39m'\u001b[39m\u001b[39mfailure_time\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/erb/Documents/sourcecode/SMARTFilter/IOMeter%20and%20Bench%20Extractor.ipynb#ch0000004?line=6'>7</a>\u001b[0m \u001b[39m# Removing duplicates\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/erb/Documents/sourcecode/SMARTFilter/IOMeter%20and%20Bench%20Extractor.ipynb#ch0000004?line=7'>8</a>\u001b[0m df_Failurelogs_FailuresAppsLocation \u001b[39m=\u001b[39m df_Failurelogs_FailuresAppsLocation\u001b[39m.\u001b[39;49mdrop_duplicates(subset\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdisk_id\u001b[39;49m\u001b[39m'\u001b[39;49m, keep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFirst\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erb/Documents/sourcecode/SMARTFilter/IOMeter%20and%20Bench%20Extractor.ipynb#ch0000004?line=9'>10</a>\u001b[0m \u001b[39m# Forcing sorting\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erb/Documents/sourcecode/SMARTFilter/IOMeter%20and%20Bench%20Extractor.ipynb#ch0000004?line=10'>11</a>\u001b[0m df_Failurelogs_FailuresAppsLocation \u001b[39m=\u001b[39m df_Failurelogs_FailuresAppsLocation\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfailure_time\u001b[39m\u001b[39m'\u001b[39m], ascending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py?line=304'>305</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py?line=305'>306</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py?line=306'>307</a>\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py?line=307'>308</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py?line=308'>309</a>\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py?line=309'>310</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py?line=310'>311</a>\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:6110\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6107'>6108</a>\u001b[0m inplace \u001b[39m=\u001b[39m validate_bool_kwarg(inplace, \u001b[39m\"\u001b[39m\u001b[39minplace\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6108'>6109</a>\u001b[0m ignore_index \u001b[39m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[39m\"\u001b[39m\u001b[39mignore_index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6109'>6110</a>\u001b[0m duplicated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mduplicated(subset, keep\u001b[39m=\u001b[39;49mkeep)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6111'>6112</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m[\u001b[39m-\u001b[39mduplicated]\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6112'>6113</a>\u001b[0m \u001b[39mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:6257\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6246'>6247</a>\u001b[0m labels, shape \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(f, vals)))\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6248'>6249</a>\u001b[0m ids \u001b[39m=\u001b[39m get_group_index(\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6249'>6250</a>\u001b[0m     labels,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6250'>6251</a>\u001b[0m     \u001b[39m# error: Argument 1 to \"tuple\" has incompatible type \"List[_T]\";\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6254'>6255</a>\u001b[0m     xnull\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6255'>6256</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6256'>6257</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/pandas/core/frame.py?line=6257'>6258</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mduplicated\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32mpandas/_libs/hashtable_func_helper.pxi:2363\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.__pyx_fuse_8duplicated\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_func_helper.pxi:2374\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.duplicated\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_func_helper.pxi:1661\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.duplicated_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: keep must be either \"first\", \"last\" or False"
     ]
    }
   ],
   "source": [
    "# Merging Failures and location datasets and fixing columns types (to have ssd failure data that has location, failure time, and smart att)\n",
    "df_Failurelogs_FailuresAppsLocation =  pd.merge(df_AlibabaOver_Failurelogs, df_AlibabaSnapShot_FailuresAppsLocation, how = 'inner', on = ['disk_id', 'failure_time'])\n",
    "\n",
    "# Changing failure time column to datetime type\n",
    "df_Failurelogs_FailuresAppsLocation['failure_time'] =  pd.to_datetime(df_Failurelogs_FailuresAppsLocation['failure_time'])\n",
    "\n",
    "# Removing duplicates\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.drop_duplicates(subset='disk_id', keep=\"first\")\n",
    "\n",
    "# Forcing sorting\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.sort_values(by=['failure_time'], ascending=True)\n",
    "\n",
    "df_Failurelogs_FailuresAppsLocation.shape\n",
    "# Changing data type\n",
    "#df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.astype(datatype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Failurelogs_FailuresAppsLocation = \n",
    "\n",
    "dataset_write_cenUm = dataset_cenUm.query('operation == \"W\" & individual_total == \"T\"')\n",
    "\n",
    "dataset_write_cenUm.iops_total[dataset_write_cenUm.iops_total != 0] = (1/dataset_write_cenUm.iops_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.catplot(x=\"capacity\", y=\"iops_total\", hue=\"object_size\", data=dataset_write_cenUm,\n",
    "               row=\"pattern\", col=\"work_outstd\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"Capacity\", \"IOPS\")\n",
    "fig.set_xticklabels([\"80GB HDD\", \"500GB HDD\", \"1TB HDD\", \"1TB WDHDD\", \"120GB SSD\", \"Hybrid\"])\n",
    "\n",
    "plt.savefig('write_iops.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading AlibabaOvertime dataset using Pandas\n",
    "\n",
    "start_date = date(2018, 2, 1)\n",
    "end_date = date(2018, 5, 1)\n",
    "delta = timedelta(days=1)\n",
    "df_AlibabaOver_SMARTlogs = pd.DataFrame()\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogs = pd.read_csv(path)\n",
    "        df_AlibabaOver_SMARTlogs = pd.DataFrame(df_AlibabaOver_SMARTlogs)\n",
    "        #df_AlibabaOver_SMARTlogs = pd.concat([df_AlibabaOver_SMARTlogs, df_AlibabaOver_SMARTlogsTemp], ignore_index=True)\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading AlibabaOvertime dataset using Dask\n",
    "\n",
    "start_date = date(2018, 2, 1)\n",
    "start_dateAux = start_date\n",
    "end_date = date(2018, 5, 31)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogsTemp = dd.read_csv(path)\n",
    "        if start_date == start_dateAux:\n",
    "            df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogsTemp\n",
    "        else: \n",
    "            df_AlibabaOver_SMARTlogs = dd.concat([df_AlibabaOver_SMARTlogs, df_AlibabaOver_SMARTlogsTemp])\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_x</th>\n",
       "      <th>failure_time_x</th>\n",
       "      <th>disk_id</th>\n",
       "      <th>model_y</th>\n",
       "      <th>failure_time_y</th>\n",
       "      <th>failure</th>\n",
       "      <th>app</th>\n",
       "      <th>r_5</th>\n",
       "      <th>n_5</th>\n",
       "      <th>r_183</th>\n",
       "      <th>n_183</th>\n",
       "      <th>r_184</th>\n",
       "      <th>n_184</th>\n",
       "      <th>r_187</th>\n",
       "      <th>n_187</th>\n",
       "      <th>r_195</th>\n",
       "      <th>n_195</th>\n",
       "      <th>r_197</th>\n",
       "      <th>n_197</th>\n",
       "      <th>r_199</th>\n",
       "      <th>n_199</th>\n",
       "      <th>r_program</th>\n",
       "      <th>n_program</th>\n",
       "      <th>r_erase</th>\n",
       "      <th>n_erase</th>\n",
       "      <th>n_blocks</th>\n",
       "      <th>n_wearout</th>\n",
       "      <th>r_241</th>\n",
       "      <th>n_241</th>\n",
       "      <th>r_242</th>\n",
       "      <th>n_242</th>\n",
       "      <th>r_9</th>\n",
       "      <th>n_9</th>\n",
       "      <th>r_12</th>\n",
       "      <th>n_12</th>\n",
       "      <th>r_174</th>\n",
       "      <th>n_174</th>\n",
       "      <th>n_175</th>\n",
       "      <th>node_id</th>\n",
       "      <th>rack_id</th>\n",
       "      <th>machine_room_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MA2</td>\n",
       "      <td>2019-03-22 10:24:38</td>\n",
       "      <td>4711</td>\n",
       "      <td>A3</td>\n",
       "      <td>2018-08-11 03:19:22</td>\n",
       "      <td>1</td>\n",
       "      <td>RM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.604913e+09</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42675.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>27760</td>\n",
       "      <td>10547</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MA2</td>\n",
       "      <td>2019-03-22 10:24:38</td>\n",
       "      <td>4711</td>\n",
       "      <td>A2</td>\n",
       "      <td>2019-03-22 10:24:38</td>\n",
       "      <td>1</td>\n",
       "      <td>RM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8344060.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>647937.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37998.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8579.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8558.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9780</td>\n",
       "      <td>19604</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MA1</td>\n",
       "      <td>2018-08-11 03:19:22</td>\n",
       "      <td>4711</td>\n",
       "      <td>A3</td>\n",
       "      <td>2018-08-11 03:19:22</td>\n",
       "      <td>1</td>\n",
       "      <td>RM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.604913e+09</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42675.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>27760</td>\n",
       "      <td>10547</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MA1</td>\n",
       "      <td>2018-08-11 03:19:22</td>\n",
       "      <td>4711</td>\n",
       "      <td>A2</td>\n",
       "      <td>2019-03-22 10:24:38</td>\n",
       "      <td>1</td>\n",
       "      <td>RM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>8344060.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>647937.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37998.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8579.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8558.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9780</td>\n",
       "      <td>19604</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MA2</td>\n",
       "      <td>2019-02-22 04:56:06</td>\n",
       "      <td>82064</td>\n",
       "      <td>A2</td>\n",
       "      <td>2019-02-22 04:56:06</td>\n",
       "      <td>1</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>49839324.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>30328338.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28553.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>160662</td>\n",
       "      <td>16416</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MA2</td>\n",
       "      <td>2018-12-17 12:16:33</td>\n",
       "      <td>32311</td>\n",
       "      <td>A2</td>\n",
       "      <td>2018-12-17 12:16:33</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6906.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>15019.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>5971482.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1933774.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38380.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64300</td>\n",
       "      <td>4708</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MA2</td>\n",
       "      <td>2018-05-19 17:32:03</td>\n",
       "      <td>18316</td>\n",
       "      <td>A2</td>\n",
       "      <td>2018-05-19 17:32:03</td>\n",
       "      <td>1</td>\n",
       "      <td>RM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>4033465.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>876219.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>35637.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>36496</td>\n",
       "      <td>10078</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MA2</td>\n",
       "      <td>2018-10-25 04:00:50</td>\n",
       "      <td>32466</td>\n",
       "      <td>A2</td>\n",
       "      <td>2018-10-25 04:00:50</td>\n",
       "      <td>1</td>\n",
       "      <td>RM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>3251023.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>320966.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>34202.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>64549</td>\n",
       "      <td>743</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MA2</td>\n",
       "      <td>2018-12-20 03:44:54</td>\n",
       "      <td>30501</td>\n",
       "      <td>A2</td>\n",
       "      <td>2018-12-20 03:44:54</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>14677712.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3985934.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39858.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60836</td>\n",
       "      <td>28394</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MA2</td>\n",
       "      <td>2019-03-22 10:28:12</td>\n",
       "      <td>76145</td>\n",
       "      <td>A2</td>\n",
       "      <td>2019-03-22 10:28:12</td>\n",
       "      <td>1</td>\n",
       "      <td>RM</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7563780.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>40399280.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>37841.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5518.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>149063</td>\n",
       "      <td>19604</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_x      failure_time_x  disk_id model_y      failure_time_y  failure  \\\n",
       "0     MA2 2019-03-22 10:24:38     4711      A3 2018-08-11 03:19:22        1   \n",
       "1     MA2 2019-03-22 10:24:38     4711      A2 2019-03-22 10:24:38        1   \n",
       "2     MA1 2018-08-11 03:19:22     4711      A3 2018-08-11 03:19:22        1   \n",
       "3     MA1 2018-08-11 03:19:22     4711      A2 2019-03-22 10:24:38        1   \n",
       "4     MA2 2019-02-22 04:56:06    82064      A2 2019-02-22 04:56:06        1   \n",
       "5     MA2 2018-12-17 12:16:33    32311      A2 2018-12-17 12:16:33        1   \n",
       "6     MA2 2018-05-19 17:32:03    18316      A2 2018-05-19 17:32:03        1   \n",
       "7     MA2 2018-10-25 04:00:50    32466      A2 2018-10-25 04:00:50        1   \n",
       "8     MA2 2018-12-20 03:44:54    30501      A2 2018-12-20 03:44:54        1   \n",
       "9     MA2 2019-03-22 10:28:12    76145      A2 2019-03-22 10:28:12        1   \n",
       "\n",
       "    app  r_5    n_5   r_183  n_183  r_184  n_184  r_187  n_187         r_195  \\\n",
       "0    RM  2.0   93.0     NaN    NaN    0.0  100.0    0.0  100.0  2.604913e+09   \n",
       "1    RM  2.0  100.0     0.0  100.0    0.0  100.0    0.0  100.0           NaN   \n",
       "2    RM  2.0   93.0     NaN    NaN    0.0  100.0    0.0  100.0  2.604913e+09   \n",
       "3    RM  2.0  100.0     0.0  100.0    0.0  100.0    0.0  100.0           NaN   \n",
       "4    SS  0.0  100.0     0.0  100.0    0.0  100.0    0.0  100.0           NaN   \n",
       "5  none  0.0  100.0  6906.0  100.0    0.0  100.0    0.0  100.0           NaN   \n",
       "6    RM  0.0  100.0     0.0  100.0    0.0  100.0    0.0  100.0           NaN   \n",
       "7    RM  0.0  100.0     0.0  100.0    0.0  100.0    0.0  100.0           NaN   \n",
       "8  none  5.0  100.0     0.0  100.0    0.0  100.0    0.0  100.0           NaN   \n",
       "9    RM  0.0  100.0     0.0  100.0    0.0  100.0    0.0  100.0           NaN   \n",
       "\n",
       "   n_195  r_197  n_197    r_199  n_199  r_program  n_program  r_erase  \\\n",
       "0  100.0    0.0  100.0     63.0  100.0        8.0      100.0      0.0   \n",
       "1    NaN    0.0  100.0     11.0  100.0        4.0      100.0      0.0   \n",
       "2  100.0    0.0  100.0     63.0  100.0        8.0      100.0      0.0   \n",
       "3    NaN    0.0  100.0     11.0  100.0        4.0      100.0      0.0   \n",
       "4    NaN    0.0  100.0      0.0  100.0        0.0      100.0      0.0   \n",
       "5    NaN    0.0  100.0  15019.0  100.0        0.0      100.0      0.0   \n",
       "6    NaN    0.0  100.0      0.0  100.0        0.0      100.0      0.0   \n",
       "7    NaN    0.0  100.0      0.0  100.0        0.0      100.0      0.0   \n",
       "8    NaN    0.0  100.0      0.0  100.0        0.0      100.0      0.0   \n",
       "9    NaN    0.0  100.0    205.0  100.0        0.0      100.0      0.0   \n",
       "\n",
       "   n_erase  n_blocks  n_wearout       r_241  n_241       r_242  n_242  \\\n",
       "0    100.0     130.0        1.0         NaN    NaN         NaN    NaN   \n",
       "1    100.0      99.0       69.0   8344060.0  100.0    647937.0  100.0   \n",
       "2    100.0     130.0        1.0         NaN    NaN         NaN    NaN   \n",
       "3    100.0      99.0       69.0   8344060.0  100.0    647937.0  100.0   \n",
       "4    100.0     100.0        8.0  49839324.0  100.0  30328338.0  100.0   \n",
       "5    100.0     100.0       89.0   5971482.0  100.0   1933774.0  100.0   \n",
       "6    100.0     100.0       91.0   4033465.0  100.0    876219.0  100.0   \n",
       "7    100.0     100.0       91.0   3251023.0  100.0    320966.0  100.0   \n",
       "8    100.0      99.0       61.0  14677712.0  100.0   3985934.0  100.0   \n",
       "9    100.0     100.0       80.0   7563780.0  100.0  40399280.0  100.0   \n",
       "\n",
       "       r_9    n_9    r_12   n_12   r_174  n_174  n_175  node_id  rack_id  \\\n",
       "0  42675.0  100.0    40.0  100.0    37.0  100.0  100.0    27760    10547   \n",
       "1  37998.0  100.0  8579.0  100.0  8558.0  100.0  100.0     9780    19604   \n",
       "2  42675.0  100.0    40.0  100.0    37.0  100.0  100.0    27760    10547   \n",
       "3  37998.0  100.0  8579.0  100.0  8558.0  100.0  100.0     9780    19604   \n",
       "4  28553.0  100.0    11.0  100.0     8.0  100.0  100.0   160662    16416   \n",
       "5  38380.0  100.0   130.0  100.0   126.0  100.0  100.0    64300     4708   \n",
       "6  35637.0  100.0    73.0  100.0    71.0  100.0  100.0    36496    10078   \n",
       "7  34202.0  100.0    99.0  100.0    96.0  100.0  100.0    64549      743   \n",
       "8  39858.0  100.0    32.0  100.0    28.0  100.0  100.0    60836    28394   \n",
       "9  37841.0  100.0  5518.0  100.0  5510.0  100.0  100.0   149063    19604   \n",
       "\n",
       "   machine_room_id  \n",
       "0              408  \n",
       "1              235  \n",
       "2              408  \n",
       "3              235  \n",
       "4              212  \n",
       "5              491  \n",
       "6              455  \n",
       "7              400  \n",
       "8              667  \n",
       "9              235  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Alibaba dataset\n",
    "\n",
    "# settings to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "#df_AlibabaOver_SMARTlogs.head()\n",
    "#df_AlibabaOver_Failurelogs.head(10)\n",
    "#df_AlibabaSnapShot_FailuresAppsLocation.head()\n",
    "#df_Failurelogs_FailuresAppsLocation.head(10)\n",
    "#df_Failurelogs_FailuresAppsLocation.query('model_x == \"MA2\"')\n",
    "#df_Failurelogs_FailuresAppsLocation.dtypes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Backblaze dataset\n",
    "\n",
    "start_dateBB = date(2021, 12, 29)\n",
    "start_dateAuxBB = start_dateBB\n",
    "end_dateBB = date(2021, 12, 31)\n",
    "deltaBB = timedelta(days=1)\n",
    "\n",
    "while start_dateBB <= end_dateBB:\n",
    "    pathBB = Path('/media/erb/hdd1/DataSet/backblaze/smartlogs/' + start_dateBB.strftime(\"%Y-%m-%d\") + '.csv')\n",
    "\n",
    "    if pathBB.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_BackBlaze_SMARTlogsTemp = pd.read_csv(pathBB)\n",
    "        if start_dateBB == start_dateAuxBB: # due to dask instancing of variables\n",
    "            df_BackBlaze_SMARTlogs = df_BackBlaze_SMARTlogsTemp\n",
    "        else: \n",
    "            df_BackBlaze_SMARTlogs = pd.concat([df_BackBlaze_SMARTlogs, df_BackBlaze_SMARTlogsTemp])\n",
    "    start_dateBB += deltaBB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing BackBlaze dataset\n",
    "\n",
    "df_BackBlaze_SMARTlogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alterando nome das colunas do arquivo bench e transformando a coluna timestamp no tipo datetime\n",
    "\n",
    "#PARA HDD 80GB\n",
    "\n",
    "df_80HDDBenchCenUm4KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm4KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm4KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenUm128KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm128KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm128KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenUm512KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm512KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm512KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenUm1MB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm1MB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm1MB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenDois4KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenDois4KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenDois4KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenDois128KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenDois128KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenDois128KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenDois512KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenDois512KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenDois512KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenDois1MB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenDois1MB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenDois1MB['timestamp'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinando todos os DataFrames do IOMeter\n",
    "\n",
    "#Cenario1\n",
    "\n",
    "df_IOMeterCenUm_Total = [dfTemp_80HDDMeterCenUm4KB,dfTemp_80HDDMeterCenUm128KB,dfTemp_80HDDMeterCenUm512KB,dfTemp_80HDDMeterCenUm1MB,dfTemp_500HDDMeterCenUm4KB,dfTemp_500HDDMeterCenUm128KB,dfTemp_500HDDMeterCenUm512KB,dfTemp_500HDDMeterCenUm1MB\n",
    "                         ,dfTemp_1HDDMeterCenUm4KB,dfTemp_1HDDMeterCenUm128KB,dfTemp_1HDDMeterCenUm512KB,dfTemp_1HDDMeterCenUm1MB,dfTemp_1WDHDDMeterCenUm4KB,dfTemp_1WDHDDMeterCenUm128KB,dfTemp_1WDHDDMeterCenUm512KB,dfTemp_1WDHDDMeterCenUm1MB,\n",
    "                         dfTemp_120SSDMeterCenUm4KB,dfTemp_120SSDMeterCenUm128KB,dfTemp_120SSDMeterCenUm512KB,dfTemp_120SSDMeterCenUm1MB,dfTemp_HybridMeterCenUm4KB,dfTemp_HybridMeterCenUm128KB,dfTemp_HybridMeterCenUm512KB,dfTemp_HybridMeterCenUm1MB]\n",
    "\n",
    "#Cenario2\n",
    "\n",
    "df_IOMeterCenDois_Total = [dfTemp_80HDDMeterCenDois4KB,dfTemp_80HDDMeterCenDois128KB,dfTemp_80HDDMeterCenDois512KB,dfTemp_80HDDMeterCenDois1MB,dfTemp_500HDDMeterCenDois4KB,dfTemp_500HDDMeterCenDois128KB,dfTemp_500HDDMeterCenDois512KB,dfTemp_500HDDMeterCenDois1MB\n",
    "                         ,dfTemp_1HDDMeterCenDois4KB,dfTemp_1HDDMeterCenDois128KB,dfTemp_1HDDMeterCenDois512KB,dfTemp_1HDDMeterCenDois1MB,dfTemp_1WDHDDMeterCenDois4KB,dfTemp_1WDHDDMeterCenDois128KB,dfTemp_1WDHDDMeterCenDois512KB,dfTemp_1WDHDDMeterCenDois1MB\n",
    "                           ,dfTemp_120SSDMeterCenDois4KB,dfTemp_120SSDMeterCenDois128KB,dfTemp_120SSDMeterCenDois512KB,dfTemp_120SSDMeterCenDois1MB,dfTemp_HybridMeterCenDois4KB,dfTemp_HybridMeterCenDois128KB,dfTemp_HybridMeterCenDois512KB,dfTemp_HybridMeterCenDois1MB]\n",
    "\n",
    "\n",
    "df_IOMeter_Total = df_IOMeterCenUm_Total + df_IOMeterCenDois_Total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.csv', 'w', newline='') as f:\n",
    "    dataset = csv.DictWriter(f, fieldnames = ['timestamp','capacity','object_size','pattern','operation','scenario','work_outstd','repetition','sample_count','individual_total','iops_total','iops_read','iops_write','avg_response_time_total','avg_response_time_read','avg_response_time_write','maximum_response_time_total','maximum_response_time_read','maximum_response_time_write','errors_total','errors_read','errors_write','cpu_utilization','power_maximum1_hdd','power_maximum2_hdd','power_average1_hdd','power_average2_hdd','power_maximum1_ssd','power_maximum2_ssd','power_average1_ssd','power_average2_ssd', 'power_total'], delimiter = ',')\n",
    "    dataset.writeheader()\n",
    "    #dataset= csv.writer(f, delimiter=',')\n",
    "    #dataset.writerow(['time_stamp','capacity','object_size','pattern','operation','scenario','work_outstd','repetition','individual_total','iops_total','iops_read','iops_write','avg_response_time_total','avg_response_time_read','avg_response_time_write','maximum_response_time_total','maximum_response_time_read','maximum_response_time_write','errors_total','errors_read','errors_write','cpu_utilization','power'])\n",
    "    #arquivosaida.writerow({1, 2}) -- inserindo dados\n",
    "    \n",
    "    row_index=0\n",
    "    workers_outstanding=1\n",
    "    encontrou = False\n",
    "    sample_count=0\n",
    "    repetition=1\n",
    "    pattern=1\n",
    "    kind='hdd'\n",
    "    aux_voltage_current= [0,0,0,0]\n",
    "    aux_voltage_total= [0,0,0,0]\n",
    "    aux_voltage_current_series = [0,0,0,0]\n",
    "    aux_voltage_current_hybrid= [0,0,0,0]\n",
    "    aux_voltage_total_hybrid= [0,0,0,0]\n",
    "    aux_voltage_current_series_hybrid = [0,0,0,0]\n",
    "    cenario_atual = 1\n",
    "    df_IOMeter_index = 0\n",
    "    \n",
    "    #indiv_total='I'\n",
    "    for df_IOMeter in df_IOMeter_Total:                  \n",
    "        \n",
    "        if (df_IOMeter_index<4 or (df_IOMeter_index>23 and df_IOMeter_index<28)):\n",
    "            capacity = '80GBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 0) or (df_IOMeter_index==24)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 1) or (df_IOMeter_index==25)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 2) or (df_IOMeter_index==26)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 3) or (df_IOMeter_index==27)):\n",
    "                object_size = '1MB'                \n",
    "        elif ((df_IOMeter_index > 3 and df_IOMeter_index < 8) or (df_IOMeter_index>27 and df_IOMeter_index<32)):\n",
    "            capacity = '500GBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 4) or (df_IOMeter_index==28)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 5) or (df_IOMeter_index==29)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 6) or (df_IOMeter_index==30)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 7) or (df_IOMeter_index==31)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 7 and df_IOMeter_index < 12) or (df_IOMeter_index>31 and df_IOMeter_index<36)):\n",
    "            capacity = '1TBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 8) or (df_IOMeter_index==32)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 9) or (df_IOMeter_index==33)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 10) or (df_IOMeter_index==34)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 11) or (df_IOMeter_index==35)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 11 and df_IOMeter_index<16) or (df_IOMeter_index>35 and df_IOMeter_index<40)):\n",
    "            capacity = '1TBWDHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 12) or (df_IOMeter_index==36)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 13) or (df_IOMeter_index==37)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 14) or (df_IOMeter_index==38)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 15) or (df_IOMeter_index==39)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 15 and df_IOMeter_index<20) or (df_IOMeter_index>39 and df_IOMeter_index<44)):\n",
    "            capacity = '120GBSSD'\n",
    "            kind = 'ssd'\n",
    "            if ((df_IOMeter_index == 16) or (df_IOMeter_index==40)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 17) or (df_IOMeter_index==41)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 18) or (df_IOMeter_index==42)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 19) or (df_IOMeter_index==43)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 19 and df_IOMeter_index<24) or (df_IOMeter_index>43 and df_IOMeter_index<48)):\n",
    "            capacity = 'Hybrid'\n",
    "            kind = 'hybrid'\n",
    "            if ((df_IOMeter_index == 20) or (df_IOMeter_index==44)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 21) or (df_IOMeter_index==45)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 22) or (df_IOMeter_index==46)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 23) or (df_IOMeter_index==47)):\n",
    "                object_size = '1MB'\n",
    "        for rows in df_IOMeter: \n",
    "            if (encontrou):\n",
    "                #print (rows[0])\n",
    "                aux_date = datetime.strptime(rows[0], '%Y-%m-%d %H:%M:%S:%f')\n",
    "                if (workers_outstanding == 1):\n",
    "                    sample_count = sample_count + 1\n",
    "                    if (rows[1] == 'ALL'):\n",
    "                        if (pattern == 1):\n",
    "                            if (kind == 'hdd'):\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_hdd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_hdd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_hdd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_hdd':(aux_voltage_total[3]/(sample_count-1))})\n",
    "                            elif (kind == 'ssd'):\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_ssd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_ssd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_ssd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_ssd':(aux_voltage_total[3]/(sample_count-1))})\n",
    "                            else:\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_ssd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_ssd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_ssd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_ssd':(aux_voltage_total[3]/(sample_count-1)),'power_maximum1_hdd':(aux_voltage_total_hybrid[0]/(sample_count-1)),'power_maximum2_hdd':(aux_voltage_total_hybrid[1]/(sample_count-1)),'power_average1_hdd':(aux_voltage_total_hybrid[2]/(sample_count-1)),'power_average2_hdd':(aux_voltage_total_hybrid[3]/(sample_count-1))})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Carregando dados filtrados em um dataframe para graficos de performance\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Separando por cenrios\n",
    "\n",
    "##Cenrio 1\n",
    "dataset_cenUm = dataset.query('scenario == 1')\n",
    "\n",
    "##Cenrio 2\n",
    "dataset_cenDois = dataset.query('scenario == 2')\n",
    "\n",
    "# Separando por tipo de operao\n",
    "\n",
    "##Cenrio 1\n",
    "\n",
    "### Escrita\n",
    "dataset_write_cenUm = dataset_cenUm.query('operation == \"W\" & individual_total == \"T\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUANTIDADE DE CONJUNTOS DE DADOS DO CENARIO 1\n",
    "\n",
    "dataset_cenUm_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INVERTENDO COLUNA IOPS TOTAL para performance\n",
    "\n",
    "## Cenario 1\n",
    "\n",
    "dataset_write_cenUm.iops_total[dataset_write_cenUm.iops_total != 0] = (1/dataset_write_cenUm.iops_total)\n",
    "\n",
    "dataset_read_cenUm.iops_total[dataset_read_cenUm.iops_total != 0] = (1/dataset_read_cenUm.iops_total)\n",
    "\n",
    "dataset_mix_cenUm.iops_total[dataset_mix_cenUm.iops_total != 0] = (1/dataset_mix_cenUm.iops_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## RETIRANDO RUDOS DAS MEDIES DE TENSO para performance\n",
    "\n",
    "##Cenrio 1\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_write_cenUm['power_average1_hdd'] = dataset_write_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_write_cenUm.power_average2_hdd = dataset_write_cenUm.power_average2_hdd.astype(str)\n",
    "dataset_write_cenUm['power_average2_hdd'] = dataset_write_cenUm['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average2_hdd = dataset_write_cenUm.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_write_cenUm.power_average1_ssd = dataset_write_cenUm.power_average1_ssd.astype(str)\n",
    "dataset_write_cenUm['power_average1_ssd'] = dataset_write_cenUm['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average1_ssd = dataset_write_cenUm.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_write_cenUm.power_average2_ssd = dataset_write_cenUm.power_average2_ssd.astype(str)\n",
    "dataset_write_cenUm['power_average2_ssd'] = dataset_write_cenUm['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average2_ssd = dataset_write_cenUm.power_average2_ssd.astype(float)\n",
    "\n",
    "### Leitura\n",
    "\n",
    "dataset_read_cenUm.power_average1_hdd = dataset_read_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_read_cenUm['power_average1_hdd'] = dataset_read_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenUm.power_average1_hdd = dataset_read_cenUm.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_read_cenUm.power_average2_hdd = dataset_read_cenUm.power_average2_hdd.astype(str)\n",
    "dataset_read_cenUm['power_average2_hdd'] = dataset_read_cenUm['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenUm.power_average2_hdd = dataset_read_cenUm.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_read_cenUm.power_average1_ssd = dataset_read_cenUm.power_average1_ssd.astype(str)\n",
    "dataset_read_cenUm['power_average1_ssd'] = dataset_read_cenUm['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenUm.power_average1_ssd = dataset_read_cenUm.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_read_cenUm.power_average2_ssd = dataset_read_cenUm.power_average2_ssd.astype(str)\n",
    "dataset_read_cenUm['power_average2_ssd'] = dataset_read_cenUm['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenUm.power_average2_ssd = dataset_read_cenUm.power_average2_ssd.astype(float)\n",
    "\n",
    "### Mix\n",
    "\n",
    "dataset_mix_cenUm.power_average1_hdd = dataset_mix_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_mix_cenUm['power_average1_hdd'] = dataset_mix_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenUm.power_average1_hdd = dataset_mix_cenUm.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_mix_cenUm.power_average2_hdd = dataset_mix_cenUm.power_average2_hdd.astype(str)\n",
    "dataset_mix_cenUm['power_average2_hdd'] = dataset_mix_cenUm['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenUm.power_average2_hdd = dataset_mix_cenUm.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_mix_cenUm.power_average1_ssd = dataset_mix_cenUm.power_average1_ssd.astype(str)\n",
    "dataset_mix_cenUm['power_average1_ssd'] = dataset_mix_cenUm['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenUm.power_average1_ssd = dataset_mix_cenUm.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_mix_cenUm.power_average2_ssd = dataset_mix_cenUm.power_average2_ssd.astype(str)\n",
    "dataset_mix_cenUm['power_average2_ssd'] = dataset_mix_cenUm['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenUm.power_average2_ssd = dataset_mix_cenUm.power_average2_ssd.astype(float)\n",
    "\n",
    "##Cenrio 2\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenDois.power_average1_hdd = dataset_write_cenDois.power_average1_hdd.astype(str)\n",
    "dataset_write_cenDois['power_average1_hdd'] = dataset_write_cenDois['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenDois.power_average1_hdd = dataset_write_cenDois.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_write_cenDois.power_average2_hdd = dataset_write_cenDois.power_average2_hdd.astype(str)\n",
    "dataset_write_cenDois['power_average2_hdd'] = dataset_write_cenDois['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenDois.power_average2_hdd = dataset_write_cenDois.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_write_cenDois.power_average1_ssd = dataset_write_cenDois.power_average1_ssd.astype(str)\n",
    "dataset_write_cenDois['power_average1_ssd'] = dataset_write_cenDois['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenDois.power_average1_ssd = dataset_write_cenDois.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_write_cenDois.power_average2_ssd = dataset_write_cenDois.power_average2_ssd.astype(str)\n",
    "dataset_write_cenDois['power_average2_ssd'] = dataset_write_cenDois['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenDois.power_average2_ssd = dataset_write_cenDois.power_average2_ssd.astype(float)\n",
    "\n",
    "###Leitura\n",
    "\n",
    "dataset_read_cenDois.power_average1_hdd = dataset_read_cenDois.power_average1_hdd.astype(str)\n",
    "dataset_read_cenDois['power_average1_hdd'] = dataset_read_cenDois['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenDois.power_average1_hdd = dataset_read_cenDois.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_read_cenDois.power_average2_hdd = dataset_read_cenDois.power_average2_hdd.astype(str)\n",
    "dataset_read_cenDois['power_average2_hdd'] = dataset_read_cenDois['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenDois.power_average2_hdd = dataset_read_cenDois.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_read_cenDois.power_average1_ssd = dataset_read_cenDois.power_average1_ssd.astype(str)\n",
    "dataset_read_cenDois['power_average1_ssd'] = dataset_read_cenDois['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenDois.power_average1_ssd = dataset_read_cenDois.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_read_cenDois.power_average2_ssd = dataset_read_cenDois.power_average2_ssd.astype(str)\n",
    "dataset_read_cenDois['power_average2_ssd'] = dataset_read_cenDois['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenDois.power_average2_ssd = dataset_read_cenDois.power_average2_ssd.astype(float)\n",
    "\n",
    "###Mix\n",
    "\n",
    "dataset_mix_cenDois.power_average1_hdd = dataset_mix_cenDois.power_average1_hdd.astype(str)\n",
    "dataset_mix_cenDois['power_average1_hdd'] = dataset_mix_cenDois['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenDois.power_average1_hdd = dataset_mix_cenDois.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_mix_cenDois.power_average2_hdd = dataset_mix_cenDois.power_average2_hdd.astype(str)\n",
    "dataset_mix_cenDois['power_average2_hdd'] = dataset_mix_cenDois['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenDois.power_average2_hdd = dataset_mix_cenDois.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_mix_cenDois.power_average1_ssd = dataset_mix_cenDois.power_average1_ssd.astype(str)\n",
    "dataset_mix_cenDois['power_average1_ssd'] = dataset_mix_cenDois['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenDois.power_average1_ssd = dataset_mix_cenDois.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_mix_cenDois.power_average2_ssd = dataset_mix_cenDois.power_average2_ssd.astype(str)\n",
    "dataset_mix_cenDois['power_average2_ssd'] = dataset_mix_cenDois['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenDois.power_average2_ssd = dataset_mix_cenDois.power_average2_ssd.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RETIRANDO RUDOS DAS MEDIES DE TENSO para SPN\n",
    "\n",
    "##Cenrio 1\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenUm_SPN.power_average1_hdd = dataset_write_cenUm_SPN.power_average1_hdd.astype(str)\n",
    "dataset_write_cenUm_SPN['power_average1_hdd'] = dataset_write_cenUm_SPN['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm_SPN.power_average1_hdd = dataset_write_cenUm_SPN.power_average1_hdd.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_write_cenUm.power_average1_hdd\n",
    "#dataset_teste = dataset_write_cenUm.query('capacity == \"120GBSSD\"')\n",
    "#dataset_teste.power_average2_ssd\n",
    "dataset_teste = dataset_write_cenDois.query('capacity == \"80GBHDD\"')\n",
    "#dataset_teste.power_average1_hdd\n",
    "dataset_teste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PARA PERFORMANCE\n",
    "## Calculando consumo de energia total hdd 80GB\n",
    "\n",
    "dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD'])/(1.08))*(dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_write_cenUm.power_average2_hdd[dataset_write_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD']))\n",
    "dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_read_cenUm.power_average1_hdd[dataset_read_cenUm.capacity == '80GBHDD']) /(1.08))*(dataset_read_cenUm.power_average1_hdd[dataset_read_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_read_cenUm.power_average2_hdd[dataset_read_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_read_cenUm.power_average2_hdd[dataset_read_cenUm.capacity == '80GBHDD']))\n",
    "dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_mix_cenUm.power_average1_hdd[dataset_mix_cenUm.capacity == '80GBHDD']) /(1.08))*(dataset_mix_cenUm.power_average1_hdd[dataset_mix_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_mix_cenUm.power_average2_hdd[dataset_mix_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_mix_cenUm.power_average2_hdd[dataset_mix_cenUm.capacity == '80GBHDD']))\n",
    "\n",
    "dataset_write_cenDois.power_total[dataset_write_cenDois.capacity == '80GBHDD'] = (((6.8-dataset_write_cenDois.power_average1_hdd[dataset_write_cenDois.capacity == '80GBHDD']) /(1.08))*(dataset_write_cenDois.power_average1_hdd[dataset_write_cenDois.capacity == '80GBHDD']))+(((12.7-dataset_write_cenDois.power_average2_hdd[dataset_write_cenDois.capacity == '80GBHDD']) /(0.68))*(dataset_write_cenDois.power_average2_hdd[dataset_write_cenDois.capacity == '80GBHDD']))\n",
    "dataset_read_cenDois.power_total[dataset_read_cenDois.capacity == '80GBHDD'] = (((6.8-dataset_read_cenDois.power_average1_hdd[dataset_read_cenDois.capacity == '80GBHDD'])/(1.08))*(dataset_read_cenDois.power_average1_hdd[dataset_read_cenDois.capacity == '80GBHDD']))+(((12.7-dataset_read_cenDois.power_average2_hdd[dataset_read_cenDois.capacity == '80GBHDD'])/(0.68))*(dataset_read_cenDois.power_average2_hdd[dataset_read_cenDois.capacity == '80GBHDD']))\n",
    "dataset_mix_cenDois.power_total[dataset_mix_cenDois.capacity == '80GBHDD'] = (((6.8-dataset_mix_cenDois.power_average1_hdd[dataset_mix_cenDois.capacity == '80GBHDD'])/(1.08))*(dataset_mix_cenDois.power_average1_hdd[dataset_mix_cenDois.capacity == '80GBHDD']))+(((12.7-dataset_mix_cenDois.power_average2_hdd[dataset_mix_cenDois.capacity == '80GBHDD'])/(0.68))*(dataset_mix_cenDois.power_average2_hdd[dataset_mix_cenDois.capacity == '80GBHDD']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_teste = dataset_write_cenDois.query('capacity == \"1TBWDHDD\"')\n",
    "dataset_teste.power_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# APLICANDO O MODULO para performance\n",
    "\n",
    "## hdd 80GB\n",
    "\n",
    "dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'] = abs(dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'])\n",
    "dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'] = abs(dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'])\n",
    "dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'] = abs(dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Salvando valores de energia consumida, no csv, para adicionar na analise individual do storage hibrido\n",
    "# Para Performance\n",
    "\n",
    "dataset_write_cenUm_hdd = dataset_write_cenUm.query('capacity == \"Hybrid\"')\n",
    "dataset_read_cenUm_hdd = dataset_read_cenUm.query('capacity == \"Hybrid\"')\n",
    "dataset_mix_cenUm_hdd = dataset_mix_cenUm.query('capacity == \"Hybrid\"')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_write_cenUm_hdd.power_total[dataset_write_cenUm_hdd.capacity == 'Hybrid'] = abs((((12.79-dataset_write_cenUm_hdd.power_average1_hdd[dataset_write_cenUm_hdd.capacity == 'Hybrid'])/(0.52))*(dataset_write_cenUm_hdd.power_average1_hdd[dataset_write_cenUm_hdd.capacity == 'Hybrid']))+(((6.78-dataset_write_cenUm_hdd.power_average2_hdd[dataset_write_cenUm_hdd.capacity == 'Hybrid'])/(0.43))*(dataset_write_cenUm_hdd.power_average2_hdd[dataset_write_cenUm_hdd.capacity == 'Hybrid'])))\n",
    "dataset_read_cenUm_hdd.power_total[dataset_read_cenUm_hdd.capacity == 'Hybrid'] = abs((((12.79-dataset_read_cenUm_hdd.power_average1_hdd[dataset_read_cenUm_hdd.capacity == 'Hybrid'])/(0.52))*(dataset_read_cenUm_hdd.power_average1_hdd[dataset_read_cenUm_hdd.capacity == 'Hybrid']))+(((6.78-dataset_read_cenUm_hdd.power_average2_hdd[dataset_read_cenUm_hdd.capacity == 'Hybrid'])/(0.43))*(dataset_read_cenUm_hdd.power_average2_hdd[dataset_read_cenUm_hdd.capacity == 'Hybrid'])))\n",
    "dataset_mix_cenUm_hdd.power_total[dataset_mix_cenUm_hdd.capacity == 'Hybrid'] = abs((((12.79-dataset_mix_cenUm_hdd.power_average1_hdd[dataset_mix_cenUm_hdd.capacity == 'Hybrid'])/(0.52))*(dataset_mix_cenUm_hdd.power_average1_hdd[dataset_mix_cenUm_hdd.capacity == 'Hybrid']))+(((6.78-dataset_mix_cenUm_hdd.power_average2_hdd[dataset_mix_cenUm_hdd.capacity == 'Hybrid'])/(0.43))*(dataset_mix_cenUm_hdd.power_average2_hdd[dataset_mix_cenUm_hdd.capacity == 'Hybrid'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTRO DE DADOS PARA Performance PARA DOE Minitab (com 20 replicates) - cenario 2\n",
    "\n",
    "scenario = '2'\n",
    "storages = ['80GBHDD', '500GBHDD', '1TBHDD', '1TBWDHDD', '120GBSSD']\n",
    "metrics = ['iops_total', 'avg_response_time_total', 'power_total', 'cpu_utilization']\n",
    "objects_size = ['4KB', '128KB', '512KB', '1MB']\n",
    "patterns = ['80R', 'R', 'S']\n",
    "replicates = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']\n",
    "work_outstd = ['1', '4', '16','64']\n",
    "operations = ['Write', 'Read', 'Mix']\n",
    "dataset_metric2 = pd.DataFrame()\n",
    "\n",
    "for metric in metrics:\n",
    "    for run in replicates:\n",
    "        for operation in operations:\n",
    "             for storage in storages:\n",
    "                    for object_size in objects_size:\n",
    "                        for pattern in patterns:\n",
    "                            for work in work_outstd:\n",
    "\n",
    "                                if operation == 'Write':\n",
    "                                    dataset_filter = dataset_write_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')             \n",
    "                                elif operation == 'Read':\n",
    "                                    dataset_filter = dataset_read_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "                                else:\n",
    "                                    dataset_filter = dataset_mix_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "\n",
    "\n",
    "                                Columns = [metric]\n",
    "                                dataset_metric = pd.DataFrame(columns=Columns)\n",
    "                                dataset_metric[metric] = dataset_filter[metric]\n",
    "                                dataset_metric2 = dataset_metric2.append(dataset_metric)\n",
    "                                #dataset_metric['repetition'] = dataset_filter.repetition\n",
    "    filename = \"Performance/\" + \"DOE/\" + scenario + \"/\" + metric + \"/\" + \"FilteredData.csv\"\n",
    "    dataset_metric2.to_csv(filename, index=False)\n",
    "    dataset_metric2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## FILTRO DE DADOS PARA O MODELO SPN 1 (easyfit)\n",
    "\n",
    "storages = ['80GBHDD', '500GBHDD', '1TBHDD', '1TBWDHDD', '120GBSSD', 'Hybrid']\n",
    "metrics = ['iops_total', 'avg_response_time_total', 'power_total']\n",
    "objects_size = ['4KB', '128KB', '512KB', '1MB']\n",
    "patterns = ['80R', 'R', 'S']\n",
    "work_outstd = ['1', '2', '4']\n",
    "operations = ['Write', 'Read', 'Mix']\n",
    "\n",
    "for operation in operations:\n",
    "    for metric in metrics:\n",
    "        for storage in storages:\n",
    "            for object_size in objects_size:\n",
    "                for pattern in patterns:\n",
    "                    for work in work_outstd:\n",
    "                        if operation == 'Write':\n",
    "                            dataset_filter = dataset_write_cenUm_SPN.query('capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')             \n",
    "                        elif operation == 'Read':\n",
    "                            dataset_filter = dataset_read_cenUm_SPN.query('capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "                        else:\n",
    "                            dataset_filter = dataset_mix_cenUm_SPN.query('capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "                        Columns = [metric,'repetition']\n",
    "                        Columns2 = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']\n",
    "                        dataset_metric = pd.DataFrame(columns=Columns)\n",
    "                        dataset_metric2 = pd.DataFrame()\n",
    "                        dataset_metric[metric] = dataset_filter[metric]\n",
    "                        dataset_metric['repetition'] = dataset_filter.repetition\n",
    "                        for col in Columns2:\n",
    "                            dataset_metric_temp = dataset_metric.query('repetition == @col')\n",
    "                            dataset_metric3 = pd.DataFrame()\n",
    "                            dataset_metric3[col] = dataset_metric_temp[metric].get_values()[:]\n",
    "                            dataset_metric2 = pd.concat([dataset_metric2,dataset_metric3], ignore_index=True, axis=1)\n",
    "                        filename = \"SPN1/\" + operation + \"/\" + metric + \"/\" + storage + \"/\" + object_size + \"/\" + pattern + \"/\" + work + \"/\" + \"FilteredData.csv\"\n",
    "                        dataset_metric2.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CENARIO 1\n",
    "\n",
    "## WRITE IOPS\n",
    "\n",
    "fig = sns.catplot(x=\"capacity\", y=\"iops_total\", hue=\"object_size\", data=dataset_write_cenUm,\n",
    "               row=\"pattern\", col=\"work_outstd\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"Capacity\", \"IOPS\")\n",
    "fig.set_xticklabels([\"80GB HDD\", \"500GB HDD\", \"1TB HDD\", \"1TB WDHDD\", \"120GB SSD\", \"Hybrid\"])\n",
    "\n",
    "plt.savefig('write_iops.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "teste = dataset_write_cenUm.query('pattern == \"R\" & work_outstd == 2 & capacity == \"1TBHDD\" & object_size == \"1MB\"')\n",
    "teste.power_average1\n",
    "\n",
    "teste.power_average1 = teste.power_average1.astype(str)\n",
    "teste['power_average1'] = teste['power_average1'].apply(lambda x: x.split(\"e\")[0])\n",
    "teste.power_average1 = teste.power_average1.astype(float)\n",
    "teste.power_average1\n",
    "\n",
    "\n",
    "## EXAMPLES\n",
    "\n",
    "#teste.power_average1.mean()\n",
    "#dataset_write_cenUm.iops_total.isnull().sum()\n",
    "#teste.power_average1[teste['power_average1'].str.contains('e')] = teste['power_average1'].apply(lambda x: x.split(\"e\")[0])\n",
    "#df['coluna'] = df['coluna'].apply(lambda x: x.split(\"%\")[1])\n",
    "#df.column = df.column.astype(float)\n",
    "#dataset_write_cenUm.iops_total[dataset_write_cenUm.iops_total != 0] = (1/dataset_write_cenUm.iops_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teste = dataset_cenUm.query('repetition==2 & individual_total == \"I\" & operation == \"W\" & pattern == \"R\" & work_outstd == 4 & capacity == \"1TBHDD\" & object_size == \"4KB\"')\n",
    "if (not teste.empty):\n",
    "    print ('eh')\n",
    "else:\n",
    "    x = teste.iops_total.get_values()[0]\n",
    "    print (x)\n",
    "    \n",
    "\n",
    "    \n",
    "           \n",
    "#teste.iops_total\n",
    "#teste.info()\n",
    "#print (teste.iops_total.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = dataset.query('object_size == \"128KB\" & capacity == \"120GBSSD\" & scenario == 2')\n",
    "\n",
    "#df.shape\n",
    "\n",
    "dataset.info()\n",
    "\n",
    "#dataset_write_cenUm.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
