{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading dataset - AlibabaOvertime \n",
    "\n",
    "start_date = date(2019, 12, 29)\n",
    "end_date = date(2019, 12, 31)\n",
    "delta = timedelta(days=1)\n",
    "df_AlibabaOver_SMARTlogs = pd.DataFrame()\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file():\n",
    "        df_AlibabaOver_SMARTlogsTemp = pd.read_csv(path)\n",
    "        df_AlibabaOver_SMARTlogsTemp = pd.DataFrame(df_AlibabaOver_SMARTlogsTemp)\n",
    "        #df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogs.append(df_AlibabaOver_SMARTlogsTemp)\n",
    "        df_AlibabaOver_SMARTlogs = pd.concat([df_AlibabaOver_SMARTlogs, df_AlibabaOver_SMARTlogsTemp], ignore_index=True)\n",
    "    start_date += delta    \n",
    "\n",
    "df_AlibabaOver_Failurelogs = pd.read_csv('/media/erb/hdd1/DataSet/alibabaOvertime/ssd_failure_label/ssd_failure_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "df_AlibabaOver_SMARTlogs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset - Alibaba Snapshot (goal: SSDs location and applications applied)\n",
    "\n",
    "df_AlibabaSnapShot_LocationAndApps = pd.read_csv('/media/erb/hdd1/DataSet/alibabaSnapShot/location_info_of_ssd/location_info_of_ssd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset - Backblaze\n",
    "\n",
    "start_dateBB = date(2021, 12, 29)\n",
    "end_dateBB = date(2021, 12, 31)\n",
    "deltaBB = timedelta(days=1)\n",
    "df_BackBlaze_SMARTlogs = pd.DataFrame()\n",
    "\n",
    "while start_dateBB <= end_dateBB:\n",
    "    pathBB = Path('/media/erb/hdd1/DataSet/backblaze/smartlogs/' + start_dateBB.strftime(\"%Y-%m-%d\") + '.csv')\n",
    "\n",
    "    if pathBB.is_file():\n",
    "        df_BackBlaze_SMARTlogsTemp = pd.read_csv(pathBB)\n",
    "        df_BackBlaze_SMARTlogsTemp = pd.DataFrame(df_BackBlaze_SMARTlogsTemp)\n",
    "        #df_BackBlaze_SMARTlogs = df_BackBlaze_SMARTlogs.append(df_BackBlaze_SMARTlogsTemp)\n",
    "        df_BackBlaze_SMARTlogs = pd.concat([df_BackBlaze_SMARTlogs, df_BackBlaze_SMARTlogsTemp], ignore_index=True)\n",
    "    start_dateBB += deltaBB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "df_BackBlaze_SMARTlogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alterando nome das colunas do arquivo bench e transformando a coluna timestamp no tipo datetime\n",
    "\n",
    "#PARA HDD 80GB\n",
    "\n",
    "df_80HDDBenchCenUm4KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm4KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm4KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenUm128KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm128KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm128KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenUm512KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm512KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm512KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenUm1MB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm1MB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm1MB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenDois4KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenDois4KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenDois4KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenDois128KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenDois128KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenDois128KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenDois512KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenDois512KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenDois512KB['timestamp'])\n",
    "\n",
    "df_80HDDBenchCenDois1MB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenDois1MB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenDois1MB['timestamp'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinando todos os DataFrames do IOMeter\n",
    "\n",
    "#Cenario1\n",
    "\n",
    "df_IOMeterCenUm_Total = [dfTemp_80HDDMeterCenUm4KB,dfTemp_80HDDMeterCenUm128KB,dfTemp_80HDDMeterCenUm512KB,dfTemp_80HDDMeterCenUm1MB,dfTemp_500HDDMeterCenUm4KB,dfTemp_500HDDMeterCenUm128KB,dfTemp_500HDDMeterCenUm512KB,dfTemp_500HDDMeterCenUm1MB\n",
    "                         ,dfTemp_1HDDMeterCenUm4KB,dfTemp_1HDDMeterCenUm128KB,dfTemp_1HDDMeterCenUm512KB,dfTemp_1HDDMeterCenUm1MB,dfTemp_1WDHDDMeterCenUm4KB,dfTemp_1WDHDDMeterCenUm128KB,dfTemp_1WDHDDMeterCenUm512KB,dfTemp_1WDHDDMeterCenUm1MB,\n",
    "                         dfTemp_120SSDMeterCenUm4KB,dfTemp_120SSDMeterCenUm128KB,dfTemp_120SSDMeterCenUm512KB,dfTemp_120SSDMeterCenUm1MB,dfTemp_HybridMeterCenUm4KB,dfTemp_HybridMeterCenUm128KB,dfTemp_HybridMeterCenUm512KB,dfTemp_HybridMeterCenUm1MB]\n",
    "\n",
    "#Cenario2\n",
    "\n",
    "df_IOMeterCenDois_Total = [dfTemp_80HDDMeterCenDois4KB,dfTemp_80HDDMeterCenDois128KB,dfTemp_80HDDMeterCenDois512KB,dfTemp_80HDDMeterCenDois1MB,dfTemp_500HDDMeterCenDois4KB,dfTemp_500HDDMeterCenDois128KB,dfTemp_500HDDMeterCenDois512KB,dfTemp_500HDDMeterCenDois1MB\n",
    "                         ,dfTemp_1HDDMeterCenDois4KB,dfTemp_1HDDMeterCenDois128KB,dfTemp_1HDDMeterCenDois512KB,dfTemp_1HDDMeterCenDois1MB,dfTemp_1WDHDDMeterCenDois4KB,dfTemp_1WDHDDMeterCenDois128KB,dfTemp_1WDHDDMeterCenDois512KB,dfTemp_1WDHDDMeterCenDois1MB\n",
    "                           ,dfTemp_120SSDMeterCenDois4KB,dfTemp_120SSDMeterCenDois128KB,dfTemp_120SSDMeterCenDois512KB,dfTemp_120SSDMeterCenDois1MB,dfTemp_HybridMeterCenDois4KB,dfTemp_HybridMeterCenDois128KB,dfTemp_HybridMeterCenDois512KB,dfTemp_HybridMeterCenDois1MB]\n",
    "\n",
    "\n",
    "df_IOMeter_Total = df_IOMeterCenUm_Total + df_IOMeterCenDois_Total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.csv', 'w', newline='') as f:\n",
    "    dataset = csv.DictWriter(f, fieldnames = ['timestamp','capacity','object_size','pattern','operation','scenario','work_outstd','repetition','sample_count','individual_total','iops_total','iops_read','iops_write','avg_response_time_total','avg_response_time_read','avg_response_time_write','maximum_response_time_total','maximum_response_time_read','maximum_response_time_write','errors_total','errors_read','errors_write','cpu_utilization','power_maximum1_hdd','power_maximum2_hdd','power_average1_hdd','power_average2_hdd','power_maximum1_ssd','power_maximum2_ssd','power_average1_ssd','power_average2_ssd', 'power_total'], delimiter = ',')\n",
    "    dataset.writeheader()\n",
    "    #dataset= csv.writer(f, delimiter=',')\n",
    "    #dataset.writerow(['time_stamp','capacity','object_size','pattern','operation','scenario','work_outstd','repetition','individual_total','iops_total','iops_read','iops_write','avg_response_time_total','avg_response_time_read','avg_response_time_write','maximum_response_time_total','maximum_response_time_read','maximum_response_time_write','errors_total','errors_read','errors_write','cpu_utilization','power'])\n",
    "    #arquivosaida.writerow({1, 2}) -- inserindo dados\n",
    "    \n",
    "    row_index=0\n",
    "    workers_outstanding=1\n",
    "    encontrou = False\n",
    "    sample_count=0\n",
    "    repetition=1\n",
    "    pattern=1\n",
    "    kind='hdd'\n",
    "    aux_voltage_current= [0,0,0,0]\n",
    "    aux_voltage_total= [0,0,0,0]\n",
    "    aux_voltage_current_series = [0,0,0,0]\n",
    "    aux_voltage_current_hybrid= [0,0,0,0]\n",
    "    aux_voltage_total_hybrid= [0,0,0,0]\n",
    "    aux_voltage_current_series_hybrid = [0,0,0,0]\n",
    "    cenario_atual = 1\n",
    "    df_IOMeter_index = 0\n",
    "    \n",
    "    #indiv_total='I'\n",
    "    for df_IOMeter in df_IOMeter_Total:                  \n",
    "        \n",
    "        if (df_IOMeter_index<4 or (df_IOMeter_index>23 and df_IOMeter_index<28)):\n",
    "            capacity = '80GBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 0) or (df_IOMeter_index==24)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 1) or (df_IOMeter_index==25)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 2) or (df_IOMeter_index==26)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 3) or (df_IOMeter_index==27)):\n",
    "                object_size = '1MB'                \n",
    "        elif ((df_IOMeter_index > 3 and df_IOMeter_index < 8) or (df_IOMeter_index>27 and df_IOMeter_index<32)):\n",
    "            capacity = '500GBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 4) or (df_IOMeter_index==28)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 5) or (df_IOMeter_index==29)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 6) or (df_IOMeter_index==30)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 7) or (df_IOMeter_index==31)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 7 and df_IOMeter_index < 12) or (df_IOMeter_index>31 and df_IOMeter_index<36)):\n",
    "            capacity = '1TBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 8) or (df_IOMeter_index==32)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 9) or (df_IOMeter_index==33)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 10) or (df_IOMeter_index==34)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 11) or (df_IOMeter_index==35)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 11 and df_IOMeter_index<16) or (df_IOMeter_index>35 and df_IOMeter_index<40)):\n",
    "            capacity = '1TBWDHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 12) or (df_IOMeter_index==36)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 13) or (df_IOMeter_index==37)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 14) or (df_IOMeter_index==38)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 15) or (df_IOMeter_index==39)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 15 and df_IOMeter_index<20) or (df_IOMeter_index>39 and df_IOMeter_index<44)):\n",
    "            capacity = '120GBSSD'\n",
    "            kind = 'ssd'\n",
    "            if ((df_IOMeter_index == 16) or (df_IOMeter_index==40)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 17) or (df_IOMeter_index==41)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 18) or (df_IOMeter_index==42)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 19) or (df_IOMeter_index==43)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 19 and df_IOMeter_index<24) or (df_IOMeter_index>43 and df_IOMeter_index<48)):\n",
    "            capacity = 'Hybrid'\n",
    "            kind = 'hybrid'\n",
    "            if ((df_IOMeter_index == 20) or (df_IOMeter_index==44)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 21) or (df_IOMeter_index==45)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 22) or (df_IOMeter_index==46)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 23) or (df_IOMeter_index==47)):\n",
    "                object_size = '1MB'\n",
    "        for rows in df_IOMeter: \n",
    "            if (encontrou):\n",
    "                #print (rows[0])\n",
    "                aux_date = datetime.strptime(rows[0], '%Y-%m-%d %H:%M:%S:%f')\n",
    "                if (workers_outstanding == 1):\n",
    "                    sample_count = sample_count + 1\n",
    "                    if (rows[1] == 'ALL'):\n",
    "                        if (pattern == 1):\n",
    "                            if (kind == 'hdd'):\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_hdd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_hdd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_hdd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_hdd':(aux_voltage_total[3]/(sample_count-1))})\n",
    "                            elif (kind == 'ssd'):\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_ssd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_ssd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_ssd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_ssd':(aux_voltage_total[3]/(sample_count-1))})\n",
    "                            else:\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_ssd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_ssd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_ssd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_ssd':(aux_voltage_total[3]/(sample_count-1)),'power_maximum1_hdd':(aux_voltage_total_hybrid[0]/(sample_count-1)),'power_maximum2_hdd':(aux_voltage_total_hybrid[1]/(sample_count-1)),'power_average1_hdd':(aux_voltage_total_hybrid[2]/(sample_count-1)),'power_average2_hdd':(aux_voltage_total_hybrid[3]/(sample_count-1))})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Carregando dados filtrados em um dataframe para graficos de performance\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Separando por cenários\n",
    "\n",
    "##Cenário 1\n",
    "dataset_cenUm = dataset.query('scenario == 1')\n",
    "\n",
    "##Cenário 2\n",
    "dataset_cenDois = dataset.query('scenario == 2')\n",
    "\n",
    "# Separando por tipo de operação\n",
    "\n",
    "##Cenário 1\n",
    "\n",
    "### Escrita\n",
    "dataset_write_cenUm = dataset_cenUm.query('operation == \"W\" & individual_total == \"T\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12960, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### QUANTIDADE DE CONJUNTOS DE DADOS DO CENARIO 1\n",
    "\n",
    "dataset_cenUm_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INVERTENDO COLUNA IOPS TOTAL para performance\n",
    "\n",
    "## Cenario 1\n",
    "\n",
    "dataset_write_cenUm.iops_total[dataset_write_cenUm.iops_total != 0] = (1/dataset_write_cenUm.iops_total)\n",
    "\n",
    "dataset_read_cenUm.iops_total[dataset_read_cenUm.iops_total != 0] = (1/dataset_read_cenUm.iops_total)\n",
    "\n",
    "dataset_mix_cenUm.iops_total[dataset_mix_cenUm.iops_total != 0] = (1/dataset_mix_cenUm.iops_total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:3643: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "## RETIRANDO RUÍDOS DAS MEDIÇÕES DE TENSÃO para performance\n",
    "\n",
    "##Cenário 1\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_write_cenUm['power_average1_hdd'] = dataset_write_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_write_cenUm.power_average2_hdd = dataset_write_cenUm.power_average2_hdd.astype(str)\n",
    "dataset_write_cenUm['power_average2_hdd'] = dataset_write_cenUm['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average2_hdd = dataset_write_cenUm.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_write_cenUm.power_average1_ssd = dataset_write_cenUm.power_average1_ssd.astype(str)\n",
    "dataset_write_cenUm['power_average1_ssd'] = dataset_write_cenUm['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average1_ssd = dataset_write_cenUm.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_write_cenUm.power_average2_ssd = dataset_write_cenUm.power_average2_ssd.astype(str)\n",
    "dataset_write_cenUm['power_average2_ssd'] = dataset_write_cenUm['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average2_ssd = dataset_write_cenUm.power_average2_ssd.astype(float)\n",
    "\n",
    "### Leitura\n",
    "\n",
    "dataset_read_cenUm.power_average1_hdd = dataset_read_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_read_cenUm['power_average1_hdd'] = dataset_read_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenUm.power_average1_hdd = dataset_read_cenUm.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_read_cenUm.power_average2_hdd = dataset_read_cenUm.power_average2_hdd.astype(str)\n",
    "dataset_read_cenUm['power_average2_hdd'] = dataset_read_cenUm['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenUm.power_average2_hdd = dataset_read_cenUm.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_read_cenUm.power_average1_ssd = dataset_read_cenUm.power_average1_ssd.astype(str)\n",
    "dataset_read_cenUm['power_average1_ssd'] = dataset_read_cenUm['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenUm.power_average1_ssd = dataset_read_cenUm.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_read_cenUm.power_average2_ssd = dataset_read_cenUm.power_average2_ssd.astype(str)\n",
    "dataset_read_cenUm['power_average2_ssd'] = dataset_read_cenUm['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenUm.power_average2_ssd = dataset_read_cenUm.power_average2_ssd.astype(float)\n",
    "\n",
    "### Mix\n",
    "\n",
    "dataset_mix_cenUm.power_average1_hdd = dataset_mix_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_mix_cenUm['power_average1_hdd'] = dataset_mix_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenUm.power_average1_hdd = dataset_mix_cenUm.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_mix_cenUm.power_average2_hdd = dataset_mix_cenUm.power_average2_hdd.astype(str)\n",
    "dataset_mix_cenUm['power_average2_hdd'] = dataset_mix_cenUm['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenUm.power_average2_hdd = dataset_mix_cenUm.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_mix_cenUm.power_average1_ssd = dataset_mix_cenUm.power_average1_ssd.astype(str)\n",
    "dataset_mix_cenUm['power_average1_ssd'] = dataset_mix_cenUm['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenUm.power_average1_ssd = dataset_mix_cenUm.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_mix_cenUm.power_average2_ssd = dataset_mix_cenUm.power_average2_ssd.astype(str)\n",
    "dataset_mix_cenUm['power_average2_ssd'] = dataset_mix_cenUm['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenUm.power_average2_ssd = dataset_mix_cenUm.power_average2_ssd.astype(float)\n",
    "\n",
    "##Cenário 2\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenDois.power_average1_hdd = dataset_write_cenDois.power_average1_hdd.astype(str)\n",
    "dataset_write_cenDois['power_average1_hdd'] = dataset_write_cenDois['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenDois.power_average1_hdd = dataset_write_cenDois.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_write_cenDois.power_average2_hdd = dataset_write_cenDois.power_average2_hdd.astype(str)\n",
    "dataset_write_cenDois['power_average2_hdd'] = dataset_write_cenDois['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenDois.power_average2_hdd = dataset_write_cenDois.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_write_cenDois.power_average1_ssd = dataset_write_cenDois.power_average1_ssd.astype(str)\n",
    "dataset_write_cenDois['power_average1_ssd'] = dataset_write_cenDois['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenDois.power_average1_ssd = dataset_write_cenDois.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_write_cenDois.power_average2_ssd = dataset_write_cenDois.power_average2_ssd.astype(str)\n",
    "dataset_write_cenDois['power_average2_ssd'] = dataset_write_cenDois['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenDois.power_average2_ssd = dataset_write_cenDois.power_average2_ssd.astype(float)\n",
    "\n",
    "###Leitura\n",
    "\n",
    "dataset_read_cenDois.power_average1_hdd = dataset_read_cenDois.power_average1_hdd.astype(str)\n",
    "dataset_read_cenDois['power_average1_hdd'] = dataset_read_cenDois['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenDois.power_average1_hdd = dataset_read_cenDois.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_read_cenDois.power_average2_hdd = dataset_read_cenDois.power_average2_hdd.astype(str)\n",
    "dataset_read_cenDois['power_average2_hdd'] = dataset_read_cenDois['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenDois.power_average2_hdd = dataset_read_cenDois.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_read_cenDois.power_average1_ssd = dataset_read_cenDois.power_average1_ssd.astype(str)\n",
    "dataset_read_cenDois['power_average1_ssd'] = dataset_read_cenDois['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenDois.power_average1_ssd = dataset_read_cenDois.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_read_cenDois.power_average2_ssd = dataset_read_cenDois.power_average2_ssd.astype(str)\n",
    "dataset_read_cenDois['power_average2_ssd'] = dataset_read_cenDois['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_read_cenDois.power_average2_ssd = dataset_read_cenDois.power_average2_ssd.astype(float)\n",
    "\n",
    "###Mix\n",
    "\n",
    "dataset_mix_cenDois.power_average1_hdd = dataset_mix_cenDois.power_average1_hdd.astype(str)\n",
    "dataset_mix_cenDois['power_average1_hdd'] = dataset_mix_cenDois['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenDois.power_average1_hdd = dataset_mix_cenDois.power_average1_hdd.astype(float)\n",
    "\n",
    "dataset_mix_cenDois.power_average2_hdd = dataset_mix_cenDois.power_average2_hdd.astype(str)\n",
    "dataset_mix_cenDois['power_average2_hdd'] = dataset_mix_cenDois['power_average2_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenDois.power_average2_hdd = dataset_mix_cenDois.power_average2_hdd.astype(float)\n",
    "\n",
    "dataset_mix_cenDois.power_average1_ssd = dataset_mix_cenDois.power_average1_ssd.astype(str)\n",
    "dataset_mix_cenDois['power_average1_ssd'] = dataset_mix_cenDois['power_average1_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenDois.power_average1_ssd = dataset_mix_cenDois.power_average1_ssd.astype(float)\n",
    "\n",
    "dataset_mix_cenDois.power_average2_ssd = dataset_mix_cenDois.power_average2_ssd.astype(str)\n",
    "dataset_mix_cenDois['power_average2_ssd'] = dataset_mix_cenDois['power_average2_ssd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_mix_cenDois.power_average2_ssd = dataset_mix_cenDois.power_average2_ssd.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RETIRANDO RUÍDOS DAS MEDIÇÕES DE TENSÃO para SPN\n",
    "\n",
    "##Cenário 1\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenUm_SPN.power_average1_hdd = dataset_write_cenUm_SPN.power_average1_hdd.astype(str)\n",
    "dataset_write_cenUm_SPN['power_average1_hdd'] = dataset_write_cenUm_SPN['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm_SPN.power_average1_hdd = dataset_write_cenUm_SPN.power_average1_hdd.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_write_cenUm.power_average1_hdd\n",
    "#dataset_teste = dataset_write_cenUm.query('capacity == \"120GBSSD\"')\n",
    "#dataset_teste.power_average2_ssd\n",
    "dataset_teste = dataset_write_cenDois.query('capacity == \"80GBHDD\"')\n",
    "#dataset_teste.power_average1_hdd\n",
    "dataset_teste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PARA PERFORMANCE\n",
    "## Calculando consumo de energia total hdd 80GB\n",
    "\n",
    "dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD'])/(1.08))*(dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_write_cenUm.power_average2_hdd[dataset_write_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD']))\n",
    "dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_read_cenUm.power_average1_hdd[dataset_read_cenUm.capacity == '80GBHDD']) /(1.08))*(dataset_read_cenUm.power_average1_hdd[dataset_read_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_read_cenUm.power_average2_hdd[dataset_read_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_read_cenUm.power_average2_hdd[dataset_read_cenUm.capacity == '80GBHDD']))\n",
    "dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_mix_cenUm.power_average1_hdd[dataset_mix_cenUm.capacity == '80GBHDD']) /(1.08))*(dataset_mix_cenUm.power_average1_hdd[dataset_mix_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_mix_cenUm.power_average2_hdd[dataset_mix_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_mix_cenUm.power_average2_hdd[dataset_mix_cenUm.capacity == '80GBHDD']))\n",
    "\n",
    "dataset_write_cenDois.power_total[dataset_write_cenDois.capacity == '80GBHDD'] = (((6.8-dataset_write_cenDois.power_average1_hdd[dataset_write_cenDois.capacity == '80GBHDD']) /(1.08))*(dataset_write_cenDois.power_average1_hdd[dataset_write_cenDois.capacity == '80GBHDD']))+(((12.7-dataset_write_cenDois.power_average2_hdd[dataset_write_cenDois.capacity == '80GBHDD']) /(0.68))*(dataset_write_cenDois.power_average2_hdd[dataset_write_cenDois.capacity == '80GBHDD']))\n",
    "dataset_read_cenDois.power_total[dataset_read_cenDois.capacity == '80GBHDD'] = (((6.8-dataset_read_cenDois.power_average1_hdd[dataset_read_cenDois.capacity == '80GBHDD'])/(1.08))*(dataset_read_cenDois.power_average1_hdd[dataset_read_cenDois.capacity == '80GBHDD']))+(((12.7-dataset_read_cenDois.power_average2_hdd[dataset_read_cenDois.capacity == '80GBHDD'])/(0.68))*(dataset_read_cenDois.power_average2_hdd[dataset_read_cenDois.capacity == '80GBHDD']))\n",
    "dataset_mix_cenDois.power_total[dataset_mix_cenDois.capacity == '80GBHDD'] = (((6.8-dataset_mix_cenDois.power_average1_hdd[dataset_mix_cenDois.capacity == '80GBHDD'])/(1.08))*(dataset_mix_cenDois.power_average1_hdd[dataset_mix_cenDois.capacity == '80GBHDD']))+(((12.7-dataset_mix_cenDois.power_average2_hdd[dataset_mix_cenDois.capacity == '80GBHDD'])/(0.68))*(dataset_mix_cenDois.power_average2_hdd[dataset_mix_cenDois.capacity == '80GBHDD']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_teste = dataset_write_cenDois.query('capacity == \"1TBWDHDD\"')\n",
    "dataset_teste.power_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# APLICANDO O MODULO para performance\n",
    "\n",
    "## hdd 80GB\n",
    "\n",
    "dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'] = abs(dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'])\n",
    "dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'] = abs(dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'])\n",
    "dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'] = abs(dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Salvando valores de energia consumida, no csv, para adicionar na analise individual do storage hibrido\n",
    "# Para Performance\n",
    "\n",
    "dataset_write_cenUm_hdd = dataset_write_cenUm.query('capacity == \"Hybrid\"')\n",
    "dataset_read_cenUm_hdd = dataset_read_cenUm.query('capacity == \"Hybrid\"')\n",
    "dataset_mix_cenUm_hdd = dataset_mix_cenUm.query('capacity == \"Hybrid\"')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset_write_cenUm_hdd.power_total[dataset_write_cenUm_hdd.capacity == 'Hybrid'] = abs((((12.79-dataset_write_cenUm_hdd.power_average1_hdd[dataset_write_cenUm_hdd.capacity == 'Hybrid'])/(0.52))*(dataset_write_cenUm_hdd.power_average1_hdd[dataset_write_cenUm_hdd.capacity == 'Hybrid']))+(((6.78-dataset_write_cenUm_hdd.power_average2_hdd[dataset_write_cenUm_hdd.capacity == 'Hybrid'])/(0.43))*(dataset_write_cenUm_hdd.power_average2_hdd[dataset_write_cenUm_hdd.capacity == 'Hybrid'])))\n",
    "dataset_read_cenUm_hdd.power_total[dataset_read_cenUm_hdd.capacity == 'Hybrid'] = abs((((12.79-dataset_read_cenUm_hdd.power_average1_hdd[dataset_read_cenUm_hdd.capacity == 'Hybrid'])/(0.52))*(dataset_read_cenUm_hdd.power_average1_hdd[dataset_read_cenUm_hdd.capacity == 'Hybrid']))+(((6.78-dataset_read_cenUm_hdd.power_average2_hdd[dataset_read_cenUm_hdd.capacity == 'Hybrid'])/(0.43))*(dataset_read_cenUm_hdd.power_average2_hdd[dataset_read_cenUm_hdd.capacity == 'Hybrid'])))\n",
    "dataset_mix_cenUm_hdd.power_total[dataset_mix_cenUm_hdd.capacity == 'Hybrid'] = abs((((12.79-dataset_mix_cenUm_hdd.power_average1_hdd[dataset_mix_cenUm_hdd.capacity == 'Hybrid'])/(0.52))*(dataset_mix_cenUm_hdd.power_average1_hdd[dataset_mix_cenUm_hdd.capacity == 'Hybrid']))+(((6.78-dataset_mix_cenUm_hdd.power_average2_hdd[dataset_mix_cenUm_hdd.capacity == 'Hybrid'])/(0.43))*(dataset_mix_cenUm_hdd.power_average2_hdd[dataset_mix_cenUm_hdd.capacity == 'Hybrid'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTRO DE DADOS PARA Performance PARA DOE Minitab (com 20 replicates) - cenario 2\n",
    "\n",
    "scenario = '2'\n",
    "storages = ['80GBHDD', '500GBHDD', '1TBHDD', '1TBWDHDD', '120GBSSD']\n",
    "metrics = ['iops_total', 'avg_response_time_total', 'power_total', 'cpu_utilization']\n",
    "objects_size = ['4KB', '128KB', '512KB', '1MB']\n",
    "patterns = ['80R', 'R', 'S']\n",
    "replicates = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']\n",
    "work_outstd = ['1', '4', '16','64']\n",
    "operations = ['Write', 'Read', 'Mix']\n",
    "dataset_metric2 = pd.DataFrame()\n",
    "\n",
    "for metric in metrics:\n",
    "    for run in replicates:\n",
    "        for operation in operations:\n",
    "             for storage in storages:\n",
    "                    for object_size in objects_size:\n",
    "                        for pattern in patterns:\n",
    "                            for work in work_outstd:\n",
    "\n",
    "                                if operation == 'Write':\n",
    "                                    dataset_filter = dataset_write_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')             \n",
    "                                elif operation == 'Read':\n",
    "                                    dataset_filter = dataset_read_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "                                else:\n",
    "                                    dataset_filter = dataset_mix_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "\n",
    "\n",
    "                                Columns = [metric]\n",
    "                                dataset_metric = pd.DataFrame(columns=Columns)\n",
    "                                dataset_metric[metric] = dataset_filter[metric]\n",
    "                                dataset_metric2 = dataset_metric2.append(dataset_metric)\n",
    "                                #dataset_metric['repetition'] = dataset_filter.repetition\n",
    "    filename = \"Performance/\" + \"DOE/\" + scenario + \"/\" + metric + \"/\" + \"FilteredData.csv\"\n",
    "    dataset_metric2.to_csv(filename, index=False)\n",
    "    dataset_metric2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## FILTRO DE DADOS PARA O MODELO SPN 1 (easyfit)\n",
    "\n",
    "storages = ['80GBHDD', '500GBHDD', '1TBHDD', '1TBWDHDD', '120GBSSD', 'Hybrid']\n",
    "metrics = ['iops_total', 'avg_response_time_total', 'power_total']\n",
    "objects_size = ['4KB', '128KB', '512KB', '1MB']\n",
    "patterns = ['80R', 'R', 'S']\n",
    "work_outstd = ['1', '2', '4']\n",
    "operations = ['Write', 'Read', 'Mix']\n",
    "\n",
    "for operation in operations:\n",
    "    for metric in metrics:\n",
    "        for storage in storages:\n",
    "            for object_size in objects_size:\n",
    "                for pattern in patterns:\n",
    "                    for work in work_outstd:\n",
    "                        if operation == 'Write':\n",
    "                            dataset_filter = dataset_write_cenUm_SPN.query('capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')             \n",
    "                        elif operation == 'Read':\n",
    "                            dataset_filter = dataset_read_cenUm_SPN.query('capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "                        else:\n",
    "                            dataset_filter = dataset_mix_cenUm_SPN.query('capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "                        Columns = [metric,'repetition']\n",
    "                        Columns2 = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']\n",
    "                        dataset_metric = pd.DataFrame(columns=Columns)\n",
    "                        dataset_metric2 = pd.DataFrame()\n",
    "                        dataset_metric[metric] = dataset_filter[metric]\n",
    "                        dataset_metric['repetition'] = dataset_filter.repetition\n",
    "                        for col in Columns2:\n",
    "                            dataset_metric_temp = dataset_metric.query('repetition == @col')\n",
    "                            dataset_metric3 = pd.DataFrame()\n",
    "                            dataset_metric3[col] = dataset_metric_temp[metric].get_values()[:]\n",
    "                            dataset_metric2 = pd.concat([dataset_metric2,dataset_metric3], ignore_index=True, axis=1)\n",
    "                        filename = \"SPN1/\" + operation + \"/\" + metric + \"/\" + storage + \"/\" + object_size + \"/\" + pattern + \"/\" + work + \"/\" + \"FilteredData.csv\"\n",
    "                        dataset_metric2.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CENARIO 1\n",
    "\n",
    "## WRITE IOPS\n",
    "\n",
    "fig = sns.catplot(x=\"capacity\", y=\"iops_total\", hue=\"object_size\", data=dataset_write_cenUm,\n",
    "               row=\"pattern\", col=\"work_outstd\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"Capacity\", \"IOPS\")\n",
    "fig.set_xticklabels([\"80GB HDD\", \"500GB HDD\", \"1TB HDD\", \"1TB WDHDD\", \"120GB SSD\", \"Hybrid\"])\n",
    "\n",
    "plt.savefig('write_iops.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "teste = dataset_write_cenUm.query('pattern == \"R\" & work_outstd == 2 & capacity == \"1TBHDD\" & object_size == \"1MB\"')\n",
    "teste.power_average1\n",
    "\n",
    "teste.power_average1 = teste.power_average1.astype(str)\n",
    "teste['power_average1'] = teste['power_average1'].apply(lambda x: x.split(\"e\")[0])\n",
    "teste.power_average1 = teste.power_average1.astype(float)\n",
    "teste.power_average1\n",
    "\n",
    "\n",
    "## EXAMPLES\n",
    "\n",
    "#teste.power_average1.mean()\n",
    "#dataset_write_cenUm.iops_total.isnull().sum()\n",
    "#teste.power_average1[teste['power_average1'].str.contains('e')] = teste['power_average1'].apply(lambda x: x.split(\"e\")[0])\n",
    "#df['coluna'] = df['coluna'].apply(lambda x: x.split(\"%\")[1])\n",
    "#df.column = df.column.astype(float)\n",
    "#dataset_write_cenUm.iops_total[dataset_write_cenUm.iops_total != 0] = (1/dataset_write_cenUm.iops_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teste = dataset_cenUm.query('repetition==2 & individual_total == \"I\" & operation == \"W\" & pattern == \"R\" & work_outstd == 4 & capacity == \"1TBHDD\" & object_size == \"4KB\"')\n",
    "if (not teste.empty):\n",
    "    print ('eh')\n",
    "else:\n",
    "    x = teste.iops_total.get_values()[0]\n",
    "    print (x)\n",
    "    \n",
    "\n",
    "    \n",
    "           \n",
    "#teste.iops_total\n",
    "#teste.info()\n",
    "#print (teste.iops_total.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df = dataset.query('object_size == \"128KB\" & capacity == \"120GBSSD\" & scenario == 2')\n",
    "\n",
    "#df.shape\n",
    "\n",
    "dataset.info()\n",
    "\n",
    "#dataset_write_cenUm.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
