{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset specific variables (replace regular_expression by the variables of interest)\n",
    "#%reset_selective <regular_expression>\n",
    "\n",
    "# reset all variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading datasaet - AlibabaOverTime (Failurelogs)\n",
    "\n",
    "df_AlibabaOver_Failurelogs = pd.read_csv('/media/erb/hdd1/DataSet/alibabaOvertime/ssd_failure_label/ssd_failure_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset - Alibaba Snapshot (TimeStamps of failed SSDs, SMART attributes in 39 columns, SSDs location, applications, SSD models and Disk ID)\n",
    "\n",
    "df_AlibabaSnapShot_FailuresAppsLocation = pd.read_csv('/media/erb/hdd1/DataSet/alibabaSnapShot/ssd_failure_tag/ssd_failure_tag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disk_id</th>\n",
       "      <th>failure_time</th>\n",
       "      <th>model_x</th>\n",
       "      <th>model_y</th>\n",
       "      <th>app</th>\n",
       "      <th>node_id</th>\n",
       "      <th>rack_id</th>\n",
       "      <th>machine_room_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>33722</td>\n",
       "      <td>2018-01-02 19:15:32</td>\n",
       "      <td>MA2</td>\n",
       "      <td>A2</td>\n",
       "      <td>RM</td>\n",
       "      <td>66688</td>\n",
       "      <td>11488</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>58337</td>\n",
       "      <td>2018-01-02 22:45:16</td>\n",
       "      <td>MA2</td>\n",
       "      <td>A2</td>\n",
       "      <td>RM</td>\n",
       "      <td>114833</td>\n",
       "      <td>9859</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>26378</td>\n",
       "      <td>2018-01-03 03:23:44</td>\n",
       "      <td>MA1</td>\n",
       "      <td>A3</td>\n",
       "      <td>none</td>\n",
       "      <td>145735</td>\n",
       "      <td>6860</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>39876</td>\n",
       "      <td>2018-01-03 03:29:27</td>\n",
       "      <td>MA1</td>\n",
       "      <td>A3</td>\n",
       "      <td>RM</td>\n",
       "      <td>217964</td>\n",
       "      <td>17596</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13424</th>\n",
       "      <td>199348</td>\n",
       "      <td>2018-01-03 05:03:03</td>\n",
       "      <td>MC1</td>\n",
       "      <td>C1</td>\n",
       "      <td>NAS</td>\n",
       "      <td>237451</td>\n",
       "      <td>22765</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       disk_id        failure_time model_x model_y   app  node_id  rack_id  \\\n",
       "309      33722 2018-01-02 19:15:32     MA2      A2    RM    66688    11488   \n",
       "293      58337 2018-01-02 22:45:16     MA2      A2    RM   114833     9859   \n",
       "2197     26378 2018-01-03 03:23:44     MA1      A3  none   145735     6860   \n",
       "2001     39876 2018-01-03 03:29:27     MA1      A3    RM   217964    17596   \n",
       "13424   199348 2018-01-03 05:03:03     MC1      C1   NAS   237451    22765   \n",
       "\n",
       "       machine_room_id  \n",
       "309                455  \n",
       "293                455  \n",
       "2197               127  \n",
       "2001                92  \n",
       "13424              137  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging Failures and location datasets and fixing columns types (to have ssd failure data that has location, failure time, and smart att)\n",
    "df_Failurelogs_FailuresAppsLocation =  pd.merge(df_AlibabaOver_Failurelogs, df_AlibabaSnapShot_FailuresAppsLocation, how = 'inner', on = ['disk_id', 'failure_time'])\n",
    "\n",
    "# Changing failure time column to datetime type\n",
    "df_Failurelogs_FailuresAppsLocation['failure_time'] =  pd.to_datetime(df_Failurelogs_FailuresAppsLocation['failure_time'])\n",
    "\n",
    "# Removing duplicates\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.drop_duplicates(subset='disk_id', keep=\"first\")\n",
    "\n",
    "# Forcing sorting\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.sort_values(by=['failure_time'], ascending=True)\n",
    "\n",
    "# Choosing the columns of interest\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.loc[:,['disk_id','failure_time', 'model_x','model_y','app','node_id','rack_id','machine_room_id']]\n",
    "\n",
    "# Changing data type\n",
    "#df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.astype(datatype)\n",
    "\n",
    "# Testing\n",
    "df_Failurelogs_FailuresAppsLocation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Failurelogs_FailuresAppsLocation = \n",
    "\n",
    "dataset_write_cenUm = dataset_cenUm.query('operation == \"W\" & individual_total == \"T\"')\n",
    "\n",
    "dataset_write_cenUm.iops_total[dataset_write_cenUm.iops_total != 0] = (1/dataset_write_cenUm.iops_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.catplot(x=\"capacity\", y=\"iops_total\", hue=\"object_size\", data=dataset_write_cenUm,\n",
    "               row=\"pattern\", col=\"work_outstd\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"Capacity\", \"IOPS\")\n",
    "fig.set_xticklabels([\"80GB HDD\", \"500GB HDD\", \"1TB HDD\", \"1TB WDHDD\", \"120GB SSD\", \"Hybrid\"])\n",
    "\n",
    "plt.savefig('write_iops.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading AlibabaOvertime dataset using Pandas\n",
    "\n",
    "start_date = date(2018, 2, 1)\n",
    "end_date = date(2018, 3, 1)\n",
    "delta = timedelta(days=1)\n",
    "df_AlibabaOver_SMARTlogs = pd.DataFrame()\n",
    "df_AlibabaOver_SMARTlogs_Filtered = pd.DataFrame()\n",
    "reallocated_Sectors_Count = 0\n",
    "uncorrectable_errors = 0\n",
    "program_count = 0\n",
    "erase_count = 0\n",
    "nblocks_written = 0\n",
    "nblocks_read = 0\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogs = pd.read_csv(path)\n",
    "        df_AlibabaOver_SMARTlogs = pd.DataFrame(df_AlibabaOver_SMARTlogs)\n",
    "\n",
    "        # Removing duplicates\n",
    "        df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogs.drop_duplicates(subset='disk_id', keep=\"first\")\n",
    "\n",
    "        # Changing failure time column to datetime type\n",
    "        df_AlibabaOver_SMARTlogs['ds'] =  pd.to_datetime(df_AlibabaOver_SMARTlogs['ds'], format='%Y%m%d')\n",
    "\n",
    "        # Choosing the columns of interest\n",
    "        df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogs.loc[:,['disk_id','ds', 'model','n_5','n_187','n_171','n_181','n_172','n_182','n_241','n_242']]\n",
    "\n",
    "        df_AlibabaOver_SMARTlogs_Filtered = pd.concat([df_AlibabaOver_SMARTlogs_Filtered, df_AlibabaOver_SMARTlogs], ignore_index=True)\n",
    "    start_date += delta\n",
    "\n",
    "# Changing the name of some columns to clarify their meaning\n",
    "df_AlibabaOver_SMARTlogs_Filtered.rename(columns = {'ds':'timestamp','n_5':'r_sector','n_187':'u_errors','n_171':'p_failedA','n_181':'p_failedB','n_172':'e_failedA','n_182':'e_failedB','n_241':'#b_written','n_242':'#b_read'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading AlibabaOvertime dataset using Dask\n",
    "\n",
    "start_date = date(2018, 2, 1)\n",
    "start_dateAux = start_date\n",
    "end_date = date(2018, 5, 31)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogsTemp = dd.read_csv(path)\n",
    "        if start_date == start_dateAux:\n",
    "            df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogsTemp\n",
    "        else: \n",
    "            df_AlibabaOver_SMARTlogs = dd.concat([df_AlibabaOver_SMARTlogs, df_AlibabaOver_SMARTlogsTemp])\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disk_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model</th>\n",
       "      <th>r_sector</th>\n",
       "      <th>u_errors</th>\n",
       "      <th>p_failedA</th>\n",
       "      <th>p_failedB</th>\n",
       "      <th>e_failedA</th>\n",
       "      <th>e_failedB</th>\n",
       "      <th>#b_written</th>\n",
       "      <th>#b_read</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180201</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295063</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180202</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421034</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180203</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478894</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180204</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731460</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180205</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922136</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180206</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097244</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180207</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237533</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180208</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396339</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180209</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521737</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180210</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676659</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180211</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934483</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180212</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982014</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180213</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212706</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180214</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415427</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180215</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498280</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180216</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757199</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180217</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881515</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180218</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026397</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180219</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3151781</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180220</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3418531</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180221</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501430</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180222</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728541</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180223</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970038</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180224</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082891</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180225</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4187695</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180226</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4380227</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180227</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638213</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180228</td>\n",
       "      <td>MC1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4801838</th>\n",
       "      <td>100004</td>\n",
       "      <td>1970-01-01 00:00:00.020180301</td>\n",
       "      <td>MA2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         disk_id                     timestamp model  r_sector  u_errors  \\\n",
       "0         100004 1970-01-01 00:00:00.020180201   MA2     100.0     100.0   \n",
       "295063    100004 1970-01-01 00:00:00.020180202   MA2     100.0     100.0   \n",
       "421034    100004 1970-01-01 00:00:00.020180203   MA2     100.0     100.0   \n",
       "478894    100004 1970-01-01 00:00:00.020180204   MC1     100.0     100.0   \n",
       "731460    100004 1970-01-01 00:00:00.020180205   MA2     100.0     100.0   \n",
       "922136    100004 1970-01-01 00:00:00.020180206   MC1     100.0     100.0   \n",
       "1097244   100004 1970-01-01 00:00:00.020180207   MA2     100.0     100.0   \n",
       "1237533   100004 1970-01-01 00:00:00.020180208   MC1     100.0     100.0   \n",
       "1396339   100004 1970-01-01 00:00:00.020180209   MA2     100.0     100.0   \n",
       "1521737   100004 1970-01-01 00:00:00.020180210   MA2     100.0     100.0   \n",
       "1676659   100004 1970-01-01 00:00:00.020180211   MC1     100.0     100.0   \n",
       "1934483   100004 1970-01-01 00:00:00.020180212   MC1     100.0     100.0   \n",
       "1982014   100004 1970-01-01 00:00:00.020180213   MA2     100.0     100.0   \n",
       "2212706   100004 1970-01-01 00:00:00.020180214   MC1     100.0     100.0   \n",
       "2415427   100004 1970-01-01 00:00:00.020180215   MC1     100.0     100.0   \n",
       "2498280   100004 1970-01-01 00:00:00.020180216   MA2     100.0     100.0   \n",
       "2757199   100004 1970-01-01 00:00:00.020180217   MC1     100.0     100.0   \n",
       "2881515   100004 1970-01-01 00:00:00.020180218   MA2     100.0     100.0   \n",
       "3026397   100004 1970-01-01 00:00:00.020180219   MC1     100.0     100.0   \n",
       "3151781   100004 1970-01-01 00:00:00.020180220   MA2     100.0     100.0   \n",
       "3418531   100004 1970-01-01 00:00:00.020180221   MA2     100.0     100.0   \n",
       "3501430   100004 1970-01-01 00:00:00.020180222   MA2     100.0     100.0   \n",
       "3728541   100004 1970-01-01 00:00:00.020180223   MA2     100.0     100.0   \n",
       "3970038   100004 1970-01-01 00:00:00.020180224   MC1     100.0     100.0   \n",
       "4082891   100004 1970-01-01 00:00:00.020180225   MA2     100.0     100.0   \n",
       "4187695   100004 1970-01-01 00:00:00.020180226   MA2     100.0     100.0   \n",
       "4380227   100004 1970-01-01 00:00:00.020180227   MA2     100.0     100.0   \n",
       "4638213   100004 1970-01-01 00:00:00.020180228   MC1     100.0     100.0   \n",
       "4801838   100004 1970-01-01 00:00:00.020180301   MA2     100.0     100.0   \n",
       "\n",
       "         p_failedA  p_failedB  e_failedA  e_failedB  #b_written  #b_read  \n",
       "0            100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "295063       100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "421034       100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "478894       100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "731460       100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "922136       100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "1097244      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "1237533      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "1396339      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "1521737      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "1676659      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "1934483      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "1982014      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "2212706      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "2415427      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "2498280      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "2757199      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "2881515      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "3026397      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "3151781      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "3418531      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "3501430      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "3728541      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "3970038      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "4082891      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "4187695      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "4380227      100.0        NaN      100.0        NaN       100.0    100.0  \n",
       "4638213      100.0        NaN      100.0        NaN         NaN      NaN  \n",
       "4801838      100.0        NaN      100.0        NaN       100.0    100.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing Alibaba dataset\n",
    "\n",
    "# settings to display all columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "df_AlibabaOver_SMARTlogs_Filtered.query('disk_id == 100004')\n",
    "#df_AlibabaOver_Failurelogs.head(10)\n",
    "#df_AlibabaSnapShot_FailuresAppsLocation.head()\n",
    "#df_Failurelogs_FailuresAppsLocation.head(10)\n",
    "#df_Failurelogs_FailuresAppsLocation.query('model_x == \"MA2\"')\n",
    "#df_Failurelogs_FailuresAppsLocation.dtypes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Backblaze dataset\n",
    "\n",
    "start_dateBB = date(2021, 12, 29)\n",
    "start_dateAuxBB = start_dateBB\n",
    "end_dateBB = date(2021, 12, 31)\n",
    "deltaBB = timedelta(days=1)\n",
    "\n",
    "while start_dateBB <= end_dateBB:\n",
    "    pathBB = Path('/media/erb/hdd1/DataSet/backblaze/smartlogs/' + start_dateBB.strftime(\"%Y-%m-%d\") + '.csv')\n",
    "\n",
    "    if pathBB.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_BackBlaze_SMARTlogsTemp = pd.read_csv(pathBB)\n",
    "        if start_dateBB == start_dateAuxBB: # due to dask instancing of variables\n",
    "            df_BackBlaze_SMARTlogs = df_BackBlaze_SMARTlogsTemp\n",
    "        else: \n",
    "            df_BackBlaze_SMARTlogs = pd.concat([df_BackBlaze_SMARTlogs, df_BackBlaze_SMARTlogsTemp])\n",
    "    start_dateBB += deltaBB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing BackBlaze dataset\n",
    "\n",
    "df_BackBlaze_SMARTlogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alterando nome das colunas do arquivo bench e transformando a coluna timestamp no tipo datetime\n",
    "\n",
    "#PARA HDD 80GB\n",
    "\n",
    "df_80HDDBenchCenUm4KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm4KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm4KB['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinando todos os DataFrames do IOMeter\n",
    "\n",
    "#Cenario1\n",
    "\n",
    "df_IOMeterCenUm_Total = [dfTemp_80HDDMeterCenUm4KB,dfTemp_80HDDMeterCenUm128KB,dfTemp_80HDDMeterCenUm512KB,dfTemp_80HDDMeterCenUm1MB,dfTemp_500HDDMeterCenUm4KB,dfTemp_500HDDMeterCenUm128KB,dfTemp_500HDDMeterCenUm512KB,dfTemp_500HDDMeterCenUm1MB\n",
    "                         ,dfTemp_1HDDMeterCenUm4KB,dfTemp_1HDDMeterCenUm128KB,dfTemp_1HDDMeterCenUm512KB,dfTemp_1HDDMeterCenUm1MB,dfTemp_1WDHDDMeterCenUm4KB,dfTemp_1WDHDDMeterCenUm128KB,dfTemp_1WDHDDMeterCenUm512KB,dfTemp_1WDHDDMeterCenUm1MB,\n",
    "                         dfTemp_120SSDMeterCenUm4KB,dfTemp_120SSDMeterCenUm128KB,dfTemp_120SSDMeterCenUm512KB,dfTemp_120SSDMeterCenUm1MB,dfTemp_HybridMeterCenUm4KB,dfTemp_HybridMeterCenUm128KB,dfTemp_HybridMeterCenUm512KB,dfTemp_HybridMeterCenUm1MB]\n",
    "\n",
    "df_IOMeter_Total = df_IOMeterCenUm_Total + df_IOMeterCenDois_Total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.csv', 'w', newline='') as f:\n",
    "    dataset = csv.DictWriter(f, fieldnames = ['timestamp','capacity','object_size','pattern','operation','scenario','work_outstd','repetition','sample_count','individual_total','iops_total','iops_read','iops_write','avg_response_time_total','avg_response_time_read','avg_response_time_write','maximum_response_time_total','maximum_response_time_read','maximum_response_time_write','errors_total','errors_read','errors_write','cpu_utilization','power_maximum1_hdd','power_maximum2_hdd','power_average1_hdd','power_average2_hdd','power_maximum1_ssd','power_maximum2_ssd','power_average1_ssd','power_average2_ssd', 'power_total'], delimiter = ',')\n",
    "    dataset.writeheader()\n",
    "    #dataset= csv.writer(f, delimiter=',')\n",
    "    #dataset.writerow(['time_stamp','capacity','object_size','pattern','operation','scenario','work_outstd','repetition','individual_total','iops_total','iops_read','iops_write','avg_response_time_total','avg_response_time_read','avg_response_time_write','maximum_response_time_total','maximum_response_time_read','maximum_response_time_write','errors_total','errors_read','errors_write','cpu_utilization','power'])\n",
    "    #arquivosaida.writerow({1, 2}) -- inserindo dados\n",
    "    \n",
    "    row_index=0\n",
    "    workers_outstanding=1\n",
    "    encontrou = False\n",
    "    sample_count=0\n",
    "    repetition=1\n",
    "    pattern=1\n",
    "    kind='hdd'\n",
    "    aux_voltage_current= [0,0,0,0]\n",
    "    aux_voltage_total= [0,0,0,0]\n",
    "    aux_voltage_current_series = [0,0,0,0]\n",
    "    aux_voltage_current_hybrid= [0,0,0,0]\n",
    "    aux_voltage_total_hybrid= [0,0,0,0]\n",
    "    aux_voltage_current_series_hybrid = [0,0,0,0]\n",
    "    cenario_atual = 1\n",
    "    df_IOMeter_index = 0\n",
    "    \n",
    "    #indiv_total='I'\n",
    "    for df_IOMeter in df_IOMeter_Total:                  \n",
    "        \n",
    "        if (df_IOMeter_index<4 or (df_IOMeter_index>23 and df_IOMeter_index<28)):\n",
    "            capacity = '80GBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 0) or (df_IOMeter_index==24)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 1) or (df_IOMeter_index==25)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 2) or (df_IOMeter_index==26)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 3) or (df_IOMeter_index==27)):\n",
    "                object_size = '1MB'                \n",
    "        elif ((df_IOMeter_index > 3 and df_IOMeter_index < 8) or (df_IOMeter_index>27 and df_IOMeter_index<32)):\n",
    "            capacity = '500GBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 4) or (df_IOMeter_index==28)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 5) or (df_IOMeter_index==29)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 6) or (df_IOMeter_index==30)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 7) or (df_IOMeter_index==31)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 7 and df_IOMeter_index < 12) or (df_IOMeter_index>31 and df_IOMeter_index<36)):\n",
    "            capacity = '1TBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 8) or (df_IOMeter_index==32)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 9) or (df_IOMeter_index==33)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 10) or (df_IOMeter_index==34)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 11) or (df_IOMeter_index==35)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 11 and df_IOMeter_index<16) or (df_IOMeter_index>35 and df_IOMeter_index<40)):\n",
    "            capacity = '1TBWDHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 12) or (df_IOMeter_index==36)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 13) or (df_IOMeter_index==37)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 14) or (df_IOMeter_index==38)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 15) or (df_IOMeter_index==39)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 15 and df_IOMeter_index<20) or (df_IOMeter_index>39 and df_IOMeter_index<44)):\n",
    "            capacity = '120GBSSD'\n",
    "            kind = 'ssd'\n",
    "            if ((df_IOMeter_index == 16) or (df_IOMeter_index==40)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 17) or (df_IOMeter_index==41)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 18) or (df_IOMeter_index==42)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 19) or (df_IOMeter_index==43)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 19 and df_IOMeter_index<24) or (df_IOMeter_index>43 and df_IOMeter_index<48)):\n",
    "            capacity = 'Hybrid'\n",
    "            kind = 'hybrid'\n",
    "            if ((df_IOMeter_index == 20) or (df_IOMeter_index==44)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 21) or (df_IOMeter_index==45)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 22) or (df_IOMeter_index==46)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 23) or (df_IOMeter_index==47)):\n",
    "                object_size = '1MB'\n",
    "        for rows in df_IOMeter: \n",
    "            if (encontrou):\n",
    "                #print (rows[0])\n",
    "                aux_date = datetime.strptime(rows[0], '%Y-%m-%d %H:%M:%S:%f')\n",
    "                if (workers_outstanding == 1):\n",
    "                    sample_count = sample_count + 1\n",
    "                    if (rows[1] == 'ALL'):\n",
    "                        if (pattern == 1):\n",
    "                            if (kind == 'hdd'):\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_hdd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_hdd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_hdd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_hdd':(aux_voltage_total[3]/(sample_count-1))})\n",
    "                            elif (kind == 'ssd'):\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_ssd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_ssd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_ssd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_ssd':(aux_voltage_total[3]/(sample_count-1))})\n",
    "                            else:\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_ssd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_ssd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_ssd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_ssd':(aux_voltage_total[3]/(sample_count-1)),'power_maximum1_hdd':(aux_voltage_total_hybrid[0]/(sample_count-1)),'power_maximum2_hdd':(aux_voltage_total_hybrid[1]/(sample_count-1)),'power_average1_hdd':(aux_voltage_total_hybrid[2]/(sample_count-1)),'power_average2_hdd':(aux_voltage_total_hybrid[3]/(sample_count-1))})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Carregando dados filtrados em um dataframe para graficos de performance\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Separando por cenários\n",
    "\n",
    "##Cenário 1\n",
    "dataset_cenUm = dataset.query('scenario == 1')\n",
    "\n",
    "### Escrita\n",
    "dataset_write_cenUm = dataset_cenUm.query('operation == \"W\" & individual_total == \"T\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INVERTENDO COLUNA IOPS TOTAL para performance\n",
    "\n",
    "## Cenario 1\n",
    "\n",
    "dataset_write_cenUm.iops_total[dataset_write_cenUm.iops_total != 0] = (1/dataset_write_cenUm.iops_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## RETIRANDO RUÍDOS DAS MEDIÇÕES DE TENSÃO para performance\n",
    "\n",
    "##Cenário 1\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_write_cenUm['power_average1_hdd'] = dataset_write_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_write_cenUm.power_average1_hdd\n",
    "#dataset_teste = dataset_write_cenUm.query('capacity == \"120GBSSD\"')\n",
    "#dataset_teste.power_average2_ssd\n",
    "dataset_teste = dataset_write_cenDois.query('capacity == \"80GBHDD\"')\n",
    "#dataset_teste.power_average1_hdd\n",
    "dataset_teste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PARA PERFORMANCE\n",
    "## Calculando consumo de energia total hdd 80GB\n",
    "\n",
    "dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD'])/(1.08))*(dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_write_cenUm.power_average2_hdd[dataset_write_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD']))\n",
    "dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_read_cenUm.power_average1_hdd[dataset_read_cenUm.capacity == '80GBHDD']) /(1.08))*(dataset_read_cenUm.power_average1_hdd[dataset_read_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_read_cenUm.power_average2_hdd[dataset_read_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_read_cenUm.power_average2_hdd[dataset_read_cenUm.capacity == '80GBHDD']))\n",
    "dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_mix_cenUm.power_average1_hdd[dataset_mix_cenUm.capacity == '80GBHDD']) /(1.08))*(dataset_mix_cenUm.power_average1_hdd[dataset_mix_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_mix_cenUm.power_average2_hdd[dataset_mix_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_mix_cenUm.power_average2_hdd[dataset_mix_cenUm.capacity == '80GBHDD']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# APLICANDO O MODULO para performance\n",
    "\n",
    "## hdd 80GB\n",
    "\n",
    "dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'] = abs(dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'])\n",
    "dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'] = abs(dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'])\n",
    "dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'] = abs(dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTRO DE DADOS PARA Performance PARA DOE Minitab (com 20 replicates) - cenario 2\n",
    "\n",
    "scenario = '2'\n",
    "storages = ['80GBHDD', '500GBHDD', '1TBHDD', '1TBWDHDD', '120GBSSD']\n",
    "metrics = ['iops_total', 'avg_response_time_total', 'power_total', 'cpu_utilization']\n",
    "objects_size = ['4KB', '128KB', '512KB', '1MB']\n",
    "patterns = ['80R', 'R', 'S']\n",
    "replicates = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']\n",
    "work_outstd = ['1', '4', '16','64']\n",
    "operations = ['Write', 'Read', 'Mix']\n",
    "dataset_metric2 = pd.DataFrame()\n",
    "\n",
    "for metric in metrics:\n",
    "    for run in replicates:\n",
    "        for operation in operations:\n",
    "             for storage in storages:\n",
    "                    for object_size in objects_size:\n",
    "                        for pattern in patterns:\n",
    "                            for work in work_outstd:\n",
    "\n",
    "                                if operation == 'Write':\n",
    "                                    dataset_filter = dataset_write_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')             \n",
    "                                elif operation == 'Read':\n",
    "                                    dataset_filter = dataset_read_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "                                else:\n",
    "                                    dataset_filter = dataset_mix_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "\n",
    "    filename = \"Performance/\" + \"DOE/\" + scenario + \"/\" + metric + \"/\" + \"FilteredData.csv\"\n",
    "    dataset_metric2.to_csv(filename, index=False)\n",
    "    dataset_metric2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CENARIO 1\n",
    "\n",
    "## WRITE IOPS\n",
    "\n",
    "fig = sns.catplot(x=\"capacity\", y=\"iops_total\", hue=\"object_size\", data=dataset_write_cenUm,\n",
    "               row=\"pattern\", col=\"work_outstd\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"Capacity\", \"IOPS\")\n",
    "fig.set_xticklabels([\"80GB HDD\", \"500GB HDD\", \"1TB HDD\", \"1TB WDHDD\", \"120GB SSD\", \"Hybrid\"])\n",
    "\n",
    "plt.savefig('write_iops.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teste = dataset_cenUm.query('repetition==2 & individual_total == \"I\" & operation == \"W\" & pattern == \"R\" & work_outstd == 4 & capacity == \"1TBHDD\" & object_size == \"4KB\"')\n",
    "if (not teste.empty):\n",
    "    print ('eh')\n",
    "else:\n",
    "    x = teste.iops_total.get_values()[0]\n",
    "    print (x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
