{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset specific variables (replace regular_expression by the variables of interest)\n",
    "#%reset_selective <regular_expression>\n",
    "\n",
    "# reset all variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "from mongoengine import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, read_preference=Primary(), uuidrepresentation=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating/Connecting Mongo DB instances\n",
    "\n",
    "# Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "#CONNECTION_STRING = \"mongodb+srv://<jgu>:<123>@<cluster-jgu>.mongodb.net/SMARTAttributesFilter\"\n",
    "\n",
    "connect(db='SMARTAttributesFilter', alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "connect(db='FailuresAppsLocation', alias='FailuresAppsLocation_alias')\n",
    "\n",
    "connect(db='SMARTAtt_FailuresAppsLocation', alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting document schema\n",
    "\n",
    "class SMARTAtt(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     timestamp = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     p_failedA = FloatField(required=False, default='0')\n",
    "     p_failedB = FloatField(required=False, default='0')\n",
    "     e_failedA = FloatField(required=False, default='0')\n",
    "     e_failedB = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'SMARTAttributesFilter_alias'}\n",
    "\n",
    "class FailuresAppsLocation(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     machine_room_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'FailuresAppsLocation_alias'}\n",
    "\n",
    "class SMARTAtt_FailuresAppsLocation(Document):\n",
    "     smart_att = ReferenceField(SMARTAtt)\n",
    "     failures_app_location = ReferenceField(FailuresAppsLocation)\n",
    "     meta = {'db_alias': 'SMARTAtt_FailuresAppsLocation_alias'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting DB content (for the case when the goal is to test the code from zero - otherwise the db will contain several replicas)\n",
    "\n",
    "# Creating the object related to the whole collection\n",
    "#failuresAppsLocationTeste = FailuresAppsLocation.objects() \n",
    "\n",
    "# Deleting all collection\n",
    "#failuresAppsLocationTeste.delete() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the connection to the DB\n",
    "\n",
    "disconnect(alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "disconnect(alias='FailuresAppsLocation_alias_alias')\n",
    "\n",
    "disconnect(alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading datasaet - AlibabaOverTime (Failurelogs)\n",
    "\n",
    "df_AlibabaOver_Failurelogs = pd.read_csv('/media/erb/hdd1/DataSet/alibabaOvertime/ssd_failure_label/ssd_failure_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading dataset - Alibaba Snapshot (TimeStamps of failed SSDs, SMART attributes in 39 columns, SSDs location, applications, SSD models and Disk ID)\n",
    "\n",
    "df_AlibabaSnapShot_FailuresAppsLocation = pd.read_csv('/media/erb/hdd1/DataSet/alibabaSnapShot/ssd_failure_tag/ssd_failure_tag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Failures and location datasets and fixing columns types (to have ssd failure data that has location, failure time, and smart att)\n",
    "df_Failurelogs_FailuresAppsLocation =  pd.merge(df_AlibabaOver_Failurelogs, df_AlibabaSnapShot_FailuresAppsLocation, how = 'inner', on = ['disk_id', 'failure_time'])\n",
    "\n",
    "# Changing failure time column to datetime type\n",
    "df_Failurelogs_FailuresAppsLocation['failure_time'] =  pd.to_datetime(df_Failurelogs_FailuresAppsLocation['failure_time'])\n",
    "\n",
    "# Removing duplicates\n",
    "#df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.drop_duplicates(subset='disk_id', keep=\"first\")\n",
    "\n",
    "# Forcing sorting\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.sort_values(by=['failure_time'], ascending=True)\n",
    "\n",
    "# Choosing the columns of interest\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.loc[:,['disk_id','failure_time', 'model_x','model_y','app','node_id','rack_id','machine_room_id']]\n",
    "\n",
    "# Changing data type\n",
    "#df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.astype(datatype)\n",
    "\n",
    "# Testing\n",
    "#df_Failurelogs_FailuresAppsLocation.head(20)\n",
    "#df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\"'))\n",
    "#df_Failurelogs_FailuresAppsLocation.query(('app == \"RM\"'))\n",
    "#time1 = df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\" & disk_id==39876'))\n",
    "#time2 = df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\" & disk_id==22968'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting FailuresAppsLocation into the DB\n",
    "\n",
    "for row in df_Failurelogs_FailuresAppsLocation.itertuples():\n",
    "    insert_FailuresAppsLocation = FailuresAppsLocation()\n",
    "    insert_FailuresAppsLocation.disk_id = row.disk_id\n",
    "    insert_FailuresAppsLocation.failure_time = row.failure_time\n",
    "    insert_FailuresAppsLocation.model_x = row.model_x\n",
    "    insert_FailuresAppsLocation.model_y = row.model_y\n",
    "    insert_FailuresAppsLocation.app = row.app\n",
    "    insert_FailuresAppsLocation.node_id = row.node_id\n",
    "    insert_FailuresAppsLocation.rack_id = row.rack_id\n",
    "    insert_FailuresAppsLocation.machine_room_id = row.machine_room_id\n",
    "    insert_FailuresAppsLocation.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some functions in Mongoengine\n",
    "\n",
    "# Creating the object related to the whole collection\n",
    "#failuresAppsLocationTeste = FailuresAppsLocation.objects()  \n",
    "\n",
    "# Query if you know something about the document\n",
    "#testando = FailuresAppsLocation.objects(disk_id=\"33722\").get()\n",
    "\n",
    "# Printing\n",
    "#testando.disk_id\n",
    "\n",
    "# Deleting all collection\n",
    "#failuresAppsLocationTeste.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.catplot(x=\"capacity\", y=\"iops_total\", hue=\"object_size\", data=dataset_write_cenUm,\n",
    "               row=\"pattern\", col=\"work_outstd\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"Capacity\", \"IOPS\")\n",
    "fig.set_xticklabels([\"80GB HDD\", \"500GB HDD\", \"1TB HDD\", \"1TB WDHDD\", \"120GB SSD\", \"Hybrid\"])\n",
    "\n",
    "plt.savefig('write_iops.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading AlibabaOvertime dataset using Pandas\n",
    "\n",
    "start_date = date(2018, 2, 1)\n",
    "end_date = date(2018, 3, 20)\n",
    "delta = timedelta(days=1)\n",
    "df_AlibabaOver_SMARTlogs = pd.DataFrame()\n",
    "df_AlibabaOver_SMARTlogs_Filtered = pd.DataFrame()\n",
    "reallocated_Sectors_Count = 0\n",
    "uncorrectable_errors = 0\n",
    "program_count = 0\n",
    "erase_count = 0\n",
    "nblocks_written = 0\n",
    "nblocks_read = 0\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogs = pd.read_csv(path)\n",
    "        df_AlibabaOver_SMARTlogs = pd.DataFrame(df_AlibabaOver_SMARTlogs)\n",
    "\n",
    "        # Changing failure time column to datetime type\n",
    "        df_AlibabaOver_SMARTlogs['ds'] =  pd.to_datetime(df_AlibabaOver_SMARTlogs['ds'], format='%Y%m%d')\n",
    "\n",
    "        # Choosing the columns of interest\n",
    "        df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogs.loc[:,['disk_id','ds', 'model','n_5','n_187','n_171','n_181','n_172','n_182','n_241','n_242']]\n",
    "\n",
    "        # Changing the name of some columns to clarify their meaning\n",
    "        df_AlibabaOver_SMARTlogs.rename(columns = {'ds':'timestamp', 'model':'model_x', 'n_5':'r_sectors','n_187':'u_errors','n_171':'p_failedA','n_181':'p_failedB','n_172':'e_failedA','n_182':'e_failedB','n_241':'n_b_written','n_242':'n_b_read'}, inplace=True)\n",
    "\n",
    "        for row in df_AlibabaOver_SMARTlogs.itertuples():\n",
    "            insert_SmartAttributes = SMARTAtt()\n",
    "            insert_SmartAttributes.disk_id = row.disk_id\n",
    "            insert_SmartAttributes.timestamp = row.timestamp\n",
    "            insert_SmartAttributes.model_x = row.model_x\n",
    "            insert_SmartAttributes.r_sectors = row.r_sectors\n",
    "            insert_SmartAttributes.u_errors = row.u_errors\n",
    "            insert_SmartAttributes.p_failedA = row.p_failedA\n",
    "            insert_SmartAttributes.p_failedB = row.p_failedB\n",
    "            insert_SmartAttributes.e_failedA = row.e_failedA\n",
    "            insert_SmartAttributes.e_failedB = row.e_failedB\n",
    "            insert_SmartAttributes.n_b_written = row.n_b_written\n",
    "            insert_SmartAttributes.n_b_read = row.n_b_read\n",
    "            insert_SmartAttributes.save()\n",
    "\n",
    "        #df_AlibabaOver_SMARTlogs_Filtered = pd.concat([df_AlibabaOver_SMARTlogs_Filtered, df_AlibabaOver_SMARTlogs], ignore_index=True)\n",
    "    start_date += delta\n",
    "\n",
    "# Changing the name of some columns to clarify their meaning\n",
    "#df_AlibabaOver_SMARTlogs_Filtered.rename(columns = {'ds':'timestamp', 'model':'model_x', 'n_5':'r_sectors','n_187':'u_errors','n_171':'p_failedA','n_181':'p_failedB','n_172':'e_failedA','n_182':'e_failedB','n_241':'n_b_written','n_242':'n_b_read'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading AlibabaOvertime dataset using Dask\n",
    "\n",
    "start_date = date(2018, 2, 1)\n",
    "start_dateAux = start_date\n",
    "end_date = date(2018, 5, 31)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogsTemp = dd.read_csv(path)\n",
    "        if start_date == start_dateAux:\n",
    "            df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogsTemp\n",
    "        else: \n",
    "            df_AlibabaOver_SMARTlogs = dd.concat([df_AlibabaOver_SMARTlogs, df_AlibabaOver_SMARTlogsTemp])\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Alibaba's SMART attributes over two years dataset \n",
    "\n",
    "# settings to display all columns\n",
    "#pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "#df_AlibabaOver_SMARTlogs_Filtered.query('disk_id == 4711')\n",
    "#df_AlibabaOver_Failurelogs.head(10)\n",
    "#df_Failurelogs_FailuresAppsLocation.dtypes\n",
    "\n",
    "# Creating the object related to the whole collection\n",
    "SMARTAttributesTest = SMARTAtt.objects()  \n",
    "\n",
    "# Query if you know something about the document\n",
    "SMARTAttributesTest = SMARTAtt.objects(disk_id=\"4711\").filter()\n",
    "\n",
    "# Printing\n",
    "for i in SMARTAttributesTest:\n",
    "  print(i.disk_id)\n",
    "  \n",
    "# Deleting all collection\n",
    "#for i in SMARTAttributesTest:\n",
    " # i.delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Backblaze dataset\n",
    "\n",
    "start_dateBB = date(2021, 12, 29)\n",
    "start_dateAuxBB = start_dateBB\n",
    "end_dateBB = date(2021, 12, 31)\n",
    "deltaBB = timedelta(days=1)\n",
    "\n",
    "while start_dateBB <= end_dateBB:\n",
    "    pathBB = Path('/media/erb/hdd1/DataSet/backblaze/smartlogs/' + start_dateBB.strftime(\"%Y-%m-%d\") + '.csv')\n",
    "\n",
    "    if pathBB.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_BackBlaze_SMARTlogsTemp = pd.read_csv(pathBB)\n",
    "        if start_dateBB == start_dateAuxBB: # due to dask instancing of variables\n",
    "            df_BackBlaze_SMARTlogs = df_BackBlaze_SMARTlogsTemp\n",
    "        else: \n",
    "            df_BackBlaze_SMARTlogs = pd.concat([df_BackBlaze_SMARTlogs, df_BackBlaze_SMARTlogsTemp])\n",
    "    start_dateBB += deltaBB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing BackBlaze dataset\n",
    "\n",
    "df_BackBlaze_SMARTlogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alterando nome das colunas do arquivo bench e transformando a coluna timestamp no tipo datetime\n",
    "\n",
    "#PARA HDD 80GB\n",
    "\n",
    "df_80HDDBenchCenUm4KB.rename(columns = {'Time (s)':'timestamp','Maximum(1) (V)':'maximum_1','Maximum(2) (V)':'maximum_2','Average - Full Screen(1) (V)':'average_1','Average - Full Screen(2) (V)':'average_2'}, inplace=True)\n",
    "df_80HDDBenchCenUm4KB['timestamp'] =  pd.to_datetime(df_80HDDBenchCenUm4KB['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinando todos os DataFrames do IOMeter\n",
    "\n",
    "#Cenario1\n",
    "\n",
    "df_IOMeterCenUm_Total = [dfTemp_80HDDMeterCenUm4KB,dfTemp_80HDDMeterCenUm128KB,dfTemp_80HDDMeterCenUm512KB,dfTemp_80HDDMeterCenUm1MB,dfTemp_500HDDMeterCenUm4KB,dfTemp_500HDDMeterCenUm128KB,dfTemp_500HDDMeterCenUm512KB,dfTemp_500HDDMeterCenUm1MB\n",
    "                         ,dfTemp_1HDDMeterCenUm4KB,dfTemp_1HDDMeterCenUm128KB,dfTemp_1HDDMeterCenUm512KB,dfTemp_1HDDMeterCenUm1MB,dfTemp_1WDHDDMeterCenUm4KB,dfTemp_1WDHDDMeterCenUm128KB,dfTemp_1WDHDDMeterCenUm512KB,dfTemp_1WDHDDMeterCenUm1MB,\n",
    "                         dfTemp_120SSDMeterCenUm4KB,dfTemp_120SSDMeterCenUm128KB,dfTemp_120SSDMeterCenUm512KB,dfTemp_120SSDMeterCenUm1MB,dfTemp_HybridMeterCenUm4KB,dfTemp_HybridMeterCenUm128KB,dfTemp_HybridMeterCenUm512KB,dfTemp_HybridMeterCenUm1MB]\n",
    "\n",
    "df_IOMeter_Total = df_IOMeterCenUm_Total + df_IOMeterCenDois_Total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.csv', 'w', newline='') as f:\n",
    "    dataset = csv.DictWriter(f, fieldnames = ['timestamp','capacity','object_size','pattern','operation','scenario','work_outstd','repetition','sample_count','individual_total','iops_total','iops_read','iops_write','avg_response_time_total','avg_response_time_read','avg_response_time_write','maximum_response_time_total','maximum_response_time_read','maximum_response_time_write','errors_total','errors_read','errors_write','cpu_utilization','power_maximum1_hdd','power_maximum2_hdd','power_average1_hdd','power_average2_hdd','power_maximum1_ssd','power_maximum2_ssd','power_average1_ssd','power_average2_ssd', 'power_total'], delimiter = ',')\n",
    "    dataset.writeheader()\n",
    "    #dataset= csv.writer(f, delimiter=',')\n",
    "    #dataset.writerow(['time_stamp','capacity','object_size','pattern','operation','scenario','work_outstd','repetition','individual_total','iops_total','iops_read','iops_write','avg_response_time_total','avg_response_time_read','avg_response_time_write','maximum_response_time_total','maximum_response_time_read','maximum_response_time_write','errors_total','errors_read','errors_write','cpu_utilization','power'])\n",
    "    #arquivosaida.writerow({1, 2}) -- inserindo dados\n",
    "    \n",
    "    row_index=0\n",
    "    workers_outstanding=1\n",
    "    encontrou = False\n",
    "    sample_count=0\n",
    "    repetition=1\n",
    "    pattern=1\n",
    "    kind='hdd'\n",
    "    aux_voltage_current= [0,0,0,0]\n",
    "    aux_voltage_total= [0,0,0,0]\n",
    "    aux_voltage_current_series = [0,0,0,0]\n",
    "    aux_voltage_current_hybrid= [0,0,0,0]\n",
    "    aux_voltage_total_hybrid= [0,0,0,0]\n",
    "    aux_voltage_current_series_hybrid = [0,0,0,0]\n",
    "    cenario_atual = 1\n",
    "    df_IOMeter_index = 0\n",
    "    \n",
    "    #indiv_total='I'\n",
    "    for df_IOMeter in df_IOMeter_Total:                  \n",
    "        \n",
    "        if (df_IOMeter_index<4 or (df_IOMeter_index>23 and df_IOMeter_index<28)):\n",
    "            capacity = '80GBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 0) or (df_IOMeter_index==24)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 1) or (df_IOMeter_index==25)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 2) or (df_IOMeter_index==26)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 3) or (df_IOMeter_index==27)):\n",
    "                object_size = '1MB'                \n",
    "        elif ((df_IOMeter_index > 3 and df_IOMeter_index < 8) or (df_IOMeter_index>27 and df_IOMeter_index<32)):\n",
    "            capacity = '500GBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 4) or (df_IOMeter_index==28)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 5) or (df_IOMeter_index==29)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 6) or (df_IOMeter_index==30)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 7) or (df_IOMeter_index==31)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 7 and df_IOMeter_index < 12) or (df_IOMeter_index>31 and df_IOMeter_index<36)):\n",
    "            capacity = '1TBHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 8) or (df_IOMeter_index==32)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 9) or (df_IOMeter_index==33)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 10) or (df_IOMeter_index==34)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 11) or (df_IOMeter_index==35)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 11 and df_IOMeter_index<16) or (df_IOMeter_index>35 and df_IOMeter_index<40)):\n",
    "            capacity = '1TBWDHDD'\n",
    "            kind='hdd'\n",
    "            if ((df_IOMeter_index == 12) or (df_IOMeter_index==36)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 13) or (df_IOMeter_index==37)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 14) or (df_IOMeter_index==38)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 15) or (df_IOMeter_index==39)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 15 and df_IOMeter_index<20) or (df_IOMeter_index>39 and df_IOMeter_index<44)):\n",
    "            capacity = '120GBSSD'\n",
    "            kind = 'ssd'\n",
    "            if ((df_IOMeter_index == 16) or (df_IOMeter_index==40)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 17) or (df_IOMeter_index==41)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 18) or (df_IOMeter_index==42)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 19) or (df_IOMeter_index==43)):\n",
    "                object_size = '1MB'\n",
    "        elif ((df_IOMeter_index > 19 and df_IOMeter_index<24) or (df_IOMeter_index>43 and df_IOMeter_index<48)):\n",
    "            capacity = 'Hybrid'\n",
    "            kind = 'hybrid'\n",
    "            if ((df_IOMeter_index == 20) or (df_IOMeter_index==44)):\n",
    "                object_size = '4KB'\n",
    "            elif ((df_IOMeter_index == 21) or (df_IOMeter_index==45)):\n",
    "                object_size = '128KB'\n",
    "            elif ((df_IOMeter_index == 22) or (df_IOMeter_index==46)):\n",
    "                object_size = '512KB'\n",
    "            elif ((df_IOMeter_index == 23) or (df_IOMeter_index==47)):\n",
    "                object_size = '1MB'\n",
    "        for rows in df_IOMeter: \n",
    "            if (encontrou):\n",
    "                #print (rows[0])\n",
    "                aux_date = datetime.strptime(rows[0], '%Y-%m-%d %H:%M:%S:%f')\n",
    "                if (workers_outstanding == 1):\n",
    "                    sample_count = sample_count + 1\n",
    "                    if (rows[1] == 'ALL'):\n",
    "                        if (pattern == 1):\n",
    "                            if (kind == 'hdd'):\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_hdd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_hdd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_hdd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_hdd':(aux_voltage_total[3]/(sample_count-1))})\n",
    "                            elif (kind == 'ssd'):\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_ssd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_ssd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_ssd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_ssd':(aux_voltage_total[3]/(sample_count-1))})\n",
    "                            else:\n",
    "                                dataset.writerow({'timestamp': aux_date,'capacity': capacity,'object_size':object_size,'pattern':'R','operation':'W','scenario': cenario_atual,'work_outstd': workers_outstanding,'repetition': repetition,'sample_count': sample_count,'individual_total': 'T','iops_total': rows[7],'iops_read':rows[8],'iops_write':rows[9],'avg_response_time_total':rows[18],'avg_response_time_read':rows[19],'avg_response_time_write':rows[20],'maximum_response_time_total':rows[23],'maximum_response_time_read':rows[24],'maximum_response_time_write':rows[25],'errors_total':rows[28],'errors_read':rows[29],'errors_write':rows[30],'cpu_utilization':rows[49],'power_maximum1_ssd':(aux_voltage_total[0]/(sample_count-1)),'power_maximum2_ssd':(aux_voltage_total[1]/(sample_count-1)),'power_average1_ssd':(aux_voltage_total[2]/(sample_count-1)),'power_average2_ssd':(aux_voltage_total[3]/(sample_count-1)),'power_maximum1_hdd':(aux_voltage_total_hybrid[0]/(sample_count-1)),'power_maximum2_hdd':(aux_voltage_total_hybrid[1]/(sample_count-1)),'power_average1_hdd':(aux_voltage_total_hybrid[2]/(sample_count-1)),'power_average2_hdd':(aux_voltage_total_hybrid[3]/(sample_count-1))})\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Carregando dados filtrados em um dataframe para graficos de performance\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Separando por cenários\n",
    "\n",
    "##Cenário 1\n",
    "dataset_cenUm = dataset.query('scenario == 1')\n",
    "\n",
    "### Escrita\n",
    "dataset_write_cenUm = dataset_cenUm.query('operation == \"W\" & individual_total == \"T\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INVERTENDO COLUNA IOPS TOTAL para performance\n",
    "\n",
    "## Cenario 1\n",
    "\n",
    "dataset_write_cenUm.iops_total[dataset_write_cenUm.iops_total != 0] = (1/dataset_write_cenUm.iops_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## RETIRANDO RUÍDOS DAS MEDIÇÕES DE TENSÃO para performance\n",
    "\n",
    "##Cenário 1\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_write_cenUm['power_average1_hdd'] = dataset_write_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_write_cenUm.power_average1_hdd\n",
    "#dataset_teste = dataset_write_cenUm.query('capacity == \"120GBSSD\"')\n",
    "#dataset_teste.power_average2_ssd\n",
    "dataset_teste = dataset_write_cenDois.query('capacity == \"80GBHDD\"')\n",
    "#dataset_teste.power_average1_hdd\n",
    "dataset_teste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PARA PERFORMANCE\n",
    "## Calculando consumo de energia total hdd 80GB\n",
    "\n",
    "dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD'])/(1.08))*(dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_write_cenUm.power_average2_hdd[dataset_write_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_write_cenUm.power_average1_hdd[dataset_write_cenUm.capacity == '80GBHDD']))\n",
    "dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_read_cenUm.power_average1_hdd[dataset_read_cenUm.capacity == '80GBHDD']) /(1.08))*(dataset_read_cenUm.power_average1_hdd[dataset_read_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_read_cenUm.power_average2_hdd[dataset_read_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_read_cenUm.power_average2_hdd[dataset_read_cenUm.capacity == '80GBHDD']))\n",
    "dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'] = (((6.8-dataset_mix_cenUm.power_average1_hdd[dataset_mix_cenUm.capacity == '80GBHDD']) /(1.08))*(dataset_mix_cenUm.power_average1_hdd[dataset_mix_cenUm.capacity == '80GBHDD']))+(((12.7-dataset_mix_cenUm.power_average2_hdd[dataset_mix_cenUm.capacity == '80GBHDD']) /(0.68))*(dataset_mix_cenUm.power_average2_hdd[dataset_mix_cenUm.capacity == '80GBHDD']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# APLICANDO O MODULO para performance\n",
    "\n",
    "## hdd 80GB\n",
    "\n",
    "dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'] = abs(dataset_write_cenUm.power_total[dataset_write_cenUm.capacity == '80GBHDD'])\n",
    "dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'] = abs(dataset_read_cenUm.power_total[dataset_read_cenUm.capacity == '80GBHDD'])\n",
    "dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'] = abs(dataset_mix_cenUm.power_total[dataset_mix_cenUm.capacity == '80GBHDD'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTRO DE DADOS PARA Performance PARA DOE Minitab (com 20 replicates) - cenario 2\n",
    "\n",
    "scenario = '2'\n",
    "storages = ['80GBHDD', '500GBHDD', '1TBHDD', '1TBWDHDD', '120GBSSD']\n",
    "metrics = ['iops_total', 'avg_response_time_total', 'power_total', 'cpu_utilization']\n",
    "objects_size = ['4KB', '128KB', '512KB', '1MB']\n",
    "patterns = ['80R', 'R', 'S']\n",
    "replicates = ['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20']\n",
    "work_outstd = ['1', '4', '16','64']\n",
    "operations = ['Write', 'Read', 'Mix']\n",
    "dataset_metric2 = pd.DataFrame()\n",
    "\n",
    "for metric in metrics:\n",
    "    for run in replicates:\n",
    "        for operation in operations:\n",
    "             for storage in storages:\n",
    "                    for object_size in objects_size:\n",
    "                        for pattern in patterns:\n",
    "                            for work in work_outstd:\n",
    "\n",
    "                                if operation == 'Write':\n",
    "                                    dataset_filter = dataset_write_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')             \n",
    "                                elif operation == 'Read':\n",
    "                                    dataset_filter = dataset_read_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "                                else:\n",
    "                                    dataset_filter = dataset_mix_cenDois.query('repetition == @run & scenario == @scenario & capacity == @storage & object_size == @object_size & pattern == @pattern & work_outstd == @work')\n",
    "\n",
    "    filename = \"Performance/\" + \"DOE/\" + scenario + \"/\" + metric + \"/\" + \"FilteredData.csv\"\n",
    "    dataset_metric2.to_csv(filename, index=False)\n",
    "    dataset_metric2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CENARIO 1\n",
    "\n",
    "## WRITE IOPS\n",
    "\n",
    "fig = sns.catplot(x=\"capacity\", y=\"iops_total\", hue=\"object_size\", data=dataset_write_cenUm,\n",
    "               row=\"pattern\", col=\"work_outstd\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"Capacity\", \"IOPS\")\n",
    "fig.set_xticklabels([\"80GB HDD\", \"500GB HDD\", \"1TB HDD\", \"1TB WDHDD\", \"120GB SSD\", \"Hybrid\"])\n",
    "\n",
    "plt.savefig('write_iops.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "teste = dataset_cenUm.query('repetition==2 & individual_total == \"I\" & operation == \"W\" & pattern == \"R\" & work_outstd == 4 & capacity == \"1TBHDD\" & object_size == \"4KB\"')\n",
    "if (not teste.empty):\n",
    "    print ('eh')\n",
    "else:\n",
    "    x = teste.iops_total.get_values()[0]\n",
    "    print (x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
