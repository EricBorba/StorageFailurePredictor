{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset specific variables (replace regular_expression by the variables of interest)\n",
    "#%reset_selective <regular_expression>\n",
    "\n",
    "# reset all variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from array import *\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import datetime as dt\n",
    "from pymongo import MongoClient\n",
    "from mongoengine import *\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, read_preference=Primary(), uuidrepresentation=3)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating/Connecting Mongo DB instances\n",
    "\n",
    "# Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "#CONNECTION_STRING = \"mongodb+srv://<jgu>:<123>@<cluster-jgu>.mongodb.net/SMARTAttributesFilter\"\n",
    "\n",
    "connect(db='SMARTAttributesFilter', alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "connect(db='OverTimeSSDsFailures', alias='OverTimeSSDsFailures_alias')\n",
    "\n",
    "connect(db='AllAppsSSDsFailures', alias='AllAppsSSDsFailures_alias')\n",
    "\n",
    "connect(db='AllAppsSSDsLocation', alias='AllAppsSSDsLocation_alias')\n",
    "\n",
    "connect(db='FailuresAppsLocation', alias='FailuresAppsLocation_alias')\n",
    "\n",
    "connect(db='SMARTAtt_FailuresAppsLocation', alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting document schema\n",
    "\n",
    "# SMART attributes and disk information from the 2 years daily sampling dataset (500k disks)\n",
    "class SMARTAtt(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     timestamp = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     p_failedA = FloatField(required=False, default='0')\n",
    "     p_failedB = FloatField(required=False, default='0')\n",
    "     e_failedA = FloatField(required=False, default='0')\n",
    "     e_failedB = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'SMARTAttributesFilter_alias'}\n",
    "\n",
    "# Failure time and disk information from the 2 years daily sampling dataset (500k disks)\n",
    "class OverTimeSSDsFailures(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     meta = {'db_alias': 'OverTimeSSDsFailures_alias'}\n",
    "\n",
    "# Failure time and disk information (without model) from the full datset (1M disks)\n",
    "class AllAppsSSDsFailures(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     machine_room_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'AllAppsSSDsFailures_alias'}\n",
    "\n",
    "# Apps and disks characteristics from the full dataset (1M disks)\n",
    "class AllAppsSSDsLocation(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     slot_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'AllAppsSSDsLocation_alias'}\n",
    "\n",
    "# Merge of OverTimeSSDsFailures and AllAppsSSDsFailures documents\n",
    "class FailuresAppsLocation(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     machine_room_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'FailuresAppsLocation_alias'}\n",
    "\n",
    "class SMARTAtt_FailuresAppsLocation(Document):\n",
    "     smart_att = ReferenceField(SMARTAtt)\n",
    "     failures_app_location = ReferenceField(FailuresAppsLocation)\n",
    "     meta = {'db_alias': 'SMARTAtt_FailuresAppsLocation_alias'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the connection to the DB\n",
    "\n",
    "#disconnect(alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "#disconnect(alias='FailuresAppsLocation_alias_alias')\n",
    "\n",
    "#disconnect(alias='FailuresAppsLocation_alias_alias')\n",
    "\n",
    "#disconnect(alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some queries in Mongoengine\n",
    "\n",
    "## Creating the object related to the whole collection\n",
    "#failuresAppsLocationTeste = FailuresAppsLocation.objects()\n",
    "\n",
    "## Collecting the applications\n",
    "#apps = FailuresAppsLocation.objects().distinct(\"app\")\n",
    "#print(apps)\n",
    "\n",
    "## Looping inside dcuments, which are filtered for the kind o app\n",
    "#for app_c in apps:\n",
    "#   dataPerApp = FailuresAppsLocation.objects(app=app_c).filter()\n",
    "   #dataPerAppToList = list(dataPerApp)\n",
    "   #dataPerAppDf = pd.DataFrame.from_dict(dataPerAppToList)\n",
    "     \n",
    "#   for doc in dataPerApp:\n",
    "#      arrayForDataPerApp = np.append(arrayForDataPerApp, doc.to_json())\n",
    "\n",
    "##The collection size\n",
    "#len(failuresAppsLocationTeste)\n",
    "\n",
    "## Query if you know something about the document\n",
    "#testando = FailuresAppsLocation.objects(disk_id=\"33722\").get()\n",
    "\n",
    "## Query if you know something about the document\n",
    "#SMARTAttributesTest = SMARTAtt.objects(timestamp=\"2019-12-31\").filter().limit(20)\n",
    "#SMARTAttributesTest = SMARTAtt.objects(timestamp=\"2019-12-31\").filter()\n",
    "\n",
    "## Printing\n",
    "#for i in SMARTAttributesTest:\n",
    "#    print(i.timestamp)\n",
    "    \n",
    "## Deleting all collection\n",
    "#failuresAppsLocationTeste.delete() \n",
    "  \n",
    "##Deleting all collection using for loop\n",
    "#for i in failuresAppsLocationTeste:\n",
    " # i.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some queries pandas\n",
    "\n",
    "## Find an specific line based on a specific value from a column\n",
    "#df_failuresLocationDocumentsJsonMTTF.loc[df_failuresLocationDocumentsJsonMTTF[\"mtff_node\"] == 2] \n",
    "#d1 = data[data[\"City\"] == \"Houston\"]\n",
    "#dataFrame.Reg_Price[i]\n",
    "\n",
    "## Ploting very basic graphs\n",
    "#df.groupby('Sex').sum().plot(kind='bar');\n",
    "#print(df_app)\n",
    "#df_appNode.groupby(['app']).plot(kind='bar')\n",
    "#df.groupby(['Sex', 'Survived'] )['Survived'].count().plot.bar(figsize=(8, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying/Reading the OverTimeSSDsFailures database (mongodb) and turning It into a pandas dataframe\n",
    "\n",
    "doc_AlibabaOver_Failurelogs = OverTimeSSDsFailures.objects()\n",
    "\n",
    "jSon_AlibabaOver_Failurelogs = json.loads(doc_AlibabaOver_Failurelogs.to_json())\n",
    "df_AlibabaOver_Failurelogs = pd.DataFrame.from_dict(jSon_AlibabaOver_Failurelogs) \n",
    "\n",
    "# Modifying the mongo db date type to some human-readable type\n",
    "dicDateToString = json.dumps(list(df_AlibabaOver_Failurelogs['failure_time']))\n",
    "dicStringToJson = json.loads(dicDateToString)\n",
    "dicJsonToDf = pd.DataFrame.from_dict(dicStringToJson)\n",
    "df_AlibabaOver_Failurelogs['failure_time'] = dicJsonToDf['$date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying/Reading the AllAppsSSDsFailures database (mongodb) and turning It into a pandas dataframe\n",
    "\n",
    "doc_AlibabaSnapShot_FailuresAppsLocation = AllAppsSSDsFailures.objects()\n",
    "\n",
    "jSon_AlibabaSnapShot_FailuresAppsLocation = json.loads(doc_AlibabaSnapShot_FailuresAppsLocation.to_json())\n",
    "df_AlibabaSnapShot_FailuresAppsLocation = pd.DataFrame.from_dict(jSon_AlibabaSnapShot_FailuresAppsLocation) \n",
    "\n",
    "# Modifying the mongo db date type to some human-readable type\n",
    "dicDateToString = json.dumps(list(df_AlibabaSnapShot_FailuresAppsLocation['failure_time']))\n",
    "dicStringToJson = json.loads(dicDateToString)\n",
    "dicJsonToDf = pd.DataFrame.from_dict(dicStringToJson)\n",
    "df_AlibabaSnapShot_FailuresAppsLocation['failure_time'] = dicJsonToDf['$date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying/Reading the AllAppsSSDsLocation database (mongodb)\n",
    "\n",
    "doc_AlibabaSnapShot_AllAppsSSDsLocation = AllAppsSSDsLocation.objects()\n",
    "\n",
    "jSon_AlibabaSnapShot_AllAppsSSDsLocation = json.loads(doc_AlibabaSnapShot_AllAppsSSDsLocation.to_json())\n",
    "df_AlibabaSnapShot_AllAppsSSDsLocation = pd.DataFrame.from_dict(jSon_AlibabaSnapShot_AllAppsSSDsLocation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying/Reading the FailuresAppsLocation database (mongodb) and turning It into a pandas dataframe - DB which merges both failures dataset\n",
    "\n",
    "doc_MergeDs_FailuresAppsLocation = FailuresAppsLocation.objects()\n",
    "\n",
    "jSon_MergeDs_FailuresAppsLocation = json.loads(doc_MergeDs_FailuresAppsLocation.to_json())\n",
    "df_MergeDs_FailuresAppsLocation = pd.DataFrame.from_dict(jSon_MergeDs_FailuresAppsLocation) \n",
    "\n",
    "# Modifying the mongo db date type to some human-readable type\n",
    "dicDateToString = json.dumps(list(df_MergeDs_FailuresAppsLocation['failure_time']))\n",
    "dicStringToJson = json.loads(dicDateToString)\n",
    "dicJsonToDf = pd.DataFrame.from_dict(dicStringToJson)\n",
    "df_MergeDs_FailuresAppsLocation['failure_time'] = dicJsonToDf['$date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting failure time columns to hours (for SSDs in common dataset)\n",
    "\n",
    "# creating a temporary variable to be used to calculate the general mttf equation approach\n",
    "df_genMTTF_MergeDs_FailuresAppsLocation = df_MergeDs_FailuresAppsLocation\n",
    "\n",
    "## Value representing: 2018-01-01 00:00:00 (starting date from the experiment)\n",
    "#1514764800000\n",
    "\n",
    "#Subctracting by the initial time of the experiment and turning into hours\n",
    "df_genMTTF_MergeDs_FailuresAppsLocation['failure_time'] = df_MergeDs_FailuresAppsLocation.failure_time.sub(1514764800000)\n",
    "df_genMTTF_MergeDs_FailuresAppsLocation['failure_time'] = df_MergeDs_FailuresAppsLocation.failure_time.div(1000)\n",
    "df_genMTTF_MergeDs_FailuresAppsLocation['failure_time'] = df_MergeDs_FailuresAppsLocation.failure_time.div(60)\n",
    "df_genMTTF_MergeDs_FailuresAppsLocation['failure_time'] = df_MergeDs_FailuresAppsLocation.failure_time.div(60)\n",
    "#df_failuresLocationDocumentsJsonMTTF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting failure time columns to hours (for All failed SSDs dataset)\n",
    "\n",
    "#Subctracting by the initial time of the experiment and turning into hours\n",
    "df_AlibabaSnapShot_FailuresAppsLocation['failure_time'] = df_AlibabaSnapShot_FailuresAppsLocation.failure_time.sub(1514764800000)\n",
    "df_AlibabaSnapShot_FailuresAppsLocation['failure_time'] = df_AlibabaSnapShot_FailuresAppsLocation.failure_time.div(1000)\n",
    "df_AlibabaSnapShot_FailuresAppsLocation['failure_time'] = df_AlibabaSnapShot_FailuresAppsLocation.failure_time.div(60)\n",
    "df_AlibabaSnapShot_FailuresAppsLocation['failure_time'] = df_AlibabaSnapShot_FailuresAppsLocation.failure_time.div(60)\n",
    "#df_AlibabaSnapShot_FailuresAppsLocation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_id                3016\n",
       "disk_id            3016\n",
       "failure_time       3016\n",
       "app                3016\n",
       "node_id            3016\n",
       "rack_id            3016\n",
       "machine_room_id    3016\n",
       "dtype: int64"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AlibabaSnapShot_FailuresAppsLocation.loc[df_AlibabaSnapShot_FailuresAppsLocation['app'] == 'RM'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating MTTF (general equation) per node, rack and general using failed SSDs in common\n",
    "\n",
    "# Unique values by column\n",
    "id_apps = df_genMTTF_MergeDs_FailuresAppsLocation.app.unique()\n",
    "#id_nodes = df_failuresLocationDocumentsJsonMTTF.node_id.unique()\n",
    "#id_racks = nodes = df_failuresLocationDocumentsJsonMTTF.rack_id.unique()\n",
    "#id_ssds = nodes = df_failuresLocationDocumentsJsonMTTF.disk_id.unique()\n",
    "\n",
    "#Calculating the MTTFs\n",
    "#df_general = df_failuresLocationDocumentsJsonMTTF['failure_time'].mean()\n",
    "df_appNode = df_genMTTF_MergeDs_FailuresAppsLocation.groupby(['app', 'node_id'])['failure_time'].mean()\n",
    "df_appNode = pd.DataFrame(df_appNode)\n",
    "df_appNode.rename(columns = {'failure_time':'mttf_appNode'}, inplace=True)\n",
    "df_appRack = df_genMTTF_MergeDs_FailuresAppsLocation.groupby(['app', 'rack_id'])['failure_time'].mean()\n",
    "df_appRack = pd.DataFrame(df_appRack)\n",
    "df_appRack.rename(columns = {'failure_time':'mttf_appRack'}, inplace=True)\n",
    "df_app = df_genMTTF_MergeDs_FailuresAppsLocation.groupby(['app'])['failure_time'].mean()\n",
    "df_app = pd.DataFrame(df_app)\n",
    "df_app.rename(columns = {'failure_time':'mttf_app'}, inplace=True)\n",
    "#df_all = df_failuresLocationDocumentsJsonMTTF.groupby(['app', 'rack_id', 'node_id'])['failure_time'].mean()\n",
    "\n",
    "# Merging the results with the original dataframe\n",
    "df_result = pd.merge(df_genMTTF_MergeDs_FailuresAppsLocation, df_appNode, how='left', on=['app', 'node_id'])\n",
    "df_result = pd.merge(df_result, df_appRack, how='left', on=['app', 'rack_id'])\n",
    "df_result = pd.merge(df_result, df_app, how='left', on=['app'])\n",
    "\n",
    "# Selecting the columns of interest\n",
    "df_resultFiltered = df_result.loc[:,['app','node_id', 'rack_id','mttf_appNode','mttf_appRack','mttf_app']]\n",
    "#df_resultFiltered.to_csv('failuresLocationApp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app</th>\n",
       "      <th>N_AllSSDsApp</th>\n",
       "      <th>N_failed_SDDs</th>\n",
       "      <th>AFR_SDDs</th>\n",
       "      <th>mttf_SDDs</th>\n",
       "      <th>AFR_SDDs_total</th>\n",
       "      <th>mttf_SDDs_total</th>\n",
       "      <th>N_failed_CommonSDDs</th>\n",
       "      <th>AFR_CommonSDDs</th>\n",
       "      <th>mttf_CommonSDDs</th>\n",
       "      <th>AFR_CommonSDDs_total</th>\n",
       "      <th>mttf_CommonSDDs_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RM</td>\n",
       "      <td>183981.0</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>0.008196</td>\n",
       "      <td>1.054109e+08</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>5.531749e+08</td>\n",
       "      <td>2602.0</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>1.221826e+08</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>6.411896e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>none</td>\n",
       "      <td>248757.0</td>\n",
       "      <td>3552.0</td>\n",
       "      <td>0.007139</td>\n",
       "      <td>1.210169e+08</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>4.697003e+08</td>\n",
       "      <td>2966.0</td>\n",
       "      <td>0.005962</td>\n",
       "      <td>1.449265e+08</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>5.625001e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NAS</td>\n",
       "      <td>14454.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>0.018715</td>\n",
       "      <td>4.616730e+07</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>3.083873e+09</td>\n",
       "      <td>541.0</td>\n",
       "      <td>0.018715</td>\n",
       "      <td>4.616730e+07</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>3.083873e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WSM</td>\n",
       "      <td>380170.0</td>\n",
       "      <td>8916.0</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>7.368032e+07</td>\n",
       "      <td>0.004617</td>\n",
       "      <td>1.871215e+08</td>\n",
       "      <td>8161.0</td>\n",
       "      <td>0.010733</td>\n",
       "      <td>8.049672e+07</td>\n",
       "      <td>0.004226</td>\n",
       "      <td>2.044327e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DB</td>\n",
       "      <td>26781.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>2.279683e+08</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>8.218598e+09</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>4.095360e+08</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>1.476438e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DAE</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.037937</td>\n",
       "      <td>2.277430e+07</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>1.374280e+09</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.037937</td>\n",
       "      <td>2.277430e+07</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>1.374280e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WPS</td>\n",
       "      <td>44676.0</td>\n",
       "      <td>529.0</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>1.459360e+08</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>3.153829e+09</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.005551</td>\n",
       "      <td>1.556454e+08</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>3.363660e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SS</td>\n",
       "      <td>32936.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>3.093120e+08</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>9.067257e+09</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>3.769100e+08</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.104884e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WS</td>\n",
       "      <td>17740.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.006539</td>\n",
       "      <td>1.321324e+08</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>7.191273e+09</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>5.025364e+08</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>2.735042e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    app  N_AllSSDsApp  N_failed_SDDs  AFR_SDDs     mttf_SDDs  AFR_SDDs_total  \\\n",
       "0    RM      183981.0         3016.0  0.008196  1.054109e+08        0.001562   \n",
       "1  none      248757.0         3552.0  0.007139  1.210169e+08        0.001839   \n",
       "2   NAS       14454.0          541.0  0.018715  4.616730e+07        0.000280   \n",
       "3   WSM      380170.0         8916.0  0.011726  7.368032e+07        0.004617   \n",
       "4    DB       26781.0          203.0  0.003790  2.279683e+08        0.000105   \n",
       "5   DAE       16000.0         1214.0  0.037937  2.277430e+07        0.000629   \n",
       "6   WPS       44676.0          529.0  0.005920  1.459360e+08        0.000274   \n",
       "7    SS       32936.0          184.0  0.002793  3.093120e+08        0.000095   \n",
       "8    WS       17740.0          232.0  0.006539  1.321324e+08        0.000120   \n",
       "\n",
       "   mttf_SDDs_total  N_failed_CommonSDDs  AFR_CommonSDDs  mttf_CommonSDDs  \\\n",
       "0     5.531749e+08               2602.0        0.007071     1.221826e+08   \n",
       "1     4.697003e+08               2966.0        0.005962     1.449265e+08   \n",
       "2     3.083873e+09                541.0        0.018715     4.616730e+07   \n",
       "3     1.871215e+08               8161.0        0.010733     8.049672e+07   \n",
       "4     8.218598e+09                113.0        0.002110     4.095360e+08   \n",
       "5     1.374280e+09               1214.0        0.037937     2.277430e+07   \n",
       "6     3.153829e+09                496.0        0.005551     1.556454e+08   \n",
       "7     9.067257e+09                151.0        0.002292     3.769100e+08   \n",
       "8     7.191273e+09                 61.0        0.001719     5.025364e+08   \n",
       "\n",
       "   AFR_CommonSDDs_total  mttf_CommonSDDs_total  \n",
       "0              0.001347           6.411896e+08  \n",
       "1              0.001536           5.625001e+08  \n",
       "2              0.000280           3.083873e+09  \n",
       "3              0.004226           2.044327e+08  \n",
       "4              0.000059           1.476438e+10  \n",
       "5              0.000629           1.374280e+09  \n",
       "6              0.000257           3.363660e+09  \n",
       "7              0.000078           1.104884e+10  \n",
       "8              0.000032           2.735042e+10  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To investigate the SSDs by application\n",
    "\n",
    "exp_years = 2\n",
    "exp_months = 12*exp_years\n",
    "hours_per_year = 360*24\n",
    "\n",
    "#Creating a dataframe to generate some statistics taking into account the application which is running\n",
    "df_idApps = pd.DataFrame(id_apps, columns=[\"app\"])\n",
    "#Adding the empty colum ssds_number into the dataframe above\n",
    "df_idAppsSSDs = pd.DataFrame(df_idApps, columns=[\"app\", \"N_AllSSDsApp\", \"N_failed_SDDs\",  \"AFR_SDDs\", \"mttf_SDDs\", \"AFR_SDDs_total\", \"mttf_SDDs_total\", \"N_failed_CommonSDDs\",  \"AFR_CommonSDDs\", \"mttf_CommonSDDs\",\"AFR_CommonSDDs_total\", \"mttf_CommonSDDs_total\"])\n",
    "\n",
    "\n",
    "for i in id_apps:\n",
    "    #Looping the AlibabaSnapshot_AllAppsSSDsLocation DB in order to count the total of ssds by application    \n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_AllSSDsApp'] = df_AlibabaSnapShot_AllAppsSSDsLocation.loc[(df_AlibabaSnapShot_AllAppsSSDsLocation['app']) == i]['disk_id'].count()\n",
    "    \n",
    "    #Looping the FailuresAppsLocation (merged dss) in order to count the number of failed ssds by application    \n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_failed_CommonSDDs'] = df_MergeDs_FailuresAppsLocation.loc[(df_MergeDs_FailuresAppsLocation['app']) == i]['disk_id'].count()\n",
    "    #Looping the AllAppsSSDsFailures DB in order to count the number of failed ssds by application    \n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_failed_SDDs'] = df_AlibabaSnapShot_FailuresAppsLocation.loc[(df_AlibabaSnapShot_FailuresAppsLocation['app']) == i]['disk_id'].count()\n",
    "    \n",
    "    # Calculating specific (failedSSDsperApp/TotalSSDsperAPP) app AFR and MTTF using failed ssds from the Alibaba Snapshot database (AllAppsSSDsFailures)\n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'AFR_SDDs'] = ((df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_failed_SDDs'])*(12/exp_months))/(df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_AllSSDsApp'])\n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'mttf_SDDs'] = (hours_per_year)/(df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'AFR_SDDs'])*100\n",
    "    # Calculating specific (failedSSDsCommonperApp/TotalSSDsCommonperApp) app AFR and MTTF using failed ssds in common to the Snapshot and Overtime databases (FailuresAppsLocation)\n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'AFR_CommonSDDs'] = ((df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_failed_CommonSDDs'])*(12/exp_months))/(df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_AllSSDsApp'])\n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'mttf_CommonSDDs'] = (hours_per_year)/(df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'AFR_CommonSDDs'])*100\n",
    "    \n",
    "for i in id_apps:\n",
    "    # Calculating total (failedSSDsperApp/TotalSSDsExperiment) AFR and MTTF using failed ssds from the Alibaba Snapshot database (AllAppsSSDsFailures)\n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'AFR_SDDs_total'] = ((df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_failed_SDDs'])*(12/exp_months))/(df_idAppsSSDs.N_AllSSDsApp.sum())\n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'mttf_SDDs_total'] = (hours_per_year)/(df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'AFR_SDDs_total'])*100\n",
    "    # Calculating total (failedSSDsCommonperApp/TotalSSDsExperiment) AFR and MTTF using failed ssds in common to the Snapshot and Overtime databases (FailuresAppsLocation)\n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'AFR_CommonSDDs_total'] = ((df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'N_failed_CommonSDDs'])*(12/exp_months))/(df_idAppsSSDs.N_AllSSDsApp.sum())\n",
    "    df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'mttf_CommonSDDs_total'] = (hours_per_year)/(df_idAppsSSDs.loc[(df_idAppsSSDs['app']) == i, 'AFR_CommonSDDs_total'])*100\n",
    "\n",
    "#Merging mttf per app to include the MTTF (this calculated using the general MTTF equation - not storage's specific) column into the failed ssds by application dataframe\n",
    "#df_idAppsSSDsMTTF = pd.merge(df_idAppsSSDs, df_app, how='left', on=['app'])\n",
    "#df_idAppsSSDsMTTF.head(20)\n",
    "\n",
    "df_idAppsSSDs.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FailedSSDsSet</th>\n",
       "      <th>AFR_General</th>\n",
       "      <th>MTTF_General</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FailedSSDs</td>\n",
       "      <td>0.009522</td>\n",
       "      <td>9.073668e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FailedCommonSSDs</td>\n",
       "      <td>0.008444</td>\n",
       "      <td>1.023229e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FailedSSDsSet  AFR_General  MTTF_General\n",
       "0        FailedSSDs     0.009522  9.073668e+07\n",
       "1  FailedCommonSSDs     0.008444  1.023229e+08"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To investigate the SDDs without app distinguishing\n",
    "\n",
    "#Creating a data frame to calculate the AFR and MTTF without app distinguishing.\n",
    "data = {'FailedSSDsSet':['FailedSSDs', 'FailedCommonSSDs']}\n",
    "df_general_AFRMTTF = pd.DataFrame(data)\n",
    "df_general_AFRMTTF = pd.DataFrame(df_general_AFRMTTF, columns=['FailedSSDsSet', 'AFR_General', 'MTTF_General'])\n",
    "\n",
    "# Calculating general (failedSSDs/TotalSSDsExperiment) AFR and MTTF using failed ssds from the Alibaba Snapshot database (AllAppsSSDsFailures)\n",
    "df_general_AFRMTTF.loc[(df_general_AFRMTTF['FailedSSDsSet']) == 'FailedSSDs', 'AFR_General'] = ((df_idAppsSSDs.N_failed_SDDs.sum())*(12/exp_months))/(df_idAppsSSDs.N_AllSSDsApp.sum())\n",
    "df_general_AFRMTTF.loc[(df_general_AFRMTTF['FailedSSDsSet']) == 'FailedSSDs', 'MTTF_General'] = (hours_per_year)/(df_general_AFRMTTF.loc[(df_general_AFRMTTF['FailedSSDsSet']) == 'FailedSSDs', 'AFR_General'])*100\n",
    "# Calculating general AFR and MTTF using failed ssds in common to the Snapshot and Overtime databases (FailuresAppsLocation)\n",
    "df_general_AFRMTTF.loc[(df_general_AFRMTTF['FailedSSDsSet']) == 'FailedCommonSSDs', 'AFR_General'] = ((df_idAppsSSDs.N_failed_CommonSDDs.sum())*(12/exp_months))/(df_idAppsSSDs.N_AllSSDsApp.sum())\n",
    "df_general_AFRMTTF.loc[(df_general_AFRMTTF['FailedSSDsSet']) == 'FailedCommonSSDs', 'MTTF_General'] = (hours_per_year)/(df_general_AFRMTTF.loc[(df_general_AFRMTTF['FailedSSDsSet']) == 'FailedCommonSSDs', 'AFR_General'])*100\n",
    "df_general_AFRMTTF.head()\n",
    "#df_idAppsSSDs.N_AllSSDsApp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving sorted failure time per app into csv to be used for probability distribution fitting (SSDs in common)\n",
    "\n",
    "for i in id_apps:\n",
    "    \n",
    "    df_merge_toFitting_times = df_genMTTF_MergeDs_FailuresAppsLocation.loc[df_genMTTF_MergeDs_FailuresAppsLocation[\"app\"] == i, \"failure_time\"]\n",
    "    df_merge_toFitting_times.sort_values\n",
    "    df_merge_toFitting_times.to_csv('MergeSSDs_FailureTimesApp' + i + '.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving sorted failure time per app into csv to be used for probability distribution fitting (All failed SDDs)\n",
    "\n",
    "for i in id_apps:\n",
    "    \n",
    "    df_alibabasnapshot_toFitting = df_AlibabaSnapShot_FailuresAppsLocation.loc[df_AlibabaSnapShot_FailuresAppsLocation[\"app\"] == i, \"failure_time\"]\n",
    "    df_alibabasnapshot_toFitting.sort_values\n",
    "    df_alibabasnapshot_toFitting.to_csv('Snapshot_FailureTimesApp' + i + '.csv', header=None, index=None)\n",
    "\n",
    "df_alibabasnapshot_toFitting = df_AlibabaSnapShot_FailuresAppsLocation['failure_time']\n",
    "df_alibabasnapshot_toFitting.to_csv('Snapshot_FailureTimesApp' + 'All' + '.csv', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating Flash technology (using general mttf equation approach)\n",
    "\n",
    "flashtech = pd.DataFrame({\n",
    "'model_y':['A3', 'A2', 'B3' , 'B2', 'C1', 'C2'],  \n",
    "'flash': ['MLC', 'MLC', 'MLC' , 'MLC', '3D-TLC', '3D-TLC']})\n",
    "\n",
    "df_flashtech = pd.merge(df_result, flashtech, how='left', on=['model_y'])\n",
    "\n",
    "df_flashtech = df_flashtech.groupby(['app','flash'])['failure_time'].mean()\n",
    "\n",
    "df_flashtech = pd.DataFrame(df_flashtech)\n",
    "\n",
    "df_flashtech.rename(columns = {'failure_time':'mttf_flash'}, inplace=True)\n",
    "\n",
    "df_flashtech.to_csv('flashtech.csv')\n",
    "#df_flashtech.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating Capacity (using general mttf equation approach)\n",
    "\n",
    "capacity = pd.DataFrame({\n",
    "'model_y':['A3', 'A2', 'B3' , 'B2', 'C1', 'C2'],  \n",
    "'capacity': ['480GB', '800GB', '1920GB' , '1920GB', '1920GB', '960GB']})\n",
    "\n",
    "df_capacity = pd.merge(df_result, capacity, how='left', on=['model_y'])\n",
    "\n",
    "df_capacity = df_capacity.groupby(['app','capacity'])['failure_time'].mean()\n",
    "\n",
    "df_capacity = pd.DataFrame(df_capacity)\n",
    "\n",
    "df_capacity.rename(columns = {'failure_time':'mttf_capacity'}, inplace=True)\n",
    "\n",
    "df_capacity.to_csv('capacity.csv')\n",
    "#df_capacity.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  Creating Graphs (cell still in progress)\n",
    "\n",
    "fig = sns.catplot(x=\"app\", y=\"failure_time\", hue=\"model_x\", data=df_failuresLocationDocumentsJson,\n",
    "               row=\"node_id\", col=\"rack_id\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"App\", \"MTTF\")\n",
    "fig.set_xticklabels([\"DAE\", \"DB\", \"NAS\", \"RM\", \"SS\", \"WPS\", \"WS\", \"WSM\", \"none\"])\n",
    "\n",
    "plt.savefig('teste.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying/Reading the SMARTAtt database (mongodb) filtering by a specific SSD (all cells below still in progress)\n",
    "\n",
    "smartDocuments = SMARTAtt.objects(disk_id=\"19015\").filter()\n",
    "\n",
    "jSon_smartDocumentsJson = json.loads(smartDocuments.to_json())\n",
    "df_smartDocumentsJson = pd.DataFrame.from_dict(jSon_smartDocumentsJson) \n",
    "\n",
    "df_smartDocumentsJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsDB = SMARTAtt.objects(disk_id=\"95341\").filter()\n",
    "\n",
    "jSon_smartDocumentsDBJson = json.loads(smartDocumentsDB.to_json())\n",
    "df_smartDocumentsDBJson = pd.DataFrame.from_dict(jSon_smartDocumentsDBJson) \n",
    "\n",
    "df_smartDocumentsDBJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsNAS = SMARTAtt.objects(disk_id=\"18144\").filter()\n",
    "\n",
    "jSon_smartDocumentsNASJson = json.loads(smartDocumentsNAS.to_json())\n",
    "df_smartDocumentsNASJson = pd.DataFrame.from_dict(jSon_smartDocumentsNASJson) \n",
    "\n",
    "df_smartDocumentsNASJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsRM = SMARTAtt.objects(disk_id=\"99516\").filter()\n",
    "\n",
    "jSon_smartDocumentsRMJson = json.loads(smartDocumentsRM.to_json())\n",
    "df_smartDocumentsRMJson = pd.DataFrame.from_dict(jSon_smartDocumentsRMJson) \n",
    "\n",
    "df_smartDocumentsRMJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsSS = SMARTAtt.objects(disk_id=\"199807\").filter()\n",
    "\n",
    "jSon_smartDocumentsSSJson = json.loads(smartDocumentsSS.to_json())\n",
    "df_smartDocumentsSSJson = pd.DataFrame.from_dict(jSon_smartDocumentsSSJson) \n",
    "\n",
    "df_smartDocumentsSSJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsWPS = SMARTAtt.objects(disk_id=\"30010\").filter()\n",
    "\n",
    "jSon_smartDocumentsWPSJson = json.loads(smartDocumentsWPS.to_json())\n",
    "df_smartDocumentsWPSJson = pd.DataFrame.from_dict(jSon_smartDocumentsWPSJson) \n",
    "\n",
    "df_smartDocumentsWPSJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsWS = SMARTAtt.objects(disk_id=\"17304\").filter()\n",
    "\n",
    "jSon_smartDocumentsWSJson = json.loads(smartDocumentsWS.to_json())\n",
    "df_smartDocumentsWSJson = pd.DataFrame.from_dict(jSon_smartDocumentsWSJson) \n",
    "\n",
    "df_smartDocumentsWSJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsWSM = SMARTAtt.objects(disk_id=\"29755\").filter()\n",
    "\n",
    "jSon_smartDocumentsWSMJson = json.loads(smartDocumentsWSM.to_json())\n",
    "df_smartDocumentsWSMJson = pd.DataFrame.from_dict(jSon_smartDocumentsWSMJson) \n",
    "\n",
    "df_smartDocumentsWSMJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsnone = SMARTAtt.objects(disk_id=\"12463\").filter()\n",
    "\n",
    "jSon_smartDocumentsnoneJson = json.loads(smartDocumentsnone.to_json())\n",
    "df_smartDocumentsnoneJson = pd.DataFrame.from_dict(jSon_smartDocumentsnoneJson) \n",
    "\n",
    "df_smartDocumentsnoneJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mergin all smart attributes dataframes\n",
    "df_smartDocumentsTotal = pd.concat([df_smartDocumentsJson, df_smartDocumentsDBJson, df_smartDocumentsNASJson, df_smartDocumentsRMJson, df_smartDocumentsSSJson, df_smartDocumentsWPSJson, df_smartDocumentsWSJson, df_smartDocumentsWSMJson, df_smartDocumentsnoneJson])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the columns of interest\n",
    "df_smartDocumentsTotalFiltered = df_smartDocumentsTotal.loc[:,['disk_id', 'timestamp' , 'r_sectors', 'u_errors','n_b_written','n_b_read']]\n",
    "\n",
    "dicDateToString = json.dumps(list(df_smartDocumentsTotalFiltered['timestamp']))\n",
    "dicStringToJson = json.loads(dicDateToString)\n",
    "dicJsonToDf = pd.DataFrame.from_dict(dicStringToJson)\n",
    "df_smartDocumentsTotalFiltered['timestamp'] = dicJsonToDf['$date']\n",
    "\n",
    "#Subctracting by the initial time of the experiment and turning into hours\n",
    "df_smartDocumentsTotalFiltered['timestamp'] = df_smartDocumentsTotalFiltered.timestamp.sub(1514764800000)\n",
    "df_smartDocumentsTotalFiltered['timestamp'] = df_smartDocumentsTotalFiltered.timestamp.div(1000)\n",
    "df_smartDocumentsTotalFiltered['timestamp'] = df_smartDocumentsTotalFiltered.timestamp.div(60)\n",
    "\n",
    "# Relating to the correspodent app\n",
    "appAtt = pd.DataFrame({\n",
    "'disk_id':[19015, 95341, 18144 , 99516, 199807, 30010, 17304, 29755, 12463],  \n",
    "'app': ['DAE', 'DB', 'NAS', 'RM', 'SS', 'WPS', 'WS', 'WSM', 'none']})\n",
    "\n",
    "df_smartDocumentsTotalFiltered = pd.merge(df_smartDocumentsTotalFiltered, appAtt, how='left', on=['disk_id'])\n",
    "\n",
    "df_smartDocumentsTotalFiltered.to_csv('smartattributesanalysisMinutesApp.csv')\n",
    "#df_smartDocumentsTotalFiltered.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smartDocumentsTotalFiltered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failuresLocationDocumentsJson.to_csv('failuresLocation.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
