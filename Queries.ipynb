{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset specific variables (replace regular_expression by the variables of interest)\n",
    "#%reset_selective <regular_expression>\n",
    "\n",
    "# reset all variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from array import *\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import datetime as dt\n",
    "from pymongo import MongoClient\n",
    "from mongoengine import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, read_preference=Primary(), uuidrepresentation=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating/Connecting Mongo DB instances\n",
    "\n",
    "# Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "#CONNECTION_STRING = \"mongodb+srv://<jgu>:<123>@<cluster-jgu>.mongodb.net/SMARTAttributesFilter\"\n",
    "\n",
    "connect(db='SMARTAttributesFilter', alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "connect(db='FailuresAppsLocation', alias='FailuresAppsLocation_alias')\n",
    "\n",
    "connect(db='SMARTAtt_FailuresAppsLocation', alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting document schema\n",
    "\n",
    "class SMARTAtt(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     timestamp = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     p_failedA = FloatField(required=False, default='0')\n",
    "     p_failedB = FloatField(required=False, default='0')\n",
    "     e_failedA = FloatField(required=False, default='0')\n",
    "     e_failedB = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'SMARTAttributesFilter_alias'}\n",
    "\n",
    "class FailuresAppsLocation(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     machine_room_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'FailuresAppsLocation_alias'}\n",
    "\n",
    "class SMARTAtt_FailuresAppsLocation(Document):\n",
    "     smart_att = ReferenceField(SMARTAtt)\n",
    "     failures_app_location = ReferenceField(FailuresAppsLocation)\n",
    "     meta = {'db_alias': 'SMARTAtt_FailuresAppsLocation_alias'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the connection to the DB\n",
    "\n",
    "disconnect(alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "disconnect(alias='FailuresAppsLocation_alias_alias')\n",
    "\n",
    "disconnect(alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some queries in Mongoengine\n",
    "\n",
    "## Creating the object related to the whole collection\n",
    "#failuresAppsLocationTeste = FailuresAppsLocation.objects()\n",
    "\n",
    "## Collecting the applications\n",
    "#apps = FailuresAppsLocation.objects().distinct(\"app\")\n",
    "#print(apps)\n",
    "\n",
    "## Looping inside dcuments, which are filtered for the kind o app\n",
    "#for app_c in apps:\n",
    "#   dataPerApp = FailuresAppsLocation.objects(app=app_c).filter()\n",
    "   #dataPerAppToList = list(dataPerApp)\n",
    "   #dataPerAppDf = pd.DataFrame.from_dict(dataPerAppToList)\n",
    "     \n",
    "#   for doc in dataPerApp:\n",
    "#      arrayForDataPerApp = np.append(arrayForDataPerApp, doc.to_json())\n",
    "\n",
    "##The collection size\n",
    "#len(failuresAppsLocationTeste)\n",
    "\n",
    "## Query if you know something about the document\n",
    "#testando = FailuresAppsLocation.objects(disk_id=\"33722\").get()\n",
    "\n",
    "## Query if you know something about the document\n",
    "#SMARTAttributesTest = SMARTAtt.objects(timestamp=\"2019-12-31\").filter().limit(20)\n",
    "#SMARTAttributesTest = SMARTAtt.objects(timestamp=\"2019-12-31\").filter()\n",
    "\n",
    "## Printing\n",
    "#for i in SMARTAttributesTest:\n",
    "#    print(i.timestamp)\n",
    "    \n",
    "## Deleting all collection\n",
    "#failuresAppsLocationTeste.delete() \n",
    "  \n",
    "##Deleting all collection using for loop\n",
    "#for i in failuresAppsLocationTeste:\n",
    " # i.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some queries pandas\n",
    "\n",
    "## Find an specific line based on a specific value from a column\n",
    "#df_failuresLocationDocumentsJsonMTTF.loc[df_failuresLocationDocumentsJsonMTTF[\"mtff_node\"] == 2] \n",
    "#d1 = data[data[\"City\"] == \"Houston\"]\n",
    "#dataFrame.Reg_Price[i]\n",
    "\n",
    "## Ploting very basic graphs\n",
    "#df.groupby('Sex').sum().plot(kind='bar');\n",
    "#print(df_app)\n",
    "#df_appNode.groupby(['app']).plot(kind='bar')\n",
    "#df.groupby(['Sex', 'Survived'] )['Survived'].count().plot.bar(figsize=(8, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying/Reading the whole FailuresAppsLocation database (mongodb) and, turning It into a pandas dataframe\n",
    "\n",
    "failuresLocationDocuments = FailuresAppsLocation.objects()\n",
    "\n",
    "jSon_failuresLocationDocumentsJson = json.loads(failuresLocationDocuments.to_json())\n",
    "df_failuresLocationDocumentsJson = pd.DataFrame.from_dict(jSon_failuresLocationDocumentsJson) \n",
    "\n",
    "# Changing the mongo db date type to some human-readable type\n",
    "dicDateToString = json.dumps(list(df_failuresLocationDocumentsJson['failure_time']))\n",
    "dicStringToJson = json.loads(dicDateToString)\n",
    "dicJsonToDf = pd.DataFrame.from_dict(dicStringToJson)\n",
    "df_failuresLocationDocumentsJson['failure_time'] = dicJsonToDf['$date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10415, 1)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculating MTTF per node, rack and general\n",
    "\n",
    "df_failuresLocationDocumentsJsonMTTF = df_failuresLocationDocumentsJson\n",
    "\n",
    "## Value representing: 2018-01-01 00:00:00 (starting date from the experiment)\n",
    "#1514764800000\n",
    "\n",
    "#Subctracting by the initial time of the experiment and turning into hours\n",
    "df_failuresLocationDocumentsJsonMTTF['failure_time'] = df_failuresLocationDocumentsJson.failure_time.sub(1514764800000)\n",
    "df_failuresLocationDocumentsJsonMTTF['failure_time'] = df_failuresLocationDocumentsJson.failure_time.div(1000)\n",
    "df_failuresLocationDocumentsJsonMTTF['failure_time'] = df_failuresLocationDocumentsJson.failure_time.div(60)\n",
    "df_failuresLocationDocumentsJsonMTTF['failure_time'] = df_failuresLocationDocumentsJson.failure_time.div(60)\n",
    "#df_failuresLocationDocumentsJsonMTTF.head(3)\n",
    "\n",
    "# Number of unique values by column\n",
    "#id_apps = df_failuresLocationDocumentsJsonMTTF.app.unique()\n",
    "#id_nodes = df_failuresLocationDocumentsJsonMTTF.node_id.unique()\n",
    "#id_racks = nodes = df_failuresLocationDocumentsJsonMTTF.rack_id.unique()\n",
    "#id_ssds = nodes = df_failuresLocationDocumentsJsonMTTF.disk_id.unique()\n",
    "\n",
    "\n",
    "#Calculating the MTTFs\n",
    "#df_general = df_failuresLocationDocumentsJsonMTTF['failure_time'].mean()\n",
    "df_appNode = df_failuresLocationDocumentsJsonMTTF.groupby(['app', 'node_id'])['failure_time'].mean()\n",
    "df_appNode = pd.DataFrame(df_appNode)\n",
    "df_appNode.rename(columns = {'failure_time':'mttf_appNode'}, inplace=True)\n",
    "df_appRack = df_failuresLocationDocumentsJsonMTTF.groupby(['app', 'rack_id'])['failure_time'].mean()\n",
    "df_appRack = pd.DataFrame(df_appRack)\n",
    "df_appRack.rename(columns = {'failure_time':'mttf_appRack'}, inplace=True)\n",
    "df_app = df_failuresLocationDocumentsJsonMTTF.groupby(['app'])['failure_time'].mean()\n",
    "df_app = pd.DataFrame(df_app)\n",
    "df_app.rename(columns = {'failure_time':'mttf_app'}, inplace=True)\n",
    "#df_all = df_failuresLocationDocumentsJsonMTTF.groupby(['app', 'rack_id', 'node_id'])['failure_time'].mean()\n",
    "\n",
    "# Merging the results with the original dataframe\n",
    "df_result = pd.merge(df_failuresLocationDocumentsJsonMTTF, df_appNode, how='left', on=['app', 'node_id'])\n",
    "df_result = pd.merge(df_result, df_appRack, how='left', on=['app', 'rack_id'])\n",
    "df_result = pd.merge(df_result, df_app, how='left', on=['app'])\n",
    "\n",
    "# Selecting the columns of interest\n",
    "df_resultFiltered = df_result.loc[:,['app','node_id', 'rack_id','mttf_appNode','mttf_appRack','mttf_app']]\n",
    "df_resultFiltered.to_csv('failuresLocationApp.csv')\n",
    "\n",
    "\n",
    "\n",
    "#df_appNode.columns = ['app', 'node_id', 'mttf_appNode']\n",
    "#df_appNode.head(3)\n",
    "#df_appNode = df_appNode.loc[df_appNode[\"app\"] == \"DAE\"]\n",
    "#df_appNode = df_appNode['mttf_appNode'].mean() \n",
    "#print(df_appSSD)\n",
    "\n",
    "#df_appSSD = df_failuresLocationDocumentsJsonMTTF.groupby(['app','disk_id'])\n",
    "#df_appSSD = pd.DataFrame(df_appSSD)\n",
    "#df_appSSD = df_appSSD[df_appSSD[\"app\" == \"DAE\"]].value_counts()\n",
    "\n",
    "#df_appSSD.rename(columns = {'failure_time':'mttf_appSSD'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating Flash technology\n",
    "\n",
    "flashtech = pd.DataFrame({\n",
    "'model_y':['A3', 'A2', 'B3' , 'B2', 'C1', 'C2'],  \n",
    "'flash': ['MLC', 'MLC', 'MLC' , 'MLC', '3D-TLC', '3D-TLC']})\n",
    "\n",
    "df_flashtech = pd.merge(df_result, flashtech, how='left', on=['model_y'])\n",
    "\n",
    "df_flashtech = df_flashtech.groupby(['app','flash'])['failure_time'].mean()\n",
    "\n",
    "df_flashtech = pd.DataFrame(df_flashtech)\n",
    "\n",
    "df_flashtech.rename(columns = {'failure_time':'mttf_flash'}, inplace=True)\n",
    "\n",
    "df_flashtech.to_csv('flashtech.csv')\n",
    "#df_flashtech.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating Capacity\n",
    "\n",
    "capacity = pd.DataFrame({\n",
    "'model_y':['A3', 'A2', 'B3' , 'B2', 'C1', 'C2'],  \n",
    "'capacity': ['480GB', '800GB', '1920GB' , '1920GB', '1920GB', '960GB']})\n",
    "\n",
    "df_capacity = pd.merge(df_result, capacity, how='left', on=['model_y'])\n",
    "\n",
    "df_capacity = df_capacity.groupby(['app','capacity'])['failure_time'].mean()\n",
    "\n",
    "df_capacity = pd.DataFrame(df_capacity)\n",
    "\n",
    "df_capacity.rename(columns = {'failure_time':'mttf_capacity'}, inplace=True)\n",
    "\n",
    "df_capacity.to_csv('capacity.csv')\n",
    "#df_capacity.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##  Creating Graphs\n",
    "\n",
    "fig = sns.catplot(x=\"app\", y=\"failure_time\", hue=\"model_x\", data=df_failuresLocationDocumentsJson,\n",
    "               row=\"node_id\", col=\"rack_id\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"App\", \"MTTF\")\n",
    "fig.set_xticklabels([\"DAE\", \"DB\", \"NAS\", \"RM\", \"SS\", \"WPS\", \"WS\", \"WSM\", \"none\"])\n",
    "\n",
    "plt.savefig('teste.pdf', dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying/Reading the SMARTAtt database (mongodb) filtering by a specific SSD\n",
    "\n",
    "smartDocuments = SMARTAtt.objects(disk_id=\"19015\").filter()\n",
    "\n",
    "jSon_smartDocumentsJson = json.loads(smartDocuments.to_json())\n",
    "df_smartDocumentsJson = pd.DataFrame.from_dict(jSon_smartDocumentsJson) \n",
    "\n",
    "df_smartDocumentsJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsDB = SMARTAtt.objects(disk_id=\"95341\").filter()\n",
    "\n",
    "jSon_smartDocumentsDBJson = json.loads(smartDocumentsDB.to_json())\n",
    "df_smartDocumentsDBJson = pd.DataFrame.from_dict(jSon_smartDocumentsDBJson) \n",
    "\n",
    "df_smartDocumentsDBJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsNAS = SMARTAtt.objects(disk_id=\"18144\").filter()\n",
    "\n",
    "jSon_smartDocumentsNASJson = json.loads(smartDocumentsNAS.to_json())\n",
    "df_smartDocumentsNASJson = pd.DataFrame.from_dict(jSon_smartDocumentsNASJson) \n",
    "\n",
    "df_smartDocumentsNASJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsRM = SMARTAtt.objects(disk_id=\"99516\").filter()\n",
    "\n",
    "jSon_smartDocumentsRMJson = json.loads(smartDocumentsRM.to_json())\n",
    "df_smartDocumentsRMJson = pd.DataFrame.from_dict(jSon_smartDocumentsRMJson) \n",
    "\n",
    "df_smartDocumentsRMJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsSS = SMARTAtt.objects(disk_id=\"199807\").filter()\n",
    "\n",
    "jSon_smartDocumentsSSJson = json.loads(smartDocumentsSS.to_json())\n",
    "df_smartDocumentsSSJson = pd.DataFrame.from_dict(jSon_smartDocumentsSSJson) \n",
    "\n",
    "df_smartDocumentsSSJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsWPS = SMARTAtt.objects(disk_id=\"30010\").filter()\n",
    "\n",
    "jSon_smartDocumentsWPSJson = json.loads(smartDocumentsWPS.to_json())\n",
    "df_smartDocumentsWPSJson = pd.DataFrame.from_dict(jSon_smartDocumentsWPSJson) \n",
    "\n",
    "df_smartDocumentsWPSJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsWS = SMARTAtt.objects(disk_id=\"17304\").filter()\n",
    "\n",
    "jSon_smartDocumentsWSJson = json.loads(smartDocumentsWS.to_json())\n",
    "df_smartDocumentsWSJson = pd.DataFrame.from_dict(jSon_smartDocumentsWSJson) \n",
    "\n",
    "df_smartDocumentsWSJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsWSM = SMARTAtt.objects(disk_id=\"29755\").filter()\n",
    "\n",
    "jSon_smartDocumentsWSMJson = json.loads(smartDocumentsWSM.to_json())\n",
    "df_smartDocumentsWSMJson = pd.DataFrame.from_dict(jSon_smartDocumentsWSMJson) \n",
    "\n",
    "df_smartDocumentsWSMJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smartDocumentsnone = SMARTAtt.objects(disk_id=\"12463\").filter()\n",
    "\n",
    "jSon_smartDocumentsnoneJson = json.loads(smartDocumentsnone.to_json())\n",
    "df_smartDocumentsnoneJson = pd.DataFrame.from_dict(jSon_smartDocumentsnoneJson) \n",
    "\n",
    "df_smartDocumentsnoneJson.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mergin all smart attributes dataframes\n",
    "df_smartDocumentsTotal = pd.concat([df_smartDocumentsJson, df_smartDocumentsDBJson, df_smartDocumentsNASJson, df_smartDocumentsRMJson, df_smartDocumentsSSJson, df_smartDocumentsWPSJson, df_smartDocumentsWSJson, df_smartDocumentsWSMJson, df_smartDocumentsnoneJson])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the columns of interest\n",
    "df_smartDocumentsTotalFiltered = df_smartDocumentsTotal.loc[:,['disk_id', 'timestamp' , 'r_sectors', 'u_errors','n_b_written','n_b_read']]\n",
    "\n",
    "dicDateToString = json.dumps(list(df_smartDocumentsTotalFiltered['timestamp']))\n",
    "dicStringToJson = json.loads(dicDateToString)\n",
    "dicJsonToDf = pd.DataFrame.from_dict(dicStringToJson)\n",
    "df_smartDocumentsTotalFiltered['timestamp'] = dicJsonToDf['$date']\n",
    "\n",
    "#Subctracting by the initial time of the experiment and turning into hours\n",
    "df_smartDocumentsTotalFiltered['timestamp'] = df_smartDocumentsTotalFiltered.timestamp.sub(1514764800000)\n",
    "df_smartDocumentsTotalFiltered['timestamp'] = df_smartDocumentsTotalFiltered.timestamp.div(1000)\n",
    "df_smartDocumentsTotalFiltered['timestamp'] = df_smartDocumentsTotalFiltered.timestamp.div(60)\n",
    "\n",
    "# Relating to the correspodent app\n",
    "appAtt = pd.DataFrame({\n",
    "'disk_id':[19015, 95341, 18144 , 99516, 199807, 30010, 17304, 29755, 12463],  \n",
    "'app': ['DAE', 'DB', 'NAS', 'RM', 'SS', 'WPS', 'WS', 'WSM', 'none']})\n",
    "\n",
    "df_smartDocumentsTotalFiltered = pd.merge(df_smartDocumentsTotalFiltered, appAtt, how='left', on=['disk_id'])\n",
    "\n",
    "df_smartDocumentsTotalFiltered.to_csv('smartattributesanalysisMinutesApp.csv')\n",
    "#df_smartDocumentsTotalFiltered.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smartDocumentsTotalFiltered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_failuresLocationDocumentsJson.to_csv('failuresLocation.csv', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
