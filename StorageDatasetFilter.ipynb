{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset specific variables (replace regular_expression by the variables of interest)\n",
    "#%reset_selective <regular_expression>\n",
    "\n",
    "# reset all variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "from mongoengine import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, read_preference=Primary(), uuidrepresentation=3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating/Connecting Mongo DB instances\n",
    "\n",
    "# Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "#CONNECTION_STRING = \"mongodb+srv://<jgu>:<123>@<cluster-jgu>.mongodb.net/SMARTAttributesFilter\"\n",
    "\n",
    "connect(db='SMARTAttributesFilter', alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "connect(db='SMARTAttributesFilterOverWear', alias='SMARTAttributesFilterOverWear_alias')\n",
    "\n",
    "connect(db='SMARTAttributesFilterFull', alias='SMARTAttributesFilterFull_alias')\n",
    "\n",
    "connect(db='OverTimeSSDsFailures', alias='OverTimeSSDsFailures_alias')\n",
    "\n",
    "connect(db='AllAppsSSDsFailures', alias='AllAppsSSDsFailures_alias')\n",
    "\n",
    "connect(db='AllAppsSSDsLocation', alias='AllAppsSSDsLocation_alias')\n",
    "\n",
    "connect(db='AllDiskIDSMARTAttributes', alias='AllDiskIDSMARTAttributes_alias')\n",
    "\n",
    "connect(db='AllDiskIDSMARTAttributesFirstDay', alias='AllDiskIDSMARTAttributesFirstDay_alias')\n",
    "\n",
    "connect(db='FailuresAppsLocation', alias='FailuresAppsLocation_alias')\n",
    "\n",
    "connect(db='SMARTAtt_FailuresAppsLocation', alias='SMARTAtt_FailuresAppsLocation_alias')\n",
    "\n",
    "connect(db='SMARTAttFullBackBlaze', alias='SMARTAttFullBackBlaze_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting document schema\n",
    "\n",
    "# SMART attributes and disk information from the 2 years daily sampling dataset (500k disks)\n",
    "class SMARTAtt(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     timestamp = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     p_failedA = FloatField(required=False, default='0')\n",
    "     p_failedB = FloatField(required=False, default='0')\n",
    "     e_failedA = FloatField(required=False, default='0')\n",
    "     e_failedB = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'SMARTAttributesFilter_alias'}\n",
    "\n",
    "# SMART attributes (related to wear) and disk information from the 2 years daily sampling dataset (500k disks)\n",
    "class SMARTAttOverWear(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     timestamp = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     w_l_count = FloatField(required=False, default='0')\n",
    "     w_r_d = FloatField(required=False, default='0')\n",
    "     media_wearout_i = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'SMARTAttributesFilterOverWear_alias'}\n",
    "\n",
    "\n",
    "# SMART attributes and disk information from the 2 years daily sampling dataset (500k disks)\n",
    "class SMARTAttFull(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     timestamp = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     power_hours = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     p_failedA = FloatField(required=False, default='0')\n",
    "     p_failedB = FloatField(required=False, default='0')\n",
    "     e_failedA = FloatField(required=False, default='0')\n",
    "     e_failedB = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     w_l_count = FloatField(required=False, default='0')\n",
    "     w_r_d = FloatField(required=False, default='0')\n",
    "     media_wearout_i = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'SMARTAttributesFilterFull_alias'}\n",
    "\n",
    "\n",
    "# Failure time and disk information from the 2 years daily sampling dataset (500k disks)\n",
    "class OverTimeSSDsFailures(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     meta = {'db_alias': 'OverTimeSSDsFailures_alias'}\n",
    "\n",
    "# Failure time and disk information (without model) from the full datset (1M disks)\n",
    "class AllAppsSSDsFailures(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     machine_room_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'AllAppsSSDsFailures_alias'}\n",
    "\n",
    "# Apps and disks characteristics from the full dataset (1M disks)\n",
    "class AllAppsSSDsLocation(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     slot_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'AllAppsSSDsLocation_alias'}\n",
    "\n",
    "# SMART attributes from the full dataset (1M disks)\n",
    "class AllDiskIDSMARTAttributes(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     p_on = FloatField(required=False, default='0')\n",
    "     p_c_count = FloatField(required=False, default='0')\n",
    "     p_failedA = FloatField(required=False, default='0')\n",
    "     p_failedB = FloatField(required=False, default='0')\n",
    "     e_failedA = FloatField(required=False, default='0')\n",
    "     e_failedB = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     w_l_count = FloatField(required=False, default='0')\n",
    "     w_r_d = FloatField(required=False, default='0')\n",
    "     media_wearout_i = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'AllDiskIDSMARTAttributes_alias'}\n",
    "\n",
    "# First Day SMART attributes from the full dataset (1M disks)\n",
    "class AllDiskIDSMARTAttributesFirstDay(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     p_on = FloatField(required=False, default='0')\n",
    "     p_c_count = FloatField(required=False, default='0')\n",
    "     p_failedA = FloatField(required=False, default='0')\n",
    "     p_failedB = FloatField(required=False, default='0')\n",
    "     e_failedA = FloatField(required=False, default='0')\n",
    "     e_failedB = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     w_l_count = FloatField(required=False, default='0')\n",
    "     w_r_d = FloatField(required=False, default='0')\n",
    "     media_wearout_i = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'AllDiskIDSMARTAttributesFirstDay_alias'}\n",
    "\n",
    "# Merge of OverTimeSSDsFailures and AllAppsSSDsFailures documents\n",
    "class FailuresAppsLocation(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     machine_room_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'FailuresAppsLocation_alias'}\n",
    "\n",
    "class SMARTAtt_FailuresAppsLocation(Document):\n",
    "     smart_att = ReferenceField(SMARTAtt)\n",
    "     failures_app_location = ReferenceField(FailuresAppsLocation)\n",
    "     meta = {'db_alias': 'SMARTAtt_FailuresAppsLocation_alias'}\n",
    "\n",
    "class SMARTAttFullBackBlaze(Document):\n",
    "     timestamp = DateTimeField(required=False, default='0')\n",
    "     disk_id = StringField(required=False, default='0')\n",
    "     model = StringField(required=False, default='0')\n",
    "     capacity_bytes = FloatField(required=False, default='0')\n",
    "     failure = FloatField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     power_hours = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     read_error_rate = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     command_timeout = FloatField(required=False, default='0')\n",
    "     current_pending_sector_count = FloatField(required=False, default='0')\n",
    "     uncorrectable_sector_count = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'SMARTAttFullBackBlaze_alias'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting DB content (for the case when the goal is to test the code from zero - otherwise the db will contain several replicas)\n",
    "\n",
    "# Creating the object related to the whole collection\n",
    "#failuresAppsLocationTeste = FailuresAppsLocation.objects() \n",
    "\n",
    "# Deleting all collection\n",
    "#failuresAppsLocationTeste.delete() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the connection to the DB\n",
    "\n",
    "#disconnect(alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "#disconnect(alias='SMARTAttributesFilterOverWear_alias')\n",
    "\n",
    "#disconnect(alias='OverTimeSSDsFailures_alias')\n",
    "\n",
    "#disconnect(alias='AllAppsSSDsFailures_alias')\n",
    "\n",
    "#disconnect(alias='AllAppsSSDsLocation_alias')\n",
    "\n",
    "#disconnect(alias='AllDiskIDSMARTAttributes_alias')\n",
    "\n",
    "#disconnect(alias='FailuresAppsLocation_alias')\n",
    "\n",
    "#disconnect(alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading dataset - AlibabaOverTime (Failurelogs)\n",
    "\n",
    "df_AlibabaOver_Failurelogs = pd.read_csv('/media/erb/hdd1/DataSet/alibabaOvertime/ssd_failure_label/ssd_failure_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading dataset - Alibaba Snapshot (TimeStamps of failed SSDs, SMART attributes in 39 columns, SSDs location, applications, SSD models and Disk ID)\n",
    "\n",
    "df_AlibabaSnapShot_FailuresAppsLocation = pd.read_csv('/media/erb/hdd1/DataSet/alibabaSnapShot/ssd_failure_tag/ssd_failure_tag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading dataset - Alibaba Snapshot (Location of all SSDs, applications, SSD models, rack and Disk ID)\n",
    "\n",
    "df_AlibabaSnapShot_AllAppsSSDsLocation = pd.read_csv('/media/erb/hdd1/DataSet/alibabaSnapShot/location_info_of_ssd/location_info_of_ssd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading dataset - Alibaba Snapshot (SMART attributes)\n",
    "\n",
    "df_AlibabaSnapShot_SMARTAttributes = pd.read_csv('/media/erb/hdd1/DataSet/alibabaSnapShot/smart_log_20191231/20191231.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading dataset - Alibaba Snapshot (SMART attributes)\n",
    "\n",
    "df_AlibabaSnapShot_SMARTAttributesFirstDay = pd.read_csv('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/20180104.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16305"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging Failures and location datasets and fixing columns types (to have ssd failure data that has location, failure time, and smart att)\n",
    "df_MergeDs_FailuresAppsLocation =  pd.merge(df_AlibabaOver_Failurelogs, df_AlibabaSnapShot_FailuresAppsLocation, how = 'inner', on = ['disk_id', 'failure_time'])\n",
    "\n",
    "# Changing failure time column to datetime type\n",
    "df_MergeDs_FailuresAppsLocation['failure_time'] =  pd.to_datetime(df_MergeDs_FailuresAppsLocation['failure_time'])\n",
    "\n",
    "# Removing duplicates\n",
    "#df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.drop_duplicates(subset=['disk_id','failure_time'], keep=\"first\")\n",
    "\n",
    "# Forcing sorting\n",
    "df_MergeDs_FailuresAppsLocation = df_MergeDs_FailuresAppsLocation.sort_values(by=['failure_time'], ascending=True)\n",
    "\n",
    "# Choosing the columns of interest\n",
    "df_MergeDs_FailuresAppsLocation = df_MergeDs_FailuresAppsLocation.loc[:,['disk_id','failure_time', 'model_x','model_y','app','node_id','rack_id','machine_room_id']]\n",
    "\n",
    "# Changing data type\n",
    "#df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.astype(datatype)\n",
    "\n",
    "# Testing\n",
    "#df_Failurelogs_FailuresAppsLocation.head(20)\n",
    "#df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\"'))\n",
    "#df_Failurelogs_FailuresAppsLocation.query(('app == \"RM\"'))\n",
    "#time1 = df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\" & disk_id==39876'))\n",
    "#time2 = df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\" & disk_id==22968'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting OverTimeSSDsFailures into the DB\n",
    "\n",
    "for row in df_AlibabaOver_Failurelogs.itertuples():\n",
    "    insert_OverTimeSSDsFailures = OverTimeSSDsFailures()\n",
    "    insert_OverTimeSSDsFailures.disk_id = row.disk_id\n",
    "    insert_OverTimeSSDsFailures.failure_time = row.failure_time\n",
    "    insert_OverTimeSSDsFailures.model_x = row.model\n",
    "    insert_OverTimeSSDsFailures.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting AllAppsSSDsFailures into the DB\n",
    "\n",
    "for row in df_AlibabaSnapShot_FailuresAppsLocation.itertuples():\n",
    "    insert_AllAppsSSDsFailures = AllAppsSSDsFailures()\n",
    "    insert_AllAppsSSDsFailures.disk_id = row.disk_id\n",
    "    insert_AllAppsSSDsFailures.app = row.app\n",
    "    insert_AllAppsSSDsFailures.failure_time = row.failure_time\n",
    "    insert_AllAppsSSDsFailures.node_id = row.node_id\n",
    "    insert_AllAppsSSDsFailures.rack_id = row.rack_id\n",
    "    insert_AllAppsSSDsFailures.machine_room_id = row.machine_room_id\n",
    "    insert_AllAppsSSDsFailures.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting AllAppsSSDsLocation into the DB\n",
    "\n",
    "for row in df_AlibabaSnapShot_AllAppsSSDsLocation.itertuples():\n",
    "    insert_AllAppsSSDsLocation = AllAppsSSDsLocation()\n",
    "    insert_AllAppsSSDsLocation.disk_id = row.disk_id\n",
    "    insert_AllAppsSSDsLocation.model_y = row.model\n",
    "    insert_AllAppsSSDsLocation.app = row.app\n",
    "    insert_AllAppsSSDsLocation.node_id = row.node_id\n",
    "    insert_AllAppsSSDsLocation.rack_id = row.rack_id\n",
    "    insert_AllAppsSSDsLocation.slot_id = row.slot_id\n",
    "    insert_AllAppsSSDsLocation.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting AllDiskIDSMARTAttributes into the DB\n",
    "\n",
    "for row in df_AlibabaSnapShot_SMARTAttributes.itertuples():\n",
    "    insert_AllDiskIDSMARTAttributes = AllDiskIDSMARTAttributes()\n",
    "    insert_AllDiskIDSMARTAttributes.disk_id = row.disk_id\n",
    "    insert_AllDiskIDSMARTAttributes.model_y = row.model\n",
    "    insert_AllDiskIDSMARTAttributes.r_sectors = row.r_5\n",
    "    insert_AllDiskIDSMARTAttributes.u_errors = row.n_187\n",
    "    insert_AllDiskIDSMARTAttributes.p_on = row.r_9\n",
    "    insert_AllDiskIDSMARTAttributes.p_c_count = row.r_12\n",
    "    insert_AllDiskIDSMARTAttributes.p_failedA = row.n_171\n",
    "    insert_AllDiskIDSMARTAttributes.p_failedB = row.n_181\n",
    "    insert_AllDiskIDSMARTAttributes.e_failedA = row.n_172\n",
    "    insert_AllDiskIDSMARTAttributes.e_failedB = row.n_182\n",
    "    insert_AllDiskIDSMARTAttributes.n_b_written = row.r_241\n",
    "    insert_AllDiskIDSMARTAttributes.n_b_read = row.r_242\n",
    "    insert_AllDiskIDSMARTAttributes.w_l_count = row.r_173\n",
    "    insert_AllDiskIDSMARTAttributes.w_r_d = row.r_177\n",
    "    insert_AllDiskIDSMARTAttributes.media_wearout_i = row.n_233\n",
    "    insert_AllDiskIDSMARTAttributes.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting AllDiskIDSMARTAttributesFirstDayExperiment into the DB\n",
    "\n",
    "for row in df_AlibabaSnapShot_SMARTAttributesFirstDay.itertuples():\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay = AllDiskIDSMARTAttributesFirstDay()\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.disk_id = row.disk_id\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.model_y = row.model\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.r_sectors = row.r_5\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.u_errors = row.n_187\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.p_on = row.r_9\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.p_c_count = row.r_12\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.p_failedA = row.n_171\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.p_failedB = row.n_181\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.e_failedA = row.n_172\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.e_failedB = row.n_182\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.n_b_written = row.r_241\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.n_b_read = row.r_242\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.w_l_count = row.r_173\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.w_r_d = row.r_177\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.media_wearout_i = row.n_233\n",
    "    insert_AllDiskIDSMARTAttributesFirstDay.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting merged (disks and failures in common) FailuresAppsLocation into the DB\n",
    "\n",
    "for row in df_MergeDs_FailuresAppsLocation.itertuples():\n",
    "    insert_FailuresAppsLocation = FailuresAppsLocation()\n",
    "    insert_FailuresAppsLocation.disk_id = row.disk_id\n",
    "    insert_FailuresAppsLocation.failure_time = row.failure_time\n",
    "    insert_FailuresAppsLocation.model_x = row.model_x\n",
    "    insert_FailuresAppsLocation.model_y = row.model_y\n",
    "    insert_FailuresAppsLocation.app = row.app\n",
    "    insert_FailuresAppsLocation.node_id = row.node_id\n",
    "    insert_FailuresAppsLocation.rack_id = row.rack_id\n",
    "    insert_FailuresAppsLocation.machine_room_id = row.machine_room_id\n",
    "    insert_FailuresAppsLocation.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Loading AlibabaOvertime dataset using Pandas and Inserting SMARTAtt into the DB \n",
    "\n",
    "start_date = date(2018, 4, 22)\n",
    "end_date = date(2019, 12, 31)\n",
    "delta = timedelta(days=1)\n",
    "df_AlibabaOver_SMARTlogs = pd.DataFrame()\n",
    "\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogs = pd.read_csv(path)\n",
    "        df_AlibabaOver_SMARTlogs = pd.DataFrame(df_AlibabaOver_SMARTlogs)\n",
    "\n",
    "        # Changing failure time column to datetime type\n",
    "        df_AlibabaOver_SMARTlogs['ds'] =  pd.to_datetime(df_AlibabaOver_SMARTlogs['ds'], format='%Y%m%d')\n",
    "\n",
    "        # Choosing the columns of interest\n",
    "        df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogs.loc[:,['disk_id','ds', 'model','n_5','n_187','n_171','n_181','n_172','n_182','n_241','n_242']]\n",
    "\n",
    "        # Changing the name of some columns to clarify their meaning\n",
    "        df_AlibabaOver_SMARTlogs.rename(columns = {'ds':'timestamp', 'model':'model_x', 'n_5':'r_sectors','n_187':'u_errors','n_171':'p_failedA','n_181':'p_failedB','n_172':'e_failedA','n_182':'e_failedB','n_241':'n_b_written','n_242':'n_b_read'}, inplace=True)\n",
    "\n",
    "        for row in df_AlibabaOver_SMARTlogs.itertuples():\n",
    "            insert_SmartAttributes = SMARTAtt()\n",
    "            insert_SmartAttributes.disk_id = row.disk_id\n",
    "            insert_SmartAttributes.timestamp = row.timestamp\n",
    "            insert_SmartAttributes.model_x = row.model_x\n",
    "\n",
    "            #Checking if the value is int/float. Without this checking an error may be raised during mongoengine validation (saving)\n",
    "\n",
    "            if isinstance(row.r_sectors, (int, float)):\n",
    "                insert_SmartAttributes.r_sectors = row.r_sectors\n",
    "            else: \n",
    "                insert_SmartAttributes.r_sectors = 0\n",
    "            if isinstance(row.u_errors, (int, float)):\n",
    "                insert_SmartAttributes.u_errors = row.u_errors\n",
    "            else: \n",
    "                insert_SmartAttributes.u_errors = 0\n",
    "            if isinstance(row.p_failedA, (int, float)):\n",
    "                insert_SmartAttributes.p_failedA = row.p_failedA\n",
    "            else: \n",
    "                insert_SmartAttributes.p_failedA = 0\n",
    "            if isinstance(row.p_failedB, (int, float)):\n",
    "                insert_SmartAttributes.p_failedB = row.p_failedB\n",
    "            else: \n",
    "                insert_SmartAttributes.p_failedB = 0\n",
    "            if isinstance(row.e_failedA, (int, float)):\n",
    "                insert_SmartAttributes.e_failedA = row.e_failedA\n",
    "            else: \n",
    "                insert_SmartAttributes.e_failedA = 0\n",
    "            if isinstance(row.e_failedB, (int, float)):\n",
    "                insert_SmartAttributes.e_failedB = row.e_failedB\n",
    "            else: \n",
    "                insert_SmartAttributes.e_failedB = 0\n",
    "            if isinstance(row.n_b_written, (int, float)):\n",
    "                insert_SmartAttributes.n_b_written = row.n_b_written\n",
    "            else: \n",
    "                insert_SmartAttributes.n_b_written = 0\n",
    "            if isinstance(row.n_b_read, (int, float)):\n",
    "                insert_SmartAttributes.n_b_read = row.n_b_read\n",
    "            else: \n",
    "                insert_SmartAttributes.n_b_read = 0\n",
    "            insert_SmartAttributes.save()\n",
    "\n",
    "        #df_AlibabaOver_SMARTlogs_Filtered = pd.concat([df_AlibabaOver_SMARTlogs_Filtered, df_AlibabaOver_SMARTlogs], ignore_index=True)\n",
    "    start_date += delta\n",
    "\n",
    "# Changing the name of some columns to clarify their meaning\n",
    "#df_AlibabaOver_SMARTlogs_Filtered.rename(columns = {'ds':'timestamp', 'model':'model_x', 'n_5':'r_sectors','n_187':'u_errors','n_171':'p_failedA','n_181':'p_failedB','n_172':'e_failedA','n_182':'e_failedB','n_241':'n_b_written','n_242':'n_b_read'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test to perform the extraction of wear leveling values\n",
    "# Loading AlibabaOvertime dataset using Pandas\n",
    "\n",
    "start_date = date(2018, 1, 1)\n",
    "end_date = date(2019, 12, 31)\n",
    "delta = timedelta(days=1)\n",
    "#df_AlibabaOver_SMARTlogs = pd.DataFrame()\n",
    "\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogsWear = pd.read_csv(path)\n",
    "        df_AlibabaOver_SMARTlogsWear = pd.DataFrame(df_AlibabaOver_SMARTlogsWear)\n",
    "\n",
    "        # Changing failure time column to datetime type\n",
    "        df_AlibabaOver_SMARTlogsWear['ds'] =  pd.to_datetime(df_AlibabaOver_SMARTlogsWear['ds'], format='%Y%m%d')\n",
    "\n",
    "        # Choosing the columns of interest\n",
    "        df_AlibabaOver_SMARTlogsWear = df_AlibabaOver_SMARTlogsWear.loc[:,['disk_id','ds', 'model','r_5','r_173','r_177','r_233']]\n",
    "\n",
    "        # Changing the name of some columns to clarify their meaning\n",
    "        df_AlibabaOver_SMARTlogsWear.rename(columns = {'ds':'timestamp', 'model':'model_x', 'r_5':'r_sectors','r_173':'w_l_count','r_177':'w_r_d','r_233':'media_wearout_i'}, inplace=True)\n",
    "\n",
    "        for row in df_AlibabaOver_SMARTlogsWear.itertuples():\n",
    "            insert_SMARTAttOverWear = SMARTAttOverWear()\n",
    "            insert_SMARTAttOverWear.disk_id = row.disk_id\n",
    "            insert_SMARTAttOverWear.timestamp = row.timestamp\n",
    "            insert_SMARTAttOverWear.model_x = row.model_x\n",
    "\n",
    "            #Checking if the value is int/float. Without this checking an error may be raised during mongoengine validation (saving)\n",
    "\n",
    "            if isinstance(row.r_sectors, (int, float)):\n",
    "                insert_SMARTAttOverWear.r_sectors = row.r_sectors\n",
    "            else: \n",
    "                insert_SMARTAttOverWear.r_sectors = 0\n",
    "            if isinstance(row.w_l_count, (int, float)):\n",
    "                insert_SMARTAttOverWear.w_l_count = row.w_l_count\n",
    "            else: \n",
    "                insert_SMARTAttOverWear.w_l_count = 0\n",
    "            if isinstance(row.w_r_d, (int, float)):\n",
    "                insert_SMARTAttOverWear.w_r_d = row.w_r_d\n",
    "            else: \n",
    "                insert_SMARTAttOverWear.w_r_d = 0\n",
    "            if isinstance(row.media_wearout_i, (int, float)):\n",
    "                insert_SMARTAttOverWear.media_wearout_i = row.media_wearout_i\n",
    "            else: \n",
    "                insert_SMARTAttOverWear.media_wearout_i = 0\n",
    "            insert_SMARTAttOverWear.save()\n",
    "\n",
    "        #df_AlibabaOver_SMARTlogs_Filtered = pd.concat([df_AlibabaOver_SMARTlogs_Filtered, df_AlibabaOver_SMARTlogs], ignore_index=True)\n",
    "    start_date += delta\n",
    "\n",
    "# Changing the name of some columns to clarify their meaning\n",
    "#df_AlibabaOver_SMARTlogs_Filtered.rename(columns = {'ds':'timestamp', 'model':'model_x', 'n_5':'r_sectors','n_187':'u_errors','n_171':'p_failedA','n_181':'p_failedB','n_172':'e_failedA','n_182':'e_failedB','n_241':'n_b_written','n_242':'n_b_read'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading AlibabaOvertime dataset using Pandas and Inserting SMARTAtt into the DB \n",
    "\n",
    "start_date = date(2018, 1, 1)\n",
    "end_date = date(2019, 12, 31)\n",
    "delta = timedelta(days=1)\n",
    "#df_AlibabaOver_SMARTlogs = pd.DataFrame()\n",
    "\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/root/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogsFull = pd.read_csv(path)\n",
    "        df_AlibabaOver_SMARTlogsFull = pd.DataFrame(df_AlibabaOver_SMARTlogsFull)\n",
    "\n",
    "        # Changing failure time column to datetime type\n",
    "        df_AlibabaOver_SMARTlogsFull['ds'] =  pd.to_datetime(df_AlibabaOver_SMARTlogsFull['ds'], format='%Y%m%d')\n",
    "\n",
    "        # Choosing the columns of interest\n",
    "        df_AlibabaOver_SMARTlogsFull = df_AlibabaOver_SMARTlogsFull.loc[:,['disk_id','ds', 'model','r_5', 'r_9','r_187','r_171','r_181','r_172','r_182','r_241','r_242','r_173','r_177','n_233']]\n",
    "        \n",
    "        # Changing the name of some columns to clarify their meaning\n",
    "        df_AlibabaOver_SMARTlogsFull.rename(columns = {'ds':'timestamp', 'model':'model_x', 'r_5':'r_sectors', 'r_9':'power_hours' ,'r_187':'u_errors','r_171':'p_failedA','r_181':'p_failedB','r_172':'e_failedA','r_182':'e_failedB','r_241':'n_b_written','r_242':'n_b_read','r_173':'w_l_count','r_177':'w_r_d','n_233':'media_wearout_i'}, inplace=True)\n",
    "\n",
    "        for row in df_AlibabaOver_SMARTlogsFull.itertuples():\n",
    "            insert_SMARTAttFull = SMARTAttFull()\n",
    "            insert_SMARTAttFull.disk_id = row.disk_id\n",
    "            insert_SMARTAttFull.timestamp = row.timestamp\n",
    "            insert_SMARTAttFull.model_x = row.model_x\n",
    "\n",
    "            #Checking if the value is int/float. Without this checking an error may be raised during mongoengine validation (saving)\n",
    "\n",
    "            if isinstance(row.r_sectors, (int, float)):\n",
    "                insert_SMARTAttFull.r_sectors = row.r_sectors\n",
    "            else: \n",
    "                insert_SMARTAttFull.r_sectors = 0\n",
    "            if isinstance(row.power_hours, (int, float)):\n",
    "                insert_SMARTAttFull.power_hours = row.power_hours\n",
    "            else: \n",
    "                insert_SMARTAttFull.power_hours = 0\n",
    "            if isinstance(row.u_errors, (int, float)):\n",
    "                insert_SMARTAttFull.u_errors = row.u_errors\n",
    "            else: \n",
    "                insert_SMARTAttFull.u_errors = 0\n",
    "            if isinstance(row.p_failedA, (int, float)):\n",
    "                insert_SMARTAttFull.p_failedA = row.p_failedA\n",
    "            else: \n",
    "                insert_SMARTAttFull.p_failedA = 0\n",
    "            if isinstance(row.p_failedB, (int, float)):\n",
    "                insert_SMARTAttFull.p_failedB = row.p_failedB\n",
    "            else: \n",
    "                insert_SMARTAttFull.p_failedB = 0\n",
    "            if isinstance(row.e_failedA, (int, float)):\n",
    "                insert_SMARTAttFull.e_failedA = row.e_failedA\n",
    "            else: \n",
    "                insert_SMARTAttFull.e_failedA = 0\n",
    "            if isinstance(row.e_failedB, (int, float)):\n",
    "                insert_SMARTAttFull.e_failedB = row.e_failedB\n",
    "            else: \n",
    "                insert_SMARTAttFull.e_failedB = 0\n",
    "            if isinstance(row.n_b_written, (int, float)):\n",
    "                insert_SMARTAttFull.n_b_written = row.n_b_written\n",
    "            else: \n",
    "                insert_SMARTAttFull.n_b_written = 0\n",
    "            if isinstance(row.n_b_read, (int, float)):\n",
    "                insert_SMARTAttFull.n_b_read = row.n_b_read\n",
    "            else: \n",
    "                insert_SMARTAttFull.n_b_read = 0\n",
    "            if isinstance(row.w_l_count, (int, float)):\n",
    "                insert_SMARTAttFull.w_l_count = row.w_l_count\n",
    "            else: \n",
    "                insert_SMARTAttFull.w_l_count = 0\n",
    "            if isinstance(row.w_r_d, (int, float)):\n",
    "                insert_SMARTAttFull.w_r_d = row.w_r_d\n",
    "            else: \n",
    "                insert_SMARTAttFull.w_r_d = 0\n",
    "            if isinstance(row.media_wearout_i, (int, float)):\n",
    "                insert_SMARTAttFull.media_wearout_i = row.media_wearout_i\n",
    "            else: \n",
    "                insert_SMARTAttFull.media_wearout_i = 0\n",
    "            insert_SMARTAttFull.save()\n",
    "\n",
    "        #df_AlibabaOver_SMARTlogs_Filtered = pd.concat([df_AlibabaOver_SMARTlogs_Filtered, df_AlibabaOver_SMARTlogs], ignore_index=True)\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Backblaze dataset using Pandas and Inserting SMARTAttFullBackBlaze into the DB \n",
    "\n",
    "start_date = date(2018, 1, 1)\n",
    "end_date = date(2022, 6, 30)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/root/backblaze/' + start_date.strftime(\"%Y-%m-%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_SMARTAttFullBackBlaze = pd.read_csv(path)\n",
    "        df_SMARTAttFullBackBlaze = pd.DataFrame(df_SMARTAttFullBackBlaze)\n",
    "\n",
    "        # Changing failure time column to datetime type\n",
    "        df_SMARTAttFullBackBlaze['date'] =  pd.to_datetime(df_SMARTAttFullBackBlaze['date'], format='%Y-%m-%d')\n",
    "\n",
    "        # Choosing the columns of interest\n",
    "        df_SMARTAttFullBackBlaze = df_SMARTAttFullBackBlaze.loc[:,['serial_number', 'model', 'date', 'capacity_bytes', 'failure', 'smart_5_raw', 'smart_9_raw','smart_187_raw','smart_1_raw','smart_241_raw','smart_242_raw','smart_188_raw','smart_197_raw','smart_198_raw']]\n",
    "        \n",
    "        # Changing the name of some columns to clarify their meaning\n",
    "        df_SMARTAttFullBackBlaze.rename(columns = {'serial_number':'disk_id', 'date':'timestamp', 'smart_5_raw':'r_sectors', 'smart_9_raw':'power_hours' ,'smart_187_raw':'u_errors','smart_1_raw':'read_error_rate','smart_241_raw':'n_b_written','smart_242_raw':'n_b_read','smart_188_raw':'command_timeout','smart_197_raw':'current_pending_sector_count','smart_198_raw':'uncorrectable_sector_count'}, inplace=True)\n",
    "\n",
    "        for row in df_SMARTAttFullBackBlaze.itertuples():\n",
    "            insert_SMARTAttFullBackBlaze = SMARTAttFullBackBlaze()\n",
    "            insert_SMARTAttFullBackBlaze.disk_id = row.disk_id\n",
    "            insert_SMARTAttFullBackBlaze.timestamp = row.timestamp\n",
    "            insert_SMARTAttFullBackBlaze.model = row.model\n",
    "\n",
    "            #Checking if the value is int/float. Without this checking an error may be raised during mongoengine validation (saving)\n",
    "            if isinstance(row.capacity_bytes, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.capacity_bytes = row.capacity_bytes\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.capacity_bytes = 0\n",
    "            if isinstance(row.failure, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.failure = row.failure\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.failure = 0            \n",
    "            if isinstance(row.r_sectors, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.r_sectors = row.r_sectors\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.r_sectors = 0\n",
    "            if isinstance(row.power_hours, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.power_hours = row.power_hours\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.power_hours = 0\n",
    "            if isinstance(row.u_errors, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.u_errors = row.u_errors\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.u_errors = 0\n",
    "            if isinstance(row.read_error_rate, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.read_error_rate = row.read_error_rate\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.read_error_rate = 0\n",
    "            if isinstance(row.n_b_written, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.n_b_written = row.n_b_written\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.n_b_written = 0\n",
    "            if isinstance(row.n_b_read, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.n_b_read = row.n_b_read\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.n_b_read = 0\n",
    "            if isinstance(row.command_timeout, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.command_timeout = row.command_timeout\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.command_timeout = 0\n",
    "            if isinstance(row.current_pending_sector_count, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.current_pending_sector_count = row.current_pending_sector_count\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.current_pending_sector_count = 0\n",
    "            if isinstance(row.uncorrectable_sector_count, (int, float)):\n",
    "                insert_SMARTAttFullBackBlaze.uncorrectable_sector_count = row.uncorrectable_sector_count\n",
    "            else: \n",
    "                insert_SMARTAttFullBackBlaze.uncorrectable_sector_count = 0\n",
    "            insert_SMARTAttFullBackBlaze.save()\n",
    "    start_date += delta"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
