{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reset specific variables (replace regular_expression by the variables of interest)\n",
    "#%reset_selective <regular_expression>\n",
    "\n",
    "# reset all variables\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from datetime import datetime, date, timedelta\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "from mongoengine import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True, read_preference=Primary(), uuidrepresentation=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating/Connecting Mongo DB instances\n",
    "\n",
    "# Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "#CONNECTION_STRING = \"mongodb+srv://<jgu>:<123>@<cluster-jgu>.mongodb.net/SMARTAttributesFilter\"\n",
    "\n",
    "connect(db='SMARTAttributesFilter', alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "connect(db='FailuresAppsLocation', alias='FailuresAppsLocation_alias')\n",
    "\n",
    "connect(db='SMARTAtt_FailuresAppsLocation', alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting document schema\n",
    "\n",
    "class SMARTAtt(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     timestamp = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     r_sectors = FloatField(required=False, default='0')\n",
    "     u_errors = FloatField(required=False, default='0')\n",
    "     p_failedA = FloatField(required=False, default='0')\n",
    "     p_failedB = FloatField(required=False, default='0')\n",
    "     e_failedA = FloatField(required=False, default='0')\n",
    "     e_failedB = FloatField(required=False, default='0')\n",
    "     n_b_written = FloatField(required=False, default='0')\n",
    "     n_b_read = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'SMARTAttributesFilter_alias'}\n",
    "\n",
    "class FailuresAppsLocation(Document):\n",
    "     disk_id = FloatField(required=False, default='0')\n",
    "     failure_time = DateTimeField(required=False, default='0')\n",
    "     model_x = StringField(required=False, default='0')\n",
    "     model_y = StringField(required=False, default='0')\n",
    "     app = StringField(required=False, default='0')\n",
    "     node_id = FloatField(required=False, default='0')\n",
    "     rack_id = FloatField(required=False, default='0')\n",
    "     machine_room_id = FloatField(required=False, default='0')\n",
    "     meta = {'db_alias': 'FailuresAppsLocation_alias'}\n",
    "\n",
    "class SMARTAtt_FailuresAppsLocation(Document):\n",
    "     smart_att = ReferenceField(SMARTAtt)\n",
    "     failures_app_location = ReferenceField(FailuresAppsLocation)\n",
    "     meta = {'db_alias': 'SMARTAtt_FailuresAppsLocation_alias'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deleting DB content (for the case when the goal is to test the code from zero - otherwise the db will contain several replicas)\n",
    "\n",
    "# Creating the object related to the whole collection\n",
    "#failuresAppsLocationTeste = FailuresAppsLocation.objects() \n",
    "\n",
    "# Deleting all collection\n",
    "#failuresAppsLocationTeste.delete() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the connection to the DB\n",
    "\n",
    "disconnect(alias='SMARTAttributesFilter_alias')\n",
    "\n",
    "disconnect(alias='FailuresAppsLocation_alias_alias')\n",
    "\n",
    "disconnect(alias='SMARTAtt_FailuresAppsLocation_alias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading datasaet - AlibabaOverTime (Failurelogs)\n",
    "\n",
    "df_AlibabaOver_Failurelogs = pd.read_csv('/media/erb/hdd1/DataSet/alibabaOvertime/ssd_failure_label/ssd_failure_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading dataset - Alibaba Snapshot (TimeStamps of failed SSDs, SMART attributes in 39 columns, SSDs location, applications, SSD models and Disk ID)\n",
    "\n",
    "df_AlibabaSnapShot_FailuresAppsLocation = pd.read_csv('/media/erb/hdd1/DataSet/alibabaSnapShot/ssd_failure_tag/ssd_failure_tag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Failures and location datasets and fixing columns types (to have ssd failure data that has location, failure time, and smart att)\n",
    "df_Failurelogs_FailuresAppsLocation =  pd.merge(df_AlibabaOver_Failurelogs, df_AlibabaSnapShot_FailuresAppsLocation, how = 'inner', on = ['disk_id', 'failure_time'])\n",
    "\n",
    "# Changing failure time column to datetime type\n",
    "df_Failurelogs_FailuresAppsLocation['failure_time'] =  pd.to_datetime(df_Failurelogs_FailuresAppsLocation['failure_time'])\n",
    "\n",
    "# Removing duplicates\n",
    "#df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.drop_duplicates(subset='disk_id', keep=\"first\")\n",
    "\n",
    "# Forcing sorting\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.sort_values(by=['failure_time'], ascending=True)\n",
    "\n",
    "# Choosing the columns of interest\n",
    "df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.loc[:,['disk_id','failure_time', 'model_x','model_y','app','node_id','rack_id','machine_room_id']]\n",
    "\n",
    "# Changing data type\n",
    "#df_Failurelogs_FailuresAppsLocation = df_Failurelogs_FailuresAppsLocation.astype(datatype)\n",
    "\n",
    "# Testing\n",
    "#df_Failurelogs_FailuresAppsLocation.head(20)\n",
    "#df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\"'))\n",
    "#df_Failurelogs_FailuresAppsLocation.query(('app == \"RM\"'))\n",
    "#time1 = df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\" & disk_id==39876'))\n",
    "#time2 = df_Failurelogs_FailuresAppsLocation.query(('rack_id == 17596 & app == \"RM\" & disk_id==22968'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting FailuresAppsLocation into the DB\n",
    "\n",
    "for row in df_Failurelogs_FailuresAppsLocation.itertuples():\n",
    "    insert_FailuresAppsLocation = FailuresAppsLocation()\n",
    "    insert_FailuresAppsLocation.disk_id = row.disk_id\n",
    "    insert_FailuresAppsLocation.failure_time = row.failure_time\n",
    "    insert_FailuresAppsLocation.model_x = row.model_x\n",
    "    insert_FailuresAppsLocation.model_y = row.model_y\n",
    "    insert_FailuresAppsLocation.app = row.app\n",
    "    insert_FailuresAppsLocation.node_id = row.node_id\n",
    "    insert_FailuresAppsLocation.rack_id = row.rack_id\n",
    "    insert_FailuresAppsLocation.machine_room_id = row.machine_room_id\n",
    "    insert_FailuresAppsLocation.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some functions in Mongoengine\n",
    "\n",
    "# Creating the object related to the whole collection\n",
    "#failuresAppsLocationTeste = FailuresAppsLocation.objects()  \n",
    "\n",
    "# Query if you know something about the document\n",
    "#testando = FailuresAppsLocation.objects(disk_id=\"33722\").get()\n",
    "\n",
    "# Printing\n",
    "#testando.disk_id\n",
    "\n",
    "# Deleting all collection\n",
    "#failuresAppsLocationTeste.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loading AlibabaOvertime dataset using Pandas\n",
    "\n",
    "start_date = date(2018, 4, 22)\n",
    "end_date = date(2019, 12, 31)\n",
    "delta = timedelta(days=1)\n",
    "df_AlibabaOver_SMARTlogs = pd.DataFrame()\n",
    "\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogs = pd.read_csv(path)\n",
    "        df_AlibabaOver_SMARTlogs = pd.DataFrame(df_AlibabaOver_SMARTlogs)\n",
    "\n",
    "        # Changing failure time column to datetime type\n",
    "        df_AlibabaOver_SMARTlogs['ds'] =  pd.to_datetime(df_AlibabaOver_SMARTlogs['ds'], format='%Y%m%d')\n",
    "\n",
    "        # Choosing the columns of interest\n",
    "        df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogs.loc[:,['disk_id','ds', 'model','n_5','n_187','n_171','n_181','n_172','n_182','n_241','n_242']]\n",
    "\n",
    "        # Changing the name of some columns to clarify their meaning\n",
    "        df_AlibabaOver_SMARTlogs.rename(columns = {'ds':'timestamp', 'model':'model_x', 'n_5':'r_sectors','n_187':'u_errors','n_171':'p_failedA','n_181':'p_failedB','n_172':'e_failedA','n_182':'e_failedB','n_241':'n_b_written','n_242':'n_b_read'}, inplace=True)\n",
    "\n",
    "        for row in df_AlibabaOver_SMARTlogs.itertuples():\n",
    "            insert_SmartAttributes = SMARTAtt()\n",
    "            insert_SmartAttributes.disk_id = row.disk_id\n",
    "            insert_SmartAttributes.timestamp = row.timestamp\n",
    "            insert_SmartAttributes.model_x = row.model_x\n",
    "\n",
    "            #Checking if the value is int/float. Without this checking an error may be raised during mongoengine validation (saving)\n",
    "\n",
    "            if isinstance(row.r_sectors, (int, float)):\n",
    "                insert_SmartAttributes.r_sectors = row.r_sectors\n",
    "            else: \n",
    "                insert_SmartAttributes.r_sectors = 0\n",
    "            if isinstance(row.u_errors, (int, float)):\n",
    "                insert_SmartAttributes.u_errors = row.u_errors\n",
    "            else: \n",
    "                insert_SmartAttributes.u_errors = 0\n",
    "            if isinstance(row.p_failedA, (int, float)):\n",
    "                insert_SmartAttributes.p_failedA = row.p_failedA\n",
    "            else: \n",
    "                insert_SmartAttributes.p_failedA = 0\n",
    "            if isinstance(row.p_failedB, (int, float)):\n",
    "                insert_SmartAttributes.p_failedB = row.p_failedB\n",
    "            else: \n",
    "                insert_SmartAttributes.p_failedB = 0\n",
    "            if isinstance(row.e_failedA, (int, float)):\n",
    "                insert_SmartAttributes.e_failedA = row.e_failedA\n",
    "            else: \n",
    "                insert_SmartAttributes.e_failedA = 0\n",
    "            if isinstance(row.e_failedB, (int, float)):\n",
    "                insert_SmartAttributes.e_failedB = row.e_failedB\n",
    "            else: \n",
    "                insert_SmartAttributes.e_failedB = 0\n",
    "            if isinstance(row.n_b_written, (int, float)):\n",
    "                insert_SmartAttributes.n_b_written = row.n_b_written\n",
    "            else: \n",
    "                insert_SmartAttributes.n_b_written = 0\n",
    "            if isinstance(row.n_b_read, (int, float)):\n",
    "                insert_SmartAttributes.n_b_read = row.n_b_read\n",
    "            else: \n",
    "                insert_SmartAttributes.n_b_read = 0\n",
    "            insert_SmartAttributes.save()\n",
    "\n",
    "        #df_AlibabaOver_SMARTlogs_Filtered = pd.concat([df_AlibabaOver_SMARTlogs_Filtered, df_AlibabaOver_SMARTlogs], ignore_index=True)\n",
    "    start_date += delta\n",
    "\n",
    "# Changing the name of some columns to clarify their meaning\n",
    "#df_AlibabaOver_SMARTlogs_Filtered.rename(columns = {'ds':'timestamp', 'model':'model_x', 'n_5':'r_sectors','n_187':'u_errors','n_171':'p_failedA','n_181':'p_failedB','n_172':'e_failedA','n_182':'e_failedB','n_241':'n_b_written','n_242':'n_b_read'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading AlibabaOvertime dataset using Dask\n",
    "\n",
    "start_date = date(2018, 2, 1)\n",
    "start_dateAux = start_date\n",
    "end_date = date(2018, 5, 31)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "while start_date <= end_date:\n",
    "    path = Path('/media/erb/hdd1/DataSet/alibabaOvertime/smartlogs/' + start_date.strftime(\"%Y%m%d\") + '.csv')\n",
    "\n",
    "    if path.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_AlibabaOver_SMARTlogsTemp = dd.read_csv(path)\n",
    "        if start_date == start_dateAux:\n",
    "            df_AlibabaOver_SMARTlogs = df_AlibabaOver_SMARTlogsTemp\n",
    "        else: \n",
    "            df_AlibabaOver_SMARTlogs = dd.concat([df_AlibabaOver_SMARTlogs, df_AlibabaOver_SMARTlogsTemp])\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Alibaba's SMART attributes over two years dataset \n",
    "\n",
    "# settings to display all columns\n",
    "#pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "#df_AlibabaOver_SMARTlogs_Filtered.query('disk_id == 4711')\n",
    "#df_AlibabaOver_Failurelogs.head(10)\n",
    "#df_Failurelogs_FailuresAppsLocation.dtypes\n",
    "\n",
    "# Creating the object related to the whole collection\n",
    "#SMARTAttributesTest = SMARTAtt.objects()  \n",
    "\n",
    "# Query if you know something about the document\n",
    "SMARTAttributesTest = SMARTAtt.objects(timestamp=\"2018-04-22\").filter()\n",
    "\n",
    "\n",
    "# Printing\n",
    "for i in SMARTAttributesTest:\n",
    "  print(i.timestamp)\n",
    "  \n",
    "# Deleting all collection\n",
    "#for i in SMARTAttributesTest:\n",
    " # i.delete()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Backblaze dataset\n",
    "\n",
    "start_dateBB = date(2021, 12, 29)\n",
    "start_dateAuxBB = start_dateBB\n",
    "end_dateBB = date(2021, 12, 31)\n",
    "deltaBB = timedelta(days=1)\n",
    "\n",
    "while start_dateBB <= end_dateBB:\n",
    "    pathBB = Path('/media/erb/hdd1/DataSet/backblaze/smartlogs/' + start_dateBB.strftime(\"%Y-%m-%d\") + '.csv')\n",
    "\n",
    "    if pathBB.is_file(): # checking if a particular file for a specific date is missing\n",
    "        df_BackBlaze_SMARTlogsTemp = pd.read_csv(pathBB)\n",
    "        if start_dateBB == start_dateAuxBB: # due to dask instancing of variables\n",
    "            df_BackBlaze_SMARTlogs = df_BackBlaze_SMARTlogsTemp\n",
    "        else: \n",
    "            df_BackBlaze_SMARTlogs = pd.concat([df_BackBlaze_SMARTlogs, df_BackBlaze_SMARTlogsTemp])\n",
    "    start_dateBB += deltaBB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing BackBlaze dataset\n",
    "\n",
    "df_BackBlaze_SMARTlogs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Carregando dados filtrados em um dataframe para graficos de performance\n",
    "\n",
    "dataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Separando por cenários\n",
    "\n",
    "##Cenário 1\n",
    "dataset_cenUm = dataset.query('scenario == 1')\n",
    "\n",
    "### Escrita\n",
    "dataset_write_cenUm = dataset_cenUm.query('operation == \"W\" & individual_total == \"T\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## RETIRANDO RUÍDOS DAS MEDIÇÕES DE TENSÃO para performance\n",
    "\n",
    "##Cenário 1\n",
    "\n",
    "### Escrita\n",
    "\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(str)\n",
    "dataset_write_cenUm['power_average1_hdd'] = dataset_write_cenUm['power_average1_hdd'].apply(lambda x: x.split(\"e\")[0])\n",
    "dataset_write_cenUm.power_average1_hdd = dataset_write_cenUm.power_average1_hdd.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CENARIO 1\n",
    "\n",
    "## WRITE IOPS\n",
    "\n",
    "fig = sns.catplot(x=\"capacity\", y=\"iops_total\", hue=\"object_size\", data=dataset_write_cenUm,\n",
    "               row=\"pattern\", col=\"work_outstd\", kind=\"bar\", ci=90, palette=\"Blues_d\", aspect=0.9, height=4.5\n",
    "                  , legend_out = True, margin_titles = True)\n",
    "\n",
    "fig.set_axis_labels(\"Capacity\", \"IOPS\")\n",
    "fig.set_xticklabels([\"80GB HDD\", \"500GB HDD\", \"1TB HDD\", \"1TB WDHDD\", \"120GB SSD\", \"Hybrid\"])\n",
    "\n",
    "plt.savefig('write_iops.pdf', dpi=1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
